RANKING 466
QUERY
authenticated garbling and efficient maliciously secure two - party computation we propose a simple and efficient framework for obtaining efficient constant - round protocols for maliciously secure two - party computation . our framework uses a function - independent preprocessing phase to generate authenticated information for the two parties ; this information is then used to construct a single " authenticated " garbled circuit which is transmitted and evaluated . we also show how to efficiently instantiate the preprocessing phase with a new , highly optimized version of the tinyot protocol by nielsen et al . our protocol outperforms existing work in both the single - execution and amortized settings , with or without preprocessing : in the single - execution setting , our protocol evaluates an aes circuit with malicious security in 37 ms with an online time of 1 ms . previous work with the best overall time requires 62 ms ( with 14 ms online time ) ; previous work with the best online time ( also 1 ms ) requires 124 ms overall . if we amortize over 1024 executions , each aes computation requires just 6.7 ms with roughly the same online time as above . the best previous work in the amortized setting has roughly the same total time but does not support function - independent preprocessing . our work shows that the performance penalty for maliciously secure two - party computation ( as compared to semi - honest security ) is much smaller than previously believed .
First cited at 1
TOP CITED PAPERS
RANK 1
faster malicious 2-party secure computation with online / offline dual execution we describe a highly optimized protocol for generalpurpose secure two - party computation ( 2pc ) in the presence of malicious adversaries . our starting point is a protocol of kolesnikov et al . ( tcc 2015 ) . we adapt that protocol to the online / offline setting , where two parties repeatedly evaluate the same function ( on possibly different inputs each time ) and perform as much of the computation as possible in an offline preprocessing phase before their inputs are known . along the way we develop several significant simplifications and optimizations to the protocol . we have implemented a prototype of our protocol and report on its performance . when two parties on amazon servers in the same region use our implementation to securely evaluate the aes circuit 1024 times , the amortized cost per evaluation is 5.1ms offline + 1.3ms online . the total offline+online cost of our protocol is in fact less than the online cost of any reported protocol with malicious security . for comparison , our protocol ’s closest competitor ( lindell & riva , ccs 2015 ) uses 74ms offline + 7ms online in an identical setup . our protocol can be further tuned to trade performance for leakage . as an example , the performance in the above scenario improves to 2.4ms offline + 1.0ms online if we allow an adversary to learn a single bit about the honest party ’s input with probability 2−20 ( but not violate any other security property , e.g. correctness ) .
RANK 3
faster secure two - party computation using garbled circuits secure two - party computation enables two parties to evaluate a function cooperatively without revealing to either party anything beyond the function ’s output . the garbled - circuit technique , a generic approach to secure two - party computation for semi - honest participants , was developed by yao in the 1980s , but has been viewed as being of limited practical significance due to its inefficiency . we demonstrate several techniques for improving the running time and memory requirements of the garbled - circuit technique , resulting in an implementation of generic secure two - party computation that is significantly faster than any previously reported while also scaling to arbitrarily large circuits . we validate our approach by demonstrating secure computation of circuits with over 109 gates at a rate of roughly 10 μs per garbled gate , and showing order - of - magnitude improvements over the best previous privacy - preserving protocols for computing hamming distance , levenshtein distance , smith - waterman genome alignment , and aes .
RANK 19
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
TOP UNCITED PAPERS
RANK 2
secure two - party computation in sublinear ( amortized ) time traditional approaches to generic secure computation begin by representing the function f being computed as a circuit . if f depends on each of its input bits , this implies a protocol with complexity at least linear in the input size . in fact , linear running time is inherent for non - trivial functions since each party must " touch " every bit of their input lest information about the other party 's input be leaked . this seems to rule out many applications of secure computation ( e.g. , database search ) in scenarios where inputs are huge . adapting and extending an idea of ostrovsky and shoup , we present an approach to secure two - party computation that yields protocols running in sublinear time , in an amortized sense , for functions that can be computed in sublinear time on a random - access machine ( ram ) . moreover , each party is required to maintain state that is only ( essentially ) linear in its own input size . our approach combines generic secure two - party computation with oblivious ram ( oram ) protocols . we present an optimized version of our approach using yao 's garbled - circuit protocol and a recent oram construction of shi et al . we describe an implementation of our resulting protocol , and evaluate its performance for obliviously searching a database with over 1 million entries . our implementation outperforms off - the - shelf secure - computation protocols for databases containing more than 218 entries .
RANK 4
pcf : a portable circuit format for scalable two - party secure computation a secure computation protocol for a function f ( x , y ) must leak no information about inputs x , y during its execution ; thus it is imperative to compute the function f in a data - oblivious manner . traditionally , this has been accomplished by compiling f into a boolean circuit . previous approaches , however , have scaled poorly as the circuit size increases . we present a new approach to compiling such circuits that is substantially more efficient than prior work . our approach is based on online circuit compression and lazy gate generation . we implemented an optimizing compiler for this new representation of circuits , and evaluated the use of this representation in two secure computation environments . our evaluation demonstrates the utility of this approach , allowing us to scale secure computation beyond any previous system while requiring substantially less cpu time and disk space . in our largest test , we evaluate an rsa-1024 signature function with more than 42 billion gates , that was generated and optimized using our compiler . with our techniques , the bottleneck in secure computation lies with the cryptographic primitives , not the compilation or storage of circuits .
RANK 5
fast and secure three - party computation : the garbled circuit approach many deployments of secure multi - party computation ( mpc ) in practice have used information - theoretic three - party protocols that tolerate a single , semi - honest corrupt party , since these protocols enjoy very high efficiency . we propose a new approach for secure three - party computation ( 3pc ) that improves security while maintaining practical efficiency that is competitive with traditional information - theoretic protocols . our protocol is based on garbled circuits and provides security against a single , malicious corrupt party . unlike information - theoretic 3pc protocols , ours uses a constant number of rounds . our protocol only uses inexpensive symmetric - key cryptography : hash functions , block ciphers , pseudorandom generators ( in particular , no oblivious transfers ) and has performance that is comparable to that of yao 's ( semi - honest ) 2pc protocol . we demonstrate the practicality of our protocol with an implementation based on the justgarble framework of bellare et al . ( s&p 2013 ) . the implementation incorporates various optimizations including the most recent techniques for efficient circuit garbling . we perform experiments on several benchmarking circuits , in different setups . our experiments confirm that , despite providing a more demanding security guarantee , our protocol has performance comparable to existing information - theoretic 3pc .
TOP 20
RANK = 1; score = 0.3500194867169313; correct = True; id = 5adc94602d07e49cc1e94e2aa2b1bdf3481a47f8
faster malicious 2-party secure computation with online / offline dual execution we describe a highly optimized protocol for generalpurpose secure two - party computation ( 2pc ) in the presence of malicious adversaries . our starting point is a protocol of kolesnikov et al . ( tcc 2015 ) . we adapt that protocol to the online / offline setting , where two parties repeatedly evaluate the same function ( on possibly different inputs each time ) and perform as much of the computation as possible in an offline preprocessing phase before their inputs are known . along the way we develop several significant simplifications and optimizations to the protocol . we have implemented a prototype of our protocol and report on its performance . when two parties on amazon servers in the same region use our implementation to securely evaluate the aes circuit 1024 times , the amortized cost per evaluation is 5.1ms offline + 1.3ms online . the total offline+online cost of our protocol is in fact less than the online cost of any reported protocol with malicious security . for comparison , our protocol ’s closest competitor ( lindell & riva , ccs 2015 ) uses 74ms offline + 7ms online in an identical setup . our protocol can be further tuned to trade performance for leakage . as an example , the performance in the above scenario improves to 2.4ms offline + 1.0ms online if we allow an adversary to learn a single bit about the honest party ’s input with probability 2−20 ( but not violate any other security property , e.g. correctness ) .
RANK = 2; score = 0.31513282313743946; correct = False; id = 2905a5c4da8c9a0970f078a211742316ef0ab77d
secure two - party computation in sublinear ( amortized ) time traditional approaches to generic secure computation begin by representing the function f being computed as a circuit . if f depends on each of its input bits , this implies a protocol with complexity at least linear in the input size . in fact , linear running time is inherent for non - trivial functions since each party must " touch " every bit of their input lest information about the other party 's input be leaked . this seems to rule out many applications of secure computation ( e.g. , database search ) in scenarios where inputs are huge . adapting and extending an idea of ostrovsky and shoup , we present an approach to secure two - party computation that yields protocols running in sublinear time , in an amortized sense , for functions that can be computed in sublinear time on a random - access machine ( ram ) . moreover , each party is required to maintain state that is only ( essentially ) linear in its own input size . our approach combines generic secure two - party computation with oblivious ram ( oram ) protocols . we present an optimized version of our approach using yao 's garbled - circuit protocol and a recent oram construction of shi et al . we describe an implementation of our resulting protocol , and evaluate its performance for obliviously searching a database with over 1 million entries . our implementation outperforms off - the - shelf secure - computation protocols for databases containing more than 218 entries .
RANK = 3; score = 0.3060558240097044; correct = True; id = 15509fdb7fc7bf065fcdf776b38cf3d72d10c113
faster secure two - party computation using garbled circuits secure two - party computation enables two parties to evaluate a function cooperatively without revealing to either party anything beyond the function ’s output . the garbled - circuit technique , a generic approach to secure two - party computation for semi - honest participants , was developed by yao in the 1980s , but has been viewed as being of limited practical significance due to its inefficiency . we demonstrate several techniques for improving the running time and memory requirements of the garbled - circuit technique , resulting in an implementation of generic secure two - party computation that is significantly faster than any previously reported while also scaling to arbitrarily large circuits . we validate our approach by demonstrating secure computation of circuits with over 109 gates at a rate of roughly 10 μs per garbled gate , and showing order - of - magnitude improvements over the best previous privacy - preserving protocols for computing hamming distance , levenshtein distance , smith - waterman genome alignment , and aes .
RANK = 4; score = 0.2946719615596834; correct = False; id = cdb352365403a6ad09cdd00232def337df0a1b96
pcf : a portable circuit format for scalable two - party secure computation a secure computation protocol for a function f ( x , y ) must leak no information about inputs x , y during its execution ; thus it is imperative to compute the function f in a data - oblivious manner . traditionally , this has been accomplished by compiling f into a boolean circuit . previous approaches , however , have scaled poorly as the circuit size increases . we present a new approach to compiling such circuits that is substantially more efficient than prior work . our approach is based on online circuit compression and lazy gate generation . we implemented an optimizing compiler for this new representation of circuits , and evaluated the use of this representation in two secure computation environments . our evaluation demonstrates the utility of this approach , allowing us to scale secure computation beyond any previous system while requiring substantially less cpu time and disk space . in our largest test , we evaluate an rsa-1024 signature function with more than 42 billion gates , that was generated and optimized using our compiler . with our techniques , the bottleneck in secure computation lies with the cryptographic primitives , not the compilation or storage of circuits .
RANK = 5; score = 0.27910102720920354; correct = False; id = a853e0842d74fa3ff146f45ea7f2ed52dac08d1a
fast and secure three - party computation : the garbled circuit approach many deployments of secure multi - party computation ( mpc ) in practice have used information - theoretic three - party protocols that tolerate a single , semi - honest corrupt party , since these protocols enjoy very high efficiency . we propose a new approach for secure three - party computation ( 3pc ) that improves security while maintaining practical efficiency that is competitive with traditional information - theoretic protocols . our protocol is based on garbled circuits and provides security against a single , malicious corrupt party . unlike information - theoretic 3pc protocols , ours uses a constant number of rounds . our protocol only uses inexpensive symmetric - key cryptography : hash functions , block ciphers , pseudorandom generators ( in particular , no oblivious transfers ) and has performance that is comparable to that of yao 's ( semi - honest ) 2pc protocol . we demonstrate the practicality of our protocol with an implementation based on the justgarble framework of bellare et al . ( s&p 2013 ) . the implementation incorporates various optimizations including the most recent techniques for efficient circuit garbling . we perform experiments on several benchmarking circuits , in different setups . our experiments confirm that , despite providing a more demanding security guarantee , our protocol has performance comparable to existing information - theoretic 3pc .
RANK = 6; score = 0.2584208585232813; correct = False; id = 26e47313c3aa208026a4e9a92e76e53aa7fc8205
zero - knowledge using garbled circuits : how to prove non - algebraic statements efficiently zero - knowledge protocols are one of the fundamental concepts in modern cryptography and have countless applications . however , after more than 30 years from their introduction , there are only very few languages ( essentially those with a group structure ) for which we can construct zero - knowledge protocols that are efficient enough to be used in practice . in this paper we address the problem of how to construct efficient zero - knowledge protocols for generic languages and we propose a protocol based on yao 's garbled circuit technique . the motivation for our work is that in many cryptographic applications it is useful to be able to prove efficiently statements of the form e.g. , " i know x s.t.y = sha-256(x ) " for a common input y ( or other " unstructured " languages ) , but no efficient protocols for this task are currently known . it is clear that zero - knowledge is a subset of secure two - party computation ( i.e. , any protocol for generic secure computation can be used to do zero - knowledge ) . the main contribution of this paper is to construct an efficient protocol for the special case of secure two - party computation where only one party has input ( like in the zero - knowledge case ) . the protocol achieves active security and is essentially only twice as slow as the passive secure version of yao 's garbled circuit protocol . this is a great improvement with respect to the cut - n - choose technique to make yao 's protocol actively secure , where the complexity grows linearly with the security parameter .
RANK = 7; score = 0.2358441742606599; correct = False; id = 13e622fca1a6b52aa85898e260f9455e4ba0d94b
high - throughput semi - honest secure three - party computation with an honest majority in this paper , we describe a new information - theoretic protocol ( and a computationally - secure variant ) for secure three - party computation with an honest majority . the protocol has very minimal computation and communication ; for boolean circuits , each party sends only a single bit for every and gate ( and nothing is sent for xor gates ) . our protocol is ( simulation - based ) secure in the presence of semi - honest adversaries , and achieves privacy in the client / server model in the presence of malicious adversaries . on a cluster of three 20-core servers with a 10gbps connection , the implementation of our protocol carries out over 1.3 million aes computations per second , which involves processing over 7 billion gates per second . in addition , we developed a kerberos extension that replaces the ticket - granting - ticket encryption on the key distribution center ( kdc ) in mit - kerberos with our protocol , using keys/ passwords that are shared between the servers . this enables the use of kerberos while protecting passwords . our implementation is able to support a login storm of over 35,000 logins per second , which suffices even for very large organizations . our work demonstrates that high - throughput secure computation is possible on standard hardware .
RANK = 8; score = 0.2339681368547189; correct = False; id = 1eb0b401e7dbd8a4e638243713b39fffc991fe9f
fast two - party secure computation with minimal assumptions almost all existing protocols for secure two - party computation require a specific hardness assumption , such as ddh , discrete logarithm , or a random oracle , even after assuming oracle access to the oblivious transfer functionality for their correctness and/or efficiency . we propose and implement a yao - based protocol that is secure against malicious adversaries and enjoys the following benefits : it requires the minimal hardness assumption , i.e. , ots ; it uses 10 rounds of communication plus ot rounds ; it has the optimal overhead complexity ( for an approach that uses the circuit - level cut - and - choose technique ) ; and it is embarrassingly parallelizable in the sense that each circuit can be processed in a pipelined manner , and all circuits can be processed in parallel . to achieve these properties , we describe novel solutions for the three main obstacles for achieving security against malicious adversaries in a cut - and - choose garbled - circuit protocol . we propose an efficient proof to establish the generator 's output authenticity ; we suggest the use of an auxiliary circuit that computes a hash to ensure the generator 's input consistency ; and we advance the performance of pinkas and lindell 's state - of - the - art approach for handling the selective failure attack . not only does our protocol require weaker cryptographic assumptions , but our implementation of this protocol also demonstrates a several factor improvement over the best prior work which relies on specific number - theoretic assumptions . thus , we show that performance does not require specific algebraic assumptions .
RANK = 9; score = 0.22527843059106462; correct = False; id = 2aa24ddd5c4eea28fc3b751fb5915c01d9337626
fast garbling of circuits under standard assumptions protocols for secure computation enable mutually distrustful parties to jointly compute on their private inputs without revealing anything but the result . over recent years , secure computation has become practical and considerable effort has been made to make it more and more efficient . a highly important tool in the design of two - party protocols is yao 's garbled circuit construction ( yao 1986 ) , and multiple optimizations on this primitive have led to performance improvements of orders of magnitude over the last years . however , many of these improvements come at the price of making very strong assumptions on the underlying cryptographic primitives being used ( e.g. , that aes is secure for related keys , that it is circular secure , and even that it behaves like a random permutation when keyed with a public fixed key ) . the justification behind making these strong assumptions has been that otherwise it is not possible to achieve fast garbling and thus fast secure computation . in this paper , we take a step back and examine whether it is really the case that such strong assumptions are needed . we provide new methods for garbling that are secure solely under the assumption that the primitive used ( e.g. , aes ) is a pseudorandom function . our results show that in many cases , the penalty incurred is not significant , and so a more conservative approach to the assumptions being used can be adopted .
RANK = 10; score = 0.22328547839497723; correct = False; id = b273f47f97fc3f1ed922c3effda9ab88c52a1680
secure two - party computations in ansi c the practical application of secure two - party computation is hindered by the difficulty to implement secure computation protocols . while recent work has proposed very simple programming languages which can be used to specify secure computations , it is still difficult for practitioners to use them , and cumbersome to translate existing source code into this format . similarly , the manual construction of two - party computation protocols , in particular ones based on the approach of garbled circuits , is labor intensive and error - prone . the central contribution of the current paper is a tool which achieves secure two - party computation for ansi c. our work is based on a combination of model checking techniques and two - party computation based on garbled circuits . our key insight is a nonstandard use of the bit - precise model checker cbmc which enables us to translate c programs into equivalent boolean circuits . to this end , we modify the standard cbmc translation from programs into boolean formulas whose variables correspond to the memory bits manipulated by the program . as cbmc attempts to minimize the size of the formulas , the circuits obtained by our tool chain are also size efficient ; to improve the efficiency of the garbled circuit evaluation , we perform optimizations on the circuits . experimental results with the new tool cbmc - gc demonstrate the practical usefulness of our approach .
RANK = 11; score = 0.22036125655965985; correct = False; id = 1c36af6daabcf5558c44656d567f8bd645aee29d
automating efficient ram - model secure computation ram - model secure computation addresses the inherent limitations of circuit - model secure computation considered in almost all previous work . here , we describe the first automated approach for ram - model secure computation in the semi - honest model . we define an intermediate representation called scvm and a corresponding type system suited for ram - model secure computation . leveraging compile - time optimizations , our approach achieves order - of - magnitude speedups compared to both circuit - model secure computation and the state - of - art ram - model secure computation .
RANK = 12; score = 0.21911044137786884; correct = False; id = 15964bef0c5a10420ccf44f4e02f4905aa9d85d0
quid - pro - quo - tocols : strengthening semi - honest protocols with dual execution known protocols for secure two - party computation that are designed to provide full security against malicious behavior are significantly less efficient than protocols intended only to thwart semi - honest adversaries . we present a concrete design and implementation of protocols achieving security guarantees that are much stronger than are possible with semi - honest protocols , at minimal extra cost . specifically , we consider protocols in which a malicious adversary may learn a single ( arbitrary ) bit of additional information about the honest party 's input . correctness of the honest party 's output is still guaranteed . adapting prior work of mohassel and franklin , the basic idea in our protocols is to conduct two separate runs of a ( specific ) semi - honest , garbled - circuit protocol , with the parties swapping roles , followed by an inexpensive secure equality test . we provide a rigorous definition and prove that this protocol leaks no more than one additional bit against a malicious adversary . in addition , we propose some heuristic enhancements to reduce the overall information a cheating adversary learns . our experiments show that protocols meeting this security level can be implemented at cost very close to that of protocols that only achieve semi - honest security . our results indicate that this model enables the large - scale , practical applications possible within the semi - honest security model , while providing dramatically stronger security guarantees .
RANK = 13; score = 0.21894303822784325; correct = False; id = 1b7d5d4e6ffb922e5b829a27d2756c1a8d969ac5
a framework for secure computations with two non - colluding servers and multiple clients , applied to recommendations we provide a generic framework that , with the help of a preprocessing phase that is independent of the inputs of the users , allows an arbitrary number of users to securely outsource a computation to two non - colluding external servers . our approach is shown to be provably secure in an adversarial model where one of the servers may arbitrarily deviate from the protocol specification , as well as employ an arbitrary number of dummy users . we use these techniques to implement a secure recommender system based on collaborative filtering that becomes more secure , and significantly more efficient than previously known implementations of such systems , when the preprocessing efforts are excluded . we suggest different alternatives for preprocessing , and discuss their merits and demerits .
RANK = 14; score = 0.21598161640292338; correct = False; id = 862267a2c5618865dc6ec1a63e1efdb64efcbd4b
automatic generation of two - party computations we present the design and implementation of a compiler that automatically generates protocols that perform two - party computations . the input to our protocol is the specification of a computation with secret inputs ( e.g. , a signature algorithm ) expressed using operations in the field zq of integers modulo a prime q and in the multiplicative subgroup of order q in z*p for q|p-1 with generator g. the output of our compiler is an implementation of each party in a two - party protocol to perform the same computation securely , i.e. , so that both parties can together compute the function but neither can alone . the protocols generated by our compiler are provably secure , in that their strength can be reduced to that of the original cryptographic computation via simulation arguments . our compiler can be applied to various cryptographic primitives ( e.g. , signature schemes , encryption schemes , oblivious transfer protocols ) and other protocols that employ a trusted party ( e.g. , key retrieval , key distribution ) .
RANK = 15; score = 0.2151237150203141; correct = False; id = bfb2ad06bfbe2bff0cd04b3e0a90e3bf5ebd3dd5
improvements to secure computation with penalties motivated by the impossibility of achieving fairness in secure computation [ cleve , stoc 1986 ] , recent works study a model of fairness in which an adversarial party that aborts on receiving output is forced to pay a mutually predefined monetary penalty to every other party that did not receive the output . these works show how to design protocols for < i > secure computation with penalties</i > that tolerate an arbitrary number of corruptions . in this work , we improve the efficiency of protocols for secure computation with penalties in a hybrid model where parties have access to the " claim - or - refund " transaction functionality . our first improvement is for the ladder protocol of bentov and kumaresan ( crypto 2014 ) where we improve the dependence of the script complexity of the protocol ( which corresponds to miner verification load and also space on the blockchain ) on the number of parties from quadratic to linear ( and in particular , is completely independent of the underlying function ) . our second improvement is for the see - saw protocol of kumaresan et al . ( ccs 2015 ) where we reduce the total number of claim - or - refund transactions and also the script complexity from quadratic to linear in the number of parties . we also present a ' dual - mode ' protocol that offers different guarantees depending on the number of corrupt parties : ( 1 ) when s < n/2 parties are corrupt , this protocol guarantees fairness ( i.e. , either all parties get the output or none do ) , and ( 2 ) when t > n/2 parties are corrupt , this protocol guarantees fairness with penalties ( i.e. , if the adversary gets the output , then either the honest parties get output as well or they get compensation via penalizing the adversary ) . the above protocol works as long as t+s < n , matching the bound obtained for secure computation protocols in the standard model ( i.e. , replacing " fairness with penalties " with " security - with - abort " ( full security except fairness ) ) by ishai et al . ( sicomp 2011 ) .
RANK = 16; score = 0.21303503192529333; correct = False; id = 3592de47a990de95ac38e9f22349d48b6a2945ae
amortizing secure computation with penalties motivated by the impossibility of achieving fairness in secure computation [ cleve , stoc 1986 ] , recent works study a model of fairness in which an adversarial party that aborts on receiving output is forced to pay a mutually predefined monetary penalty to every other party that did not receive the output . these works show how to design protocols for secure computation with penalties that guarantees that either fairness is guaranteed or that each honest party obtains a monetary penalty from the adversary . protocols for this task are typically designed in an hybrid model where parties have access to a " claim - or - refund " transaction functionality denote fcr*. in this work , we obtain improvements on the efficiency of these constructions by amortizing the cost over multiple executions of secure computation with penalties . more precisely , for computational security parameter λ , we design a protocol that implements l = poly}(λ ) instances of secure computation with penalties where the total number of calls to fcr * is independent of l.
RANK = 17; score = 0.21245417448102435; correct = False; id = 1844578c5f75884baa4931d2987cab10d70bd304
tasty : tool for automating secure two - party computations secure two - party computation allows two untrusting parties to jointly compute an arbitrary function on their respective private inputs while revealing no information beyond the outcome . existing cryptographic compilers can automatically generate secure computation protocols from high - level specifications , but are often limited in their use and efficiency of generated protocols as they are based on either garbled circuits or ( additively ) homomorphic encryption only . in this paper we present tasty , a novel tool for automating , i.e. , describing , generating , executing , benchmarking , and comparing , efficient secure two - party computation protocols . tasty is a new compiler that can generate protocols based on homomorphic encryption and efficient garbled circuits as well as combinations of both , which often yields the most efficient protocols available today . the user provides a high - level description of the computations to be performed on encrypted data in a domain - specific language . this is automatically transformed into a protocol . tasty provides most recent techniques and optimizations for practical secure two - party computation with low online latency . moreover , it allows to efficiently evaluate circuits generated by the well - known fairplay compiler . we use tasty to compare protocols for secure multiplication based on homomorphic encryption with those based on garbled circuits and highly efficient karatsuba multiplication . further , we show how tasty improves the online latency for securely evaluating the aes functionality by an order of magnitude compared to previous software implementations . tasty allows to automatically generate efficient secure protocols for many privacy - preserving applications where we consider the use cases for private set intersection and face recognition protocols .
RANK = 18; score = 0.2106821965933929; correct = False; id = 90b8cada459b26bb3efca5ed0d868fb8f15a6e11
attribute - based key exchange with general policies attribute - based methods provide authorization to parties based on whether their set of attributes ( e.g. , age , organization , etc . ) fulfills a policy . in attribute - based encryption ( abe ) , authorized parties can decrypt , and in attribute - based credentials ( abcs ) , authorized parties can authenticate themselves . in this paper , we combine elements of abe and abcs together with garbled circuits to construct attribute - based key exchange ( abke ) . our focus is on an interactive solution involving a client that holds a certificate ( issued by an authority ) vouching for that client 's attributes and a server that holds a policy computable on such a set of attributes . the goal is for the server to establish a shared key with the client but only if the client 's certified attributes satisfy the policy . our solution enjoys strong privacy guarantees for both the client and the server , including attribute privacy and unlinkability of client sessions . our main contribution is a construction of abke for arbitrary circuits with high ( concrete ) efficiency . specifically , we support general policies expressible as boolean circuits computed on a set of attributes . even for policies containing hundreds of thousands of gates the performance cost is dominated by two pairing computations per policy input . put another way , for a similar cost to prior abe / abc solutions , which can only support small formulas efficiently , we can support vastly richer policies . we implemented our solution and report on its performance . for policies with 100,000 gates and 200 inputs over a realistic network , the server and client spend 957 ms and 176 ms on computation , respectively . when using offline preprocessing and batch signature verification , this drops to only 243 ms and 97 ms .
RANK = 19; score = 0.20945948172891599; correct = True; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
RANK = 20; score = 0.20815288313726593; correct = False; id = 18b7880edc5dead10795105ae600ca19ba15f8c5
secure two - party k - means clustering the k - means clustering problem is one of the most - explored problems in data mining to date . with the advent of protocols that have proven to be successful in performing single database clustering , the focus has shifted in recent years to the question of how to extend the single database protocols to a multiple database setting . to date there have been numerous attempts to create specific multiparty k - means clustering protocols that protect the privacy of each database , but according to the standard cryptographic definitions of " privacy - protection , " so far all such attempts have fallen short of providing adequate privacy . in this paper we describe a two - party k - means clustering protocol that guarantees privacy , and is more efficient than utilizing a general multiparty " compiler " to achieve the same task . in particular , a main contribution of our result is a way to compute efficiently multiple iterations of k - means clustering without revealing the intermediate values . to achieve this , we show two techniques : to perform two - party division and to sample uniformly at random from an unknown domain size ; the resulting division protocol and random value protocol are of use to any protocol that requires the secure computation of a quotient or random sampling . our techniques can be realized based on the existence of any semantically secure homomorphic encryption scheme . for concreteness , we describe our protocol based on paillier homomorphic encryption scheme ( see [ 21 ] ) . we will also demonstrate that our protocol is efficient in terms of communication , remaining competitive with existing protocols ( such as [ 13 ] ) that fail to protect privacy .

RANKING 1963
QUERY
uw - finsent at semeval-2017 task 5 : sentiment analysis on financial news headlines using training dataset augmentation this paper discusses the approach taken by the uwaterloo team to arrive at a solution for the fine - grained sentiment analysis problem posed by task 5 of semeval 2017 . the paper describes the document vectorization and sentiment score prediction techniques used , as well as the design and implementation decisions taken while building the system for this task . the system uses text vectorization models , such as n - gram , tf - idf and paragraph embeddings , coupled with regression model variants to predict the sentiment scores . amongst the methods examined , unigrams and bigrams coupled with simple linear regression obtained the best baseline accuracy . the paper also explores data augmentation methods to supplement the training dataset . this system was designed for subtask 2 ( news statements and headlines ) .
First cited at 2370
TOP CITED PAPERS
RANK 2370
baselines and bigrams : simple , good sentiment and topic classification the existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time . such comparisons , however , are rarely made . most researchers only report results from their own experiments , a practice that allows lack of overall improvement to go unnoticed . in this paper , we analyze results achieved on the trec ad - hoc , web , terabyte , and robust collections as reported in sigir ( 1998–2008 ) and cikm ( 2004–2008 ) . dozens of individual published experiments report effectiveness improvements , and often claim statistical significance . however , there is little evidence of improvement in ad - hoc retrieval technology over the past decade . baselines are generally weak , often being below the median original trec system . and in only a handful of experiments is the score of the best trec automatic run exceeded . given this finding , we question the value of achieving even a statistically significant result over a weak baseline . we propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress , or at least prevent the lack of it from going unnoticed . we describe an online database of retrieval runs that facilitates such a practice .
TOP UNCITED PAPERS
RANK 1
commit at semeval-2016 task 5 : sentiment analysis with rhetorical structure theory this paper reports our submission to the aspect - based sentiment analysis task of semeval 2016 . it covers the prediction of sentiment for a given set of aspects ( e.g. , subtask 1 , slot 2 ) for the english language using discourse analysis . to that end , a discourse parser implementing the rhetorical structure theory is employed and the resulting information is used to determine the context of each aspect , as well as to compute the expressed sentiment in that context by weighing the discourse relations between words . while discourse analysis yields high level linguistic information that can be used to better predict sentiment , the proposed algorithm does not yet stack up to the high - performing machine learning approaches that are commonly exploited for this task .
RANK 2
umduluth - cs8761 - 12 : a novel machine learning approach for aspect based sentiment analysis this paper provides a detailed description of the approach of our system for the aspectbased sentiment analysis task of semeval2015 . the task is to identify the aspect category ( entity and attribute pair ) , opinion target and sentiment of the reviews . for the in - domain subtask that is provided with the training data , the system is developed using a supervised technique support vector machine and for the out - of - domain subtask for which the training data is not provided , it is implemented based on the sentiment score of the vocabulary . for in - domain subtask , our system is developed specifically for restaurant data .
RANK 3
sentisys at semeval-2016 task 4 : feature - based system for sentiment analysis in twitter this paper describes our sentiment analysis system which has been built for sentiment analysis in twitter task of semeval-2016 . we have used a logistic regression classifier with different groups of features . this system is an improvement to our previous system lsislif in semeval-2015 after removing some features and adding new features extracted from a new automatic constructed sentiment lexicon .
TOP 20
RANK = 1; score = 0.2841128066712007; correct = False; id = c3f91f90f45e935f8ec369172584ea69c6b405c7
commit at semeval-2016 task 5 : sentiment analysis with rhetorical structure theory this paper reports our submission to the aspect - based sentiment analysis task of semeval 2016 . it covers the prediction of sentiment for a given set of aspects ( e.g. , subtask 1 , slot 2 ) for the english language using discourse analysis . to that end , a discourse parser implementing the rhetorical structure theory is employed and the resulting information is used to determine the context of each aspect , as well as to compute the expressed sentiment in that context by weighing the discourse relations between words . while discourse analysis yields high level linguistic information that can be used to better predict sentiment , the proposed algorithm does not yet stack up to the high - performing machine learning approaches that are commonly exploited for this task .
RANK = 2; score = 0.28134537342404264; correct = False; id = e62a527aa26625980d89c89301989c6f4d53add7
umduluth - cs8761 - 12 : a novel machine learning approach for aspect based sentiment analysis this paper provides a detailed description of the approach of our system for the aspectbased sentiment analysis task of semeval2015 . the task is to identify the aspect category ( entity and attribute pair ) , opinion target and sentiment of the reviews . for the in - domain subtask that is provided with the training data , the system is developed using a supervised technique support vector machine and for the out - of - domain subtask for which the training data is not provided , it is implemented based on the sentiment score of the vocabulary . for in - domain subtask , our system is developed specifically for restaurant data .
RANK = 3; score = 0.2803498458686092; correct = False; id = 9f192a2395081ede063376e76df5cecbcbe48d5f
sentisys at semeval-2016 task 4 : feature - based system for sentiment analysis in twitter this paper describes our sentiment analysis system which has been built for sentiment analysis in twitter task of semeval-2016 . we have used a logistic regression classifier with different groups of features . this system is an improvement to our previous system lsislif in semeval-2015 after removing some features and adding new features extracted from a new automatic constructed sentiment lexicon .
RANK = 4; score = 0.27324902060884615; correct = False; id = 8a86507f78e5b71c31854fbc4520376fba3c0196
inesc - id : a regression model for large scale twitter sentiment lexicon induction we present the approach followed by inescid in the semeval 2015 twitter sentiment analysis challenge , subtask e. the goal was to determine the strength of the association of twitter terms with positive sentiment . using two labeled lexicons , we trained a regression model to predict the sentiment polarity and intensity of words and phrases . terms were represented as word embeddings induced in an unsupervised fashion from a corpus of tweets . our system attained the top ranking submission , attesting the general adequacy of the proposed approach .
RANK = 5; score = 0.27274189617230327; correct = False; id = 8208d5644984a8ef5d10c9bc2f898f539d189263
semeval-2015 task 10 : sentiment analysis in twitter in this paper , we describe the 2015 iteration of the semeval shared task on sentiment analysis in twitter . this was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years . this year ’s shared task competition consisted of five sentiment prediction subtasks . two were reruns from previous years : ( a ) sentiment expressed by a phrase in the context of a tweet , and ( b ) overall sentiment of a tweet . we further included three new subtasks asking to predict ( c ) the sentiment towards a topic in a single tweet , ( d ) the overall sentiment towards a topic in a set of tweets , and ( e ) the degree of prior polarity of a phrase .
RANK = 6; score = 0.26995914006103405; correct = False; id = 90caafb562db55f62b11cd6618f53eda44d8e2cf
unipi at semeval-2016 task 4 : convolutional neural networks for sentiment classification the paper describes our submission to the task on sentiment analysis on twitter at semeval 2016 . the approach is based on a deep learning architecture using convolutional neural networks . the approach used only word embeddings as features . the submission used embeddings created from a corpus of news articles . we report on further experiments using embeddings built for a corpus of tweets as well as sentiment specific word embeddings obtained by distant supervision .
RANK = 7; score = 0.2697743615016809; correct = False; id = e6aef043a873d32106129b65ad3a180cd8a5fda2
teragram : rule - based detection of sentiment phrases using sas sentiment analysis for semeval-2013 task 2 , a and b ( sentiment analysis in twitter ) , we use a rulebased pattern matching system that is based on an existing ‘ domain independent’ sentiment taxonomy for english , essentially a highly phrasal sentiment lexicon . we have made some modifications to our set of rules , based on what we found in the annotated training data that was made available for the task . the resulting system scores competitively , especially on task b.
RANK = 8; score = 0.2651129576956607; correct = False; id = 4949cea065dce681464862799d7a98b0128da4f8
kea : expression - level sentiment analysis from twitter data this paper describes an expression - level sentiment detection system that participated in the subtask a of semeval-2013 task 2 : sentiment analysis in twitter . our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive , negative or neutral . the proposed approach helps to understand the relevant features that contribute most in this classification task .
RANK = 9; score = 0.257595635850091; correct = False; id = 9a965844b19956032699015fc12f13ecdb277533
how can nlp tasks mutually benefit sentiment analysis ? a holistic approach to sentiment analysis existing opinion analysis techniques rely on the clues within the sentence that focus on the sentiment analysis task itself . however , the sentiment analysis task is not isolated from other nlp tasks ( co - reference resolution , entity linking , etc ) but they can benefit each other . in this paper , we define dependencies between sentiment analysis and other tasks , and express the dependencies in first order logic rules regardless of the representations of different tasks . the conceptual framework proposed in this paper using such dependency rules as constraints aims at exploiting information outside the sentence and outside the document to improve sentiment analysis . further , the framework allows exception to the rules .
RANK = 10; score = 0.25564373381102745; correct = False; id = 4a0c703fe51f60ae4e9b980b05e60046897a4b8c
open domain targeted sentiment we propose a novel approach to sentiment analysis for a low resource setting . the intuition behind this work is that sentiment expressed towards an entity , targeted sentiment , may be viewed as a span of sentiment expressed across the entity . this representation allows us to model sentiment detection as a sequence tagging problem , jointly discovering people and organizations along with whether there is sentiment directed towards them . we compare performance in both spanish and english on microblog data , using only a sentiment lexicon as an external resource . by leveraging linguisticallyinformed features within conditional random fields ( crfs ) trained to minimize empirical risk , our best models in spanish significantly outperform a strong baseline , and reach around 90 % accuracy on the combined task of named entity recognition and sentiment prediction . our models in english , trained on a much smaller dataset , are not yet statistically significant against their baselines .
RANK = 11; score = 0.2547311844823443; correct = False; id = f8bbe922912533b5da03cbbd2b293e234004936c
mdsent at semeval-2016 task 4 : a supervised system for message polarity classification this paper describes our system submitted for the sentiment analysis in twitter task of semeval-2016 , and specifically for the message polarity classification subtask . we used a system that combines convolutional neural networks and logistic regression for sentiment prediction , where the former makes use of embedding features while the later utilizes various features like lexicons and dictionaries .
RANK = 12; score = 0.25013312134304705; correct = False; id = 8797763a3dedce3f40e4ee69efdb3b55d6f985b8
aueb.twitter.sentiment at semeval-2016 task 4 : a weighted ensemble of svms for twitter sentiment analysis this paper describes the system with which we participated in semeval-2016 task 4 ( sentiment analysis in twitter ) and specifically the message polarity classification subtask . our system is a weighted ensemble of two systems . the first one is based on a previous sentiment analysis system and uses manually crafted features . the second system of our ensemble uses features based on word embeddings . our ensemble was ranked 5th among 34 teams . the source code of our system is publicly available .
RANK = 13; score = 0.2487043754321608; correct = False; id = 565161f35c102a39f65c572f841c961848170c63
uwb at semeval-2016 task 5 : aspect based sentiment analysis this paper describes our system used in the aspect based sentiment analysis ( absa ) task of semeval 2016 . our system uses maximum entropy classifier for the aspect category detection and for the sentiment polarity task . conditional random fields ( crf ) are used for opinion target extraction . we achieve state - of - the - art results in 9 experiments among the constrained systems and in 2 experiments among the unconstrained systems .
RANK = 14; score = 0.24821189705129865; correct = False; id = f32727af88ffcf79e717c58a8d6ef5e5da1de16f
uwb at semeval-2016 task 7 : novel method for automatic sentiment intensity determination we present a novel method for determining sentiment intensity . the main goal is to assign a phrase a score from 0 to 1 which indicates the strength of its association with positive sentiment . the proposed model uses a rich set of features with gaussian processes regression model that computes the final score . the system was evaluated on the data from 7th task of semeval 2016 . our regression model trained on the development data reached kendall rank correlation of 0.659 on general english phrases and 0.414 on english twitter test data .
RANK = 15; score = 0.2444305274865824; correct = False; id = 830746182677f9c1dc27c87222e01a0bb0ca4dab
dive deeper : deep semantics for sentiment analysis this paper illustrates the use of deep semantic processing for sentiment analysis . existing methods for sentiment analysis use supervised approaches which take into account all the subjective words and or phrases . due to this , the fact that not all of these words and phrases actually contribute to the overall sentiment of the text is ignored . we propose an unsupervised rule - based approach using deep semantic processing to identify only relevant subjective terms . we generate a unl ( universal networking language ) graph for the input text . rules are applied on the graph to extract relevant terms . the sentiment expressed in these terms is used to figure out the overall sentiment of the text . results on binary sentiment classification have shown promising results .
RANK = 16; score = 0.24416698117170232; correct = False; id = e87ea35ba6b05ae7c94e160fd7c7947792618d36
effect of using regression on class confidence scores in sentiment analysis of twitter data in this study , we aim to test our hypothesis that confidence scores of sentiment values of tweets aid in classification of sentiment . we used several feature sets consisting of lexical features , emoticons , features based on sentiment scores and combination of lexical and sentiment features . since our dataset includes confidence scores of real numbers in [ 0 - 1 ] range , we employ regression analysis on each class of sentiments . we determine the class label of a tweet by looking at the maximum of the confidence scores assigned to it by these regressors . we test the results against classification results obtained by converting the confidence scores into discrete labels . thus , the strength of sentiment is ignored . our expectation was that taking the strength of sentiment into consideration would improve the classification results . contrary to our expectations , our results indicate that using classification on discrete class labels and ignoring sentiment strength perform similar to using regression on continuous confidence scores .
RANK = 17; score = 0.24177422728692005; correct = False; id = 1b1ba7ad4adb86be202d0156fffee43f1bfbf5d0
iiit - h at semeval 2015 : twitter sentiment analysis - the good , the bad and the neutral ! this paper describes the system that was submitted to semeval2015 task 10 : sentiment analysis in twitter . we participated in subtask b : message polarity classification . the task is a message level classification of tweets into positive , negative and neutral sentiments . our model is primarily a supervised one which consists of well designed features fed into an svm classifier . in previous runs of this task , it was found that lexicons played an important role in determining the sentiment of a tweet . we use existing lexicons to extract lexicon specific features . the lexicon based features are further augmented by tweet specific features . we also improve our system by using acronym and emoticon dictionaries . the proposed system achieves an f1 score of 59.83 and 67.04 on the test data and progress data respectively . this placed us at the 18 position for the test dataset and the 16 position for the progress test dataset .
RANK = 18; score = 0.240814339407831; correct = False; id = 577621509af4f8e343a4d94504a52fb32e3fe882
klue : simple and robust methods for polarity classification this paper describes our approach to the semeval-2013 task on “ sentiment analysis in twitter ” . we use simple bag - of - words models , a freely available sentiment dictionary automatically extended with distributionally similar terms , as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms . the resulting system is resource - lean , making it relatively independent of a specific language . despite its simplicity , the system achieves competitive accuracies of 0.70–0.72 in detecting the sentiment of text messages . we also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message .
RANK = 19; score = 0.23967739889061598; correct = False; id = adfd48db819ce54375d9647b32b7596a3107d57e
improving twitter sentiment analysis with topic - based mixture modeling and semi - supervised training in this paper , we present multiple approaches to improve sentiment analysis on twitter data . we first establish a state - of - the - art baseline with a rich feature set . then we build a topic - based sentiment mixture model with topic - specific data in a semi - supervised training framework . the topic information is generated through topic modeling based on an efficient implementation of latent dirichlet allocation ( lda ) . the proposed sentiment model outperforms the top system in the task of sentiment analysis in twitter in semeval-2013 in terms of averaged f scores .
RANK = 20; score = 0.23921631009034475; correct = False; id = 02acc46ccd36ce4fccbdc594d3e8992874e69178
semi - supervised latent variable models for sentence - level sentiment analysis we derive two variants of a semi - supervised model for fine - grained sentiment analysis . both models leverage abundant natural supervision in the form of review ratings , as well as a small amount of manually crafted sentence labels , to learn sentence - level sentiment classifiers . the proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart . this allows for highly efficient estimation and inference algorithms with rich feature definitions . we describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence - level sentiment analysis compared to all baselines . 1 sentence - level sentiment analysis in this paper , we demonstrate how combining coarse - grained and fine - grained supervision benefits sentence - level sentiment analysis – an important task in the field of opinion classification and retrieval ( pang and lee , 2008 ) . typical supervised learning approaches to sentence - level sentiment analysis rely on sentence - level supervision . while such fine - grained supervision rarely exist naturally , and thus requires labor intensive manual annotation effort ( wiebe et al . , 2005 ) , coarse - grained supervision is naturally abundant in the form of online review ratings . this coarse - grained supervision is , of course , less informative compared to fine - grained supervision , however , by combining a small amount of sentence - level supervision with a large amount of document - level supervision , we are able to substantially improve on the sentence - level classification task . our work combines two strands of research : models for sentiment analysis that take document structure into account ; and models that use latent variables to learn unobserved phenomena from that which can be observed . exploiting document structure for sentiment analysis has attracted research attention since the early work of pang and lee ( 2004 ) , who performed minimal cuts in a sentence graph to select subjective sentences . mcdonald et al . ( 2007 ) later showed that jointly learning fine - grained ( sentence ) and coarsegrained ( document ) sentiment improves predictions at both levels . more recently , yessenalina et al . ( 2010 ) described how sentence - level latent variables can be used to improve document - level prediction and nakagawa et al . ( 2010 ) used latent variables over syntactic dependency trees to improve sentence - level prediction , using only labeled sentences for training . in a similar vein , sauper et al . ( 2010 ) integrated generative content structure models with discriminative models for multi - aspect sentiment summarization and ranking . these approaches all rely on the availability of fine - grained annotations , but täckström and mcdonald ( 2011 ) showed that latent variables can be used to learn fine - grained sentiment using only coarse - grained supervision . while this model was shown to beat a set of natural baselines with quite a wide margin , it has its shortcomings . most notably , due to the loose constraints provided by the coarse supervision , it tends to only predict the two dominant fine - grained sentiment categories well for each document sentiment category , so that almost all sentences in positive documents are deemed positive or neutral , and vice versa for negative documents . as a way of overcoming these shortcomings , we propose to fuse a coarsely supervised model with a fully supervised model . below , we describe two ways of achieving such a combined model in the framework of structured conditional latent variable models . contrary to ( generative ) topic models ( mei et al . , 2007 ; titov and

RANKING 714
QUERY
a systematic study of neural discourse models for implicit discourse relation inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing . many neural network models have been proposed to tackle this problem . however , the comparison for this task is not unified , so we could hardly draw clear conclusions about the effectiveness of various architectures . here , we propose neural network models that are based on feedforward and long - short term memory architecture and systematically study the effects of varying structures . to our surprise , the best - configured feedforward architecture outperforms lstm - based model in most cases despite thorough tuning . further , we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective . for the first time for this task , we compile and publish outputs from previous neural and nonneural systems to establish the standard for further comparison .
First cited at 27
TOP CITED PAPERS
RANK 27
when are tree structures necessary for deep learning of representations recursive neural models , which use syntactic parse trees to recursively generate representations bottom - up , are a popular architecture . but there have not been rigorous evaluations showing for exactly which tasks this syntax - based method is appropriate . in this paper we benchmark recursive neural models against sequential recurrent neural models ( simple recurrent and lstm models ) , enforcing apples - to - apples comparison as much as possible . we investigate 4 tasks : ( 1 ) sentiment classification at the sentence level and phrase level ; ( 2 ) matching questions to answer - phrases ; ( 3 ) discourse parsing ; ( 4 ) semantic relation extraction ( e.g. , component - whole between nouns ) . our goal is to understand better when , and why , recursive models can outperform simpler models . we find that recursive models help mainly on tasks ( like semantic relation extraction ) that require associating headwords across a long distance , particularly on very long sequences . we then introduce a method for allowing recurrent models to achieve similar performance : breaking long sentences into clause - like units at punctuation and processing them separately before combining . our results thus help understand the limitations of both classes of models , and suggest directions for improving recurrent models .
RANK 56
automatic sense prediction for implicit discourse relations in text we present a series of experiments on automatically identifying the sense of implicit discourse relations , i.e. relations that are not marked with a discourse connective such as “ but ” or “ because ” . we work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses . we use several linguistically informed features , including polarity tags , levin verb classes , length of verb phrases , modality , context , and lexical features . in addition , we revisit past approaches using lexical pairs from unannotated text as features , explain some of their shortcomings and propose modifications . our best combination of features outperforms the baseline from data intensive approaches by 4 % for comparison and 16 % for contingency .
RANK 88
recognizing implicit discourse relations in the penn discourse treebank we present an implicit discourse relation classifier in the penn discourse treebank ( pdtb ) . our classifier considers the context of the two arguments , word pair information , as well as the arguments’ internal constituent and dependency parses . our results on the pdtb yields a significant 14.1 % improvement over the baseline . in our error analysis , we discuss four challenges in recognizing implicit relations in the pdtb .
TOP UNCITED PAPERS
RANK 1
a stacking gated neural architecture for implicit discourse relation classification discourse parsing is considered as one of the most challenging natural language processing ( nlp ) tasks . implicit discourse relation classification is the bottleneck for discourse parsing . without the guide of explicit discourse connectives , the relation of sentence pairs are very hard to be inferred . this paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network ( cnn ) is utilized for sentence modeling and a collaborative gated neural network ( cgnn ) is proposed for feature transformation . our evaluation and comparisons show that the proposed model outperforms previous state - of - the - art systems .
RANK 2
a latent variable recurrent neural network for discourse - driven language models this paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and ( possibly latent ) discourse relations between adjacent sentences . a recurrent neural network generates individual words , thus reaping the benefits of discriminatively - trained vector representations . the discourse relations are represented with a latent variable , which can be predicted or marginalized , depending on the task . the resulting model can therefore employ a training objective that includes not only discourse relation classification , but also word prediction . as a result , it outperforms state - ofthe - art alternatives for two tasks : implicit discourse relation classification in the penn discourse treebank , and dialog act classification in the switchboard corpus . furthermore , by marginalizing over latent discourse relations at test time , we obtain a discourse informed language model , which improves over a strong lstm baseline .
RANK 3
combining recurrent and convolutional neural networks for relation classification this paper investigates two different neural architectures for the task of relation classification : convolutional neural networks and recurrent neural networks . for both models , we demonstrate the effect of different architectural choices . we present a new context representation for convolutional neural networks for relation classification ( extended middle context ) . furthermore , we propose connectionist bi - directional recurrent neural networks and introduce ranking loss for their optimization . finally , we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results . our neural models achieve state - of - the - art results on the semeval 2010 relation classification task .
TOP 20
RANK = 1; score = 0.3749115498811988; correct = False; id = cfd468bf8b138b1eed6b32ad262a1a794f9440b4
a stacking gated neural architecture for implicit discourse relation classification discourse parsing is considered as one of the most challenging natural language processing ( nlp ) tasks . implicit discourse relation classification is the bottleneck for discourse parsing . without the guide of explicit discourse connectives , the relation of sentence pairs are very hard to be inferred . this paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network ( cnn ) is utilized for sentence modeling and a collaborative gated neural network ( cgnn ) is proposed for feature transformation . our evaluation and comparisons show that the proposed model outperforms previous state - of - the - art systems .
RANK = 2; score = 0.3274714527273717; correct = False; id = 008b3fd9ea599a840d9e2eadccedaf2e4bf89099
a latent variable recurrent neural network for discourse - driven language models this paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and ( possibly latent ) discourse relations between adjacent sentences . a recurrent neural network generates individual words , thus reaping the benefits of discriminatively - trained vector representations . the discourse relations are represented with a latent variable , which can be predicted or marginalized , depending on the task . the resulting model can therefore employ a training objective that includes not only discourse relation classification , but also word prediction . as a result , it outperforms state - ofthe - art alternatives for two tasks : implicit discourse relation classification in the penn discourse treebank , and dialog act classification in the switchboard corpus . furthermore , by marginalizing over latent discourse relations at test time , we obtain a discourse informed language model , which improves over a strong lstm baseline .
RANK = 3; score = 0.3121006709813331; correct = False; id = 258d08e8dab99820361cd91364caded4975f5c3e
combining recurrent and convolutional neural networks for relation classification this paper investigates two different neural architectures for the task of relation classification : convolutional neural networks and recurrent neural networks . for both models , we demonstrate the effect of different architectural choices . we present a new context representation for convolutional neural networks for relation classification ( extended middle context ) . furthermore , we propose connectionist bi - directional recurrent neural networks and introduce ranking loss for their optimization . finally , we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results . our neural models achieve state - of - the - art results on the semeval 2010 relation classification task .
RANK = 4; score = 0.3008194363453122; correct = False; id = 9ba6db689f707f69cdb86c6daeebab7149beaec3
generative incremental dependency parsing with neural networks we propose a neural network model for scalable generative transition - based dependency parsing . a probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network . the model surpasses the accuracy and speed of previous generative dependency parsers , reaching 91.1 % uas . perplexity results show a strong improvement over n - gram language models , opening the way to the efficient integration of syntax into neural models for language generation .
RANK = 5; score = 0.29919265007736645; correct = False; id = 23f31d5ddb438b99bc9ad4f4f0a6ec031509bf2e
discourse relation recognition by comparing various units of sentence expression with recursive neural network . we propose a method for implicit discourse relation recognition using a recursive neural network ( rnn ) . many previous studies have used the word - pair feature to compare the meaning of two sentences for implicit discourse relation recognition . our proposed method differs in that we use various - sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions into vectors using the rnn . experiments showed that our method significantly improves the accuracy of identifying implicit discourse relations compared with the word - pair method .
RANK = 6; score = 0.29679275386160714; correct = False; id = 284d64a3426461c520baa03d61cbc957bcd02522
shallow convolutional neural network for implicit discourse relation recognition implicit discourse relation recognition remains a serious challenge due to the absence of discourse connectives . in this paper , we propose a shallow convolutional neural network ( scnn ) for implicit discourse relation recognition , which contains only one hidden layer but is effective in relation recognition . the shallow structure alleviates the overfitting problem , while the convolution and nonlinear operations help preserve the recognition and generalization ability of our model . experiments on the benchmark data set show that our model achieves comparable and even better performance when comparing against current state - of - the - art systems .
RANK = 7; score = 0.29357798754444686; correct = False; id = 167ad306d84cca2455bc50eb833454de9f2dcd02
joint language and translation modeling with recurrent neural networks we present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words . the weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward - based language or translation models . we tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically . our joint model builds on a well known recurrent neural network language model ( mikolov , 2012 ) augmented by a layer of additional inputs from the source language . we show competitive accuracy compared to the traditional channel model features . our best results improve the output of a system trained on wmt 2012 french - english data by up to 1.5 bleu , and by 1.1 bleu on average across several test sets .
RANK = 8; score = 0.2814654573801209; correct = False; id = 08b63b1f9a770c1b6f8545c2e1e4a9bfb6a2de6d
bidirectional recurrent convolutional neural network for relation classification relation classification is an important semantic processing task in the field of natural language processing ( nlp ) . in this paper , we present a novel model brcnn to classify the relation of two entities in a sentence . some state - of - the - art systems concentrate on modeling the shortest dependency path ( sdp ) between two entities leveraging convolutional or recurrent neural networks . we further explore how to make full use of the dependency relations information in the sdp , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( lstm ) units . we propose a bidirectional architecture to learn relation representations with directional information along the sdp forwards and backwards at the same time , which benefits classifying the direction of relations . experimental results show that our method outperforms the state - of - theart approaches on the semeval-2010 task 8 dataset .
RANK = 9; score = 0.27614761857889686; correct = False; id = 815718fe8cda56346da0e318ffd2cd25b68d77f6
recurrent convolutional neural networks for discourse compositionality the compositionality of meaning extends beyond the single sentence . just as words combine to form the meaning of sentences , so do sentences combine to form the meaning of paragraphs , dialogues and general discourse . we introduce both a sentence model and a discourse model corresponding to the two levels of compositionality . the sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network . the discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker . the discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers . without feature engineering or pretraining and with simple greedy decoding , the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment .
RANK = 10; score = 0.2739200336227609; correct = False; id = 6cf767e3afcea890e759d4f07fd1620b3f3684c7
discourse parsing with attention - based hierarchical neural networks rst - style document - level discourse parsing remains a difficult task and efficient deep learning models on this task have rarely been presented . in this paper , we propose an attention - based hierarchical neural network model for discourse parsing . we also incorporate tensor - based transformation function to model complicated feature interactions . experimental results show that our approach obtains comparable performance to the contemporary state - of - the - art systems with little manual feature engineering .
RANK = 11; score = 0.27172118480399987; correct = False; id = 474f24f2ba90c1cdcaa90c5c4aac57c5d6e63369
leveraging synthetic discourse data via multi - task learning for implicit discourse relation recognition to overcome the shortage of labeled data for implicit discourse relation recognition , previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples . however , a previous study ( sporleder and lascarides , 2008 ) showed that models trained on these synthetic data do not generalize very well to natural ( i.e. genuine ) implicit discourse data . in this work we revisit this issue and present a multi - task learning based system which can effectively use synthetic data for implicit discourse relation recognition . results on pdtb data show that under the multi - task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks , can achieve an averaged f1 improvement of 5.86 % over baseline models .
RANK = 12; score = 0.2698393664010108; correct = False; id = 1bce7021762989be033f3af42e329109e3c152a9
gencnn : a convolutional architecture for word sequence prediction we propose a convolutional neural network , named gencnn , for word sequence prediction . different from previous work on neural networkbased language modeling and generation ( e.g. , rnn or lstm ) , we choose not to greedily summarize the history of words as a fixed length vector . instead , we use a convolutional neural network to predict the next word with the history of words of variable length . also different from the existing feedforward networks for language modeling , our model can effectively fuse the local correlation and global correlation in the word sequence , with a convolution - gating strategy specifically designed for the task . we argue that our model can give adequate representation of the history , and therefore can naturally exploit both the short and long range dependencies . our model is fast , easy to train , and readily parallelized . our extensive experiments on text generation and n - best re - ranking in machine translation show that gencnn outperforms the state - ofthe - arts with big margins .
RANK = 13; score = 0.26952344690870705; correct = False; id = cdd2906f29d8103632dba24484571a8a05c09076
multi - timescale long short - term memory neural network for modelling sentences and documents neural network based methods have obtained great progress on a variety of natural language processing tasks . however , it is still a challenge task to model long texts , such as sentences and documents . in this paper , we propose a multi - timescale long short - term memory ( mt - lstm ) neural network to model long texts . mtlstm partitions the hidden states of the standard lstm into several groups . each group is activated at different time periods . thus , mt - lstm can model very long documents as well as short sentences . experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task .
RANK = 14; score = 0.2672834925025256; correct = False; id = 957d8108d740edae9499d4784b3be2427d28871b
a language - independent anaphora resolution system for understanding multilingual texts this paper describes a new discourse module within our multilingual nlp system . because of its unique data - driven architecture , the discourse module is language - independent . moreover , the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse - tagged corpora . separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena . 1 i n t r o d u c t i o n this paper describes a new discourse module within our multilingual natural language processing system which has been used for understanding texts in english , spanish and japanese ( el . [ 1 , 2 ] ) ) the following design principles underlie the discourse module : • language - independence : no processing code depends on language - dependent facts . • extensibility : it is easy to handle additional phenomena . • robustness : the discourse module does its best even when its input is incomplete or wrong . • trainability : the performance can be tuned for particular domains and applications . in the following , we first describe the architecture of the discourse module . then , we discuss how its performance is evaluated and trained using discoursetagged corpora . finally , we compare our approach to other research . 1 our s y s t e m has been used in severa l d a t a e x t r a c t i o n t a sks a n d a p r o t o t y p e n lach ine t r a n s l a t i o n sys te ln . p e r f o . m . . . . ~ n t i ~ u 2 k c $ ~ " e d v . . . . . . . . . . . . . . . . . . . . . . . . . . r . . . . . . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , l : ) i ~ ~ m o d u l e figure 1 : discourse architecture 2 d i s c o u r s e a r c h i t e c t u r e our discourse module consists of two discourse processing submodules ( the discourse a dministralor and the resolution engine ) , and three discourse knowledge bases ( the discourse knowledge source kb , the discourse phenomenon kb , and the discourse domain kb ) . the discourse administrator is a development - time tool for defining the three discourse kb 's . the resolution engine , on the other hand , is the run - time processing module which actually performs anaphora resolution using these discourse kb 's . the resolution engine also has access to an external discourse data structure called the global discourse world , which is created by the top - level text processing controller . the global discourse world holds syntactic , semantic , rhetorical , and other information about the input text derived by other parts of the system . the architecture is shown in figure i. 2.1 d i s c o u r s e d a t a s t r u c t u r e s there are four major discourse data types within the global discourse world : discourse world ( dw ) , [ ) is-
RANK = 15; score = 0.2617072394698184; correct = False; id = 9d04258144f2b97ef492bf762b0e46e8b4065bf9
an end - to - end approach to learning semantic frames with feedforward neural network . we present an end - to - end method for learning verb - specific semantic frames with feedforward neural network ( fnn ) . previous works in this area mainly adopt a multi - step procedure including part - of - speech tagging , dependency parsing and so on . on the contrary , our method uses a fnn model that maps verbspecific sentences directly to semantic frames . the simple model gets good results on annotated data and has a good generalization ability . finally we get 0.82 f - score on 63 verbs and 0.73 f - score on 407 verbs .
RANK = 16; score = 0.2616491097634029; correct = False; id = f5d93f0b6d77e2681072e0bd79735a3b0c457c1b
ccg supertagging with a recurrent neural network recent work on supertagging using a feedforward neural network achieved significant improvements for ccg supertagging and parsing ( lewis and steedman , 2014 ) . however , their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length . in this paper , we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both supertagging ( up to 1.9 % ) and parsing ( up to 1 % f1 ) , on ccgbank , wikipedia and biomedical text .
RANK = 17; score = 0.25999960325983423; correct = False; id = c954a227d581fc57ae3372458a62aa19fe5167db
towards a discourse relation - aware approach for chinese - english machine translation translation of discourse relations is one of the recent efforts of incorporating discourse information to statistical machine translation ( smt ) . while existing works focus on disambiguation of ambiguous discourse connectives , or transformation of discourse trees , only explicit discourse relations are tackled . a greater challenge exists in machine translation of chinese , since implicit discourse relations are abundant and occur both inside and outside a sentence . this thesis proposal describes ongoing work on bilingual discourse annotation and plans towards incorporating discourse relation knowledge to a chineseenglish smt system with consideration of implicit discourse relations . the final goal is a discourse - unit - based translation model unbounded by the traditional assumption of sentence - to - sentence translation .
RANK = 18; score = 0.2588447678825329; correct = False; id = 2891789ef2413f7979ae62fd3ce296b56059e7f1
variational neural discourse relation recognizer implicit discourse relation recognition is a crucial component for automatic discourse - level analysis and nature language understanding . previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations . in this paper , instead , we explore generative models and propose a variational neural discourse relation recognizer . we refer to this model as virile . virile establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse . in order to perform efficient inference and learning , we introduce a neural discourse relation model to approximate the posterior of the latent variable , and employ this approximated posterior to optimize a reparameterized variational lower bound . this allows virile to be trained with standard stochastic gradient methods . experiments on the benchmark data set show that virile can achieve competitive results against state - of - the - art baselines .
RANK = 19; score = 0.2576881046727744; correct = False; id = c27cff0fca6d12ef1d6390d4a27f34103c478472
m2l at semeval-2016 task 8 : amr parsing with neural networks this paper describes our contribution to the semeval 2016 workshop . we participated in the shared task 8 on meaning representation parsing using a transition - based approach , which builds upon the system of wang et al . ( 2015a ) and wang et al . ( 2015b ) , with additions that utilize a feedforward neural network classifier and an enriched feature set . we observed that exploiting neural networks in abstract meaning representation parsing is challenging and we could not benefit from it , while the feature enhancements yielded an improved performance over the baseline model .
RANK = 20; score = 0.25658655329122493; correct = False; id = 63869c1b3b4890e22101bc491b459e9d9b355f9f
long short - term memory neural networks for chinese word segmentation currently most of state - of - the - art methods for chinese word segmentation are based on supervised learning , whose features aremostly extracted from a local context . thesemethods can not utilize the long distance information which is also crucial for word segmentation . in this paper , we propose a novel neural network model for chinese word segmentation , which adopts the long short - term memory ( lstm ) neural network to keep the previous important information inmemory cell and avoids the limit of window size of local context . experiments on pku , msra and ctb6 benchmark datasets show that our model outperforms the previous neural network models and state - of - the - art methods .

RANKING 1848
QUERY
citius at semeval-2017 task 2 : cross - lingual similarity from comparable corpora and dependency - based contexts this article describes the distributional strategy submitted by the citius team to the semeval 2017 task 2 . even though the team participated in two subtasks , namely monolingual and crosslingual word similarity , the article is mainly focused on the cross - lingual subtask . our method uses comparable corpora and syntactic dependencies to extract count - based and transparent bilingual distributional contexts . the evaluation of the results show that our method is competitive with other cross - lingual strategies , even those using aligned and parallel texts .
First cited at 6
TOP CITED PAPERS
RANK 6
a framework for the construction of monolingual and cross - lingual word similarity datasets despite being one of the most popular tasks in lexical semantics , word similarity has often been limited to the english language . other languages , even those that are widely spoken such as spanish , do not have a reliable word similarity evaluation framework . we put forward robust methodologies for the extension of existing english datasets to other languages , both at monolingual and cross - lingual levels . we propose an automatic standardization for the construction of cross - lingual similarity datasets , and provide an evaluation , demonstrating its reliability and robustness . based on our procedure and taking the rg-65 word similarity dataset as a reference , we release two high - quality spanish and farsi ( persian ) monolingual datasets , and fifteen cross - lingual datasets for six languages : english , spanish , french , german , portuguese , and farsi .
RANK 388
automatic identification of word translations from unrelated english and german corpora algorithms for the alignment of words in translated texts are w ell established . however , only recently , new approaches have been proposed to identify word translations f rom non - parallel or even unrelated texts . this task is more difficult , because most statistical clues useful in the processing of parallel texts can not be applied for non - parallel tex ts . for this reason , whereas for parallel texts in some studies up to 99 % of the word alignments have been shown to be correct , the accuracy for non - parallel texts has been around 30 % up to now . the current study , which is based on the assumption that there is a correlation between the patterns of word cooccurrences in corpora of different languages , makes a significant improvement to about 72 % of word translations identified correctly .
RANK 611
dependency - based word embeddings while continuous word embeddings are gaining popularity , current models are based solely on linear contexts . in this work , we generalize the skip - gram model with negative sampling introduced by mikolov et al . to include arbitrary contexts . in particular , we perform experiments with dependency - based contexts , and show that they produce markedly different embeddings . the dependencybased embeddings are less topical and exhibit more functional similarity than the original skip - gram embeddings .
TOP UNCITED PAPERS
RANK 1
learning cross - lingual word embeddings via matrix co - factorization a joint - space model for cross - lingual distributed representations generalizes language - invariant semantic features . in this paper , we present a matrix cofactorization framework for learning cross - lingual word embeddings . we explicitly define monolingual training objectives in the form of matrix decomposition , and induce cross - lingual constraints for simultaneously factorizing monolingual matrices . the cross - lingual constraints can be derived from parallel corpora , with or without word alignments . empirical results on a task of cross - lingual document classification show that our method is effective to encode cross - lingual knowledge as constraints for cross - lingual word embeddings .
RANK 2
automatic cross - lingual similarization of dependency grammars for tree - based machine translation structural isomorphism between languages benefits the performance of cross - lingual applications . we propose an automatic algorithm for cross - lingual similarization of dependency grammars , which automatically learns grammars with high cross - lingual similarity . the algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classification decisions , and gradually improves the cross - lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning . the dependency grammars given by cross - lingual similarization have much higher cross - lingual similarity while maintaining non - triviality . as applications , the cross - lingually similarized grammars significantly improve the performance of dependency tree - based machine translation .
RANK 3
semi - supervised representation learning for cross - lingual text classification cross - lingual adaptation aims to learn a prediction model in a label - scarce target language by exploiting labeled data from a labelrich source language . an effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks . in this paper , we propose a new cross - lingual adaptation approach for document classification based on learning cross - lingual discriminative distributed representations of words . specifically , we propose to maximize the loglikelihood of the documents from both language domains under a cross - lingual logbilinear document model , while minimizing the prediction log - losses of labeled documents . we conduct extensive experiments on cross - lingual sentiment classification tasks of amazon product reviews . our experimental results demonstrate the efficacy of the proposed cross - lingual adaptation approach .
TOP 20
RANK = 1; score = 0.3760134299609921; correct = False; id = e6180727be1760d0d9c9a3aaa20cf05bc6a54d27
learning cross - lingual word embeddings via matrix co - factorization a joint - space model for cross - lingual distributed representations generalizes language - invariant semantic features . in this paper , we present a matrix cofactorization framework for learning cross - lingual word embeddings . we explicitly define monolingual training objectives in the form of matrix decomposition , and induce cross - lingual constraints for simultaneously factorizing monolingual matrices . the cross - lingual constraints can be derived from parallel corpora , with or without word alignments . empirical results on a task of cross - lingual document classification show that our method is effective to encode cross - lingual knowledge as constraints for cross - lingual word embeddings .
RANK = 2; score = 0.3386541739136413; correct = False; id = 961d57fc4bf51f73d8fa6bb30a7d5566255c1f82
automatic cross - lingual similarization of dependency grammars for tree - based machine translation structural isomorphism between languages benefits the performance of cross - lingual applications . we propose an automatic algorithm for cross - lingual similarization of dependency grammars , which automatically learns grammars with high cross - lingual similarity . the algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classification decisions , and gradually improves the cross - lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning . the dependency grammars given by cross - lingual similarization have much higher cross - lingual similarity while maintaining non - triviality . as applications , the cross - lingually similarized grammars significantly improve the performance of dependency tree - based machine translation .
RANK = 3; score = 0.321690431297935; correct = False; id = 6a808dd1ebee32dea657b4c0b08bb6e4d49203e9
semi - supervised representation learning for cross - lingual text classification cross - lingual adaptation aims to learn a prediction model in a label - scarce target language by exploiting labeled data from a labelrich source language . an effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks . in this paper , we propose a new cross - lingual adaptation approach for document classification based on learning cross - lingual discriminative distributed representations of words . specifically , we propose to maximize the loglikelihood of the documents from both language domains under a cross - lingual logbilinear document model , while minimizing the prediction log - losses of labeled documents . we conduct extensive experiments on cross - lingual sentiment classification tasks of amazon product reviews . our experimental results demonstrate the efficacy of the proposed cross - lingual adaptation approach .
RANK = 4; score = 0.31793833280346107; correct = False; id = 47d816bb03aefca088f5f6e02498a3a0e651b6a7
unsupervised cross - lingual lexical substitution cross - lingual lexical substitution ( clls ) is the task that aims at providing for a target word in context , several alternative substitute words in another language . the proposed sets of translations may come from external resources or be extracted from textual data . in this paper , we apply for the first time an unsupervised cross - lingual wsd method to this task . the method exploits the results of a cross - lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity . we evaluate the impact of using clustering information for clls by applying the wsd method to the semeval-2010 clls data set . our system performs better on the ’out - of - ten’ measure than the systems that participated in the semeval task , and is ranked medium on the other measures . we analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting .
RANK = 5; score = 0.31649514820567215; correct = False; id = 23bc5b2e74f6fcc57d84b4e477de1270c53573aa
cross - lingual models of word embeddings : an empirical comparison despite interest in using cross - lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature . we perform an extensive evaluation of four popular approaches of inducing cross - lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs . our evaluation setup spans four different tasks , including intrinsic evaluation on mono - lingual and cross - lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications . we show that models which require expensive cross - lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .
RANK = 6; score = 0.3125004418306041; correct = True; id = 41e733019e0d61b6b09327fd47c3e3c4e0bc10a6
a framework for the construction of monolingual and cross - lingual word similarity datasets despite being one of the most popular tasks in lexical semantics , word similarity has often been limited to the english language . other languages , even those that are widely spoken such as spanish , do not have a reliable word similarity evaluation framework . we put forward robust methodologies for the extension of existing english datasets to other languages , both at monolingual and cross - lingual levels . we propose an automatic standardization for the construction of cross - lingual similarity datasets , and provide an evaluation , demonstrating its reliability and robustness . based on our procedure and taking the rg-65 word similarity dataset as a reference , we release two high - quality spanish and farsi ( persian ) monolingual datasets , and fifteen cross - lingual datasets for six languages : english , spanish , french , german , portuguese , and farsi .
RANK = 7; score = 0.3120905707182265; correct = False; id = 1123ded2df6833525460bb3aa62d6da2f54abef2
cross - lingual distributional profiles of concepts for measuring semantic distance we present the idea of estimating semantic distance in one , possibly resource - poor , language using a knowledge source in another , possibly resource - rich , language . we do so by creating cross - lingual distributional profiles of concepts , using a bilingual lexicon and a bootstrapping algorithm , but without the use of any sense - annotated data or word - aligned corpora . the cross - lingual measures of semantic distance are evaluated on two tasks : ( 1 ) estimating semantic distance between words and ranking the word pairs according to semantic distance , and ( 2 ) solving reader ’s digest‘word power’ problems . in task ( 1 ) , cross - lingual measures are superior to conventional monolingual measures based on a wordnet . in task ( 2 ) , cross - lingual measures are able to solve more problems correctly , and despite scores being affected by many tied answers , their overall performance is again better than the best monolingual measures .
RANK = 8; score = 0.30552700922549064; correct = False; id = e9d11e7d2c27d95edd38596d1d6187e12c66266b
an efficient cross - lingual model for sentence classification using convolutional neural network in this paper , we propose a cross - lingual convolutional neural network ( cnn ) model that is based on word and phrase embeddings learned from unlabeled data in two languages and dependency grammar . compared to traditional machine translation ( mt ) based methods for cross lingual sentence modeling , our model is much simpler and does not need parallel corpora or language specific features . we only use a bilingual dictionary and dependency parser . this makes our model particularly appealing for resource poor languages . we evaluate our model using english and chinese data on several sentence classification tasks . we show that our model achieves a comparable and even better performance than the traditional mt - based method .
RANK = 9; score = 0.30354073594933595; correct = False; id = ea832e2eef525c3320f747cd8894026cb1dadb9d
fbk : cross - lingual textual entailment without translation this paper overviews fbk ’s participation in the cross - lingual textual entailment for content synchronization task organized within semeval-2012 . our participation is characterized by using cross - lingual matching features extracted from lexical and semantic phrase tables and dependency relations . the features are used for multi - class and binary classification using svms . using a combination of lexical , syntactic , and semantic features to create a cross - lingual textual entailment system , we report on experiments over the provided dataset . our best run achieved an accuracy of 50.4 % on the spanish - english dataset ( with the average score and the median system respectively achieving 40.7 % and 34.6 % ) , demonstrating the effectiveness of a “ pure ” cross - lingual approach that avoids intermediate translations .
RANK = 10; score = 0.30195313644489274; correct = False; id = 5e969e3ba8391905903e1b451d1391830a4e46fb
fbk hlt - mt at semeval-2016 task 1 : cross - lingual semantic similarity measurement using quality estimation features and compositional bilingual word embeddings this paper describes the system by fbk hltmt for cross - lingual semantic textual similarity measurement . our approach is based on supervised regression with an ensemble decision tree . in order to assign a semantic similarity score to an input sentence pair , the model combines features collected by state - of - the - art methods in machine translation quality estimation and distance metrics between crosslingual embeddings of the two sentences . in our analysis , we compare different techniques for composing sentence vectors , several distance features and ways to produce training data . the proposed system achieves a mean pearson ’s correlation of 0.39533 , ranking 7 among all participants in the cross - lingual sts task organized within the semeval 2016 evaluation campaign .
RANK = 11; score = 0.2951416191570295; correct = False; id = cff628c0f701bcb564f8ef3469ba8646655cc43c
cross - lingual word representations via spectral graph embeddings cross - lingual word embeddings are used for cross - lingual information retrieval or domain adaptations . in this paper , we extend eigenwords , spectral monolingual word embeddings based on canonical correlation analysis ( cca ) , to crosslingual settings with sentence - alignment . for incorporating cross - lingual information , cca is replaced with its generalization based on the spectral graph embeddings . the proposed method , which we refer to as cross - lingual eigenwords ( cl - eigenwords ) , is fast and scalable for computing distributed representations of words via eigenvalue decomposition . numerical experiments of english - spanish word translation tasks show that cleigenwords is competitive with stateof - the - art cross - lingual word embedding methods .
RANK = 12; score = 0.2873253507393949; correct = False; id = 63fdc6d77065ba447914aac7e7968f9a9cf73a18
an english - chinese cross - lingual word semantic similarity measure exploring attributes and relations word semantic similarity measuring is a fundamental issue to many nlp applications and the globalization has made an urgent request for cross - lingual word similarity measure . this paper proposed a word semantic similarity measure which is able to work in cross - lingual scenarios . basically , a concept can be defined by a set of attributes . the basic idea of this work is to compute the similarity between words by exploring their attributes and relations . for a given word pair , we first compute similarities between their attributes by combining distance , depth and relation information . then word similarity are computed through a combination scheme . the algorithm is implemented based on an english - chinese bilingual ontology hownet . experiments show that the proposed algorithm results in high correlation against human judgments , which encourages its broad application in cross - lingual applications .
RANK = 13; score = 0.28405057819796886; correct = False; id = 4b037a37f37562cc50edd766dbf73c1e4ac0f880
probabilistic models of cross - lingual semantic similarity in context based on latent cross - lingual concepts induced from comparable data we propose the first probabilistic approach to modeling cross - lingual semantic similarity ( clss ) in context which requires only comparable data . the approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language - pair independent latent semantic concepts ( e.g. , crosslingual topics obtained by a multilingual topic model ) . these latent cross - lingual concepts are induced from a comparable corpus without any additional lexical resources . word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts . we present new models that modulate the isolated out - ofcontext word representations with contextual knowledge . results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of crosslingual semantic similarity .
RANK = 14; score = 0.2813418106298286; correct = False; id = 65ee30a27d633757b96e61c7c0318d7a08557519
uvt - wsd1 : a cross - lingual word sense disambiguation system this paper describes the cross - lingual word sense disambiguation system uvtwsd1 , developed at tilburg university , for participation in two semeval-2 tasks : the cross - lingual word sense disambiguation task and the cross - lingual lexical substitution task . the uvt - wsd1 system makes use of k - nearest neighbour classifiers , in the form of single - word experts for each target word to be disambiguated . these classifiers can be constructed using a variety of local and global context features , and these are mapped onto the translations , i.e. the senses , of the words . the system works for a given language - pair , either english - dutch or english - spanish in the current implementation , and takes a word - aligned parallel corpus as its input .
RANK = 15; score = 0.28001678033275856; correct = False; id = 079dac954e7e23acc0771384b79488217d5ef302
sparse bilingual word representations for cross - lingual lexical entailment we introduce the task of cross - lingual lexical entailment , which aims to detect whether the meaning of a word in one language can be inferred from the meaning of a word in another language . we construct a gold standard for this task , and propose an unsupervised solution based on distributional word representations . as commonly done in the monolingual setting , we assume a word e entails a word f if the prominent context features of e are a subset of those of f . to address the challenge of comparing contexts across languages , we propose a novel method for inducing sparse bilingual word representations from monolingual and parallel texts . our approach yields an fscore of 70 % , and significantly outperforms strong baselines based on translation and on existing word representations .
RANK = 16; score = 0.27996217218736597; correct = False; id = 10dd5320f568a41ac72cdb4a148cf6809eadd0dd
a study on similarity and relatedness using distributional and wordnet - based approaches this paper presents and compares wordnetbased and distributional similarity approaches . the strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed , and a combination is presented . each of our methods independently provide the best results in their class on the rg and wordsim353 datasets , and a supervised combination of them yields the best published results on all datasets . finally , we pioneer cross - lingual similarity , showing that our methods are easily adapted for a cross - lingual task with minor losses .
RANK = 17; score = 0.2785933597863351; correct = False; id = 31299844001b0322b4262c3f840ecd1d6a98149c
cross - lingual transfer of named entity recognizers without parallel corpora we propose an approach to cross - lingual named entity recognition model transfer without the use of parallel corpora . in addition to global de - lexicalized features , we introduce multilingual gazetteers that are generated using graph propagation , and cross - lingual word representation mappings without the use of parallel data . we target the e - commerce domain , which is challenging due to its unstructured and noisy nature . the experiments have shown that our approaches beat the strong mt baseline , where the english model is transferred to two languages : spanish and chinese .
RANK = 18; score = 0.2734552700203635; correct = False; id = 42636e49faeb9d8eefac065979115c685fec1bab
learning crosslingual word embeddings without bilingual corpora crosslingual word embeddings represent lexical items from different languages in the same vector space , enabling transfer of nlp tools . however , previous attempts had expensive resource requirements , difficulty incorporating monolingual data or were unable to handle polysemy . we address these drawbacks in our method which takes advantage of a high coverage dictionary in an em style training algorithm over monolingual corpora in two languages . our model achieves state - of - theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora , and competitive results on the monolingual word similarity and cross - lingual document classification task .
RANK = 19; score = 0.27110581318490395; correct = False; id = 08c33e9d73ff3e71407ec215ef26e433082ef02b
cross - lingual genre classification automated classification of texts into genres can benefit nlp applications , in that the structure , location and even interpretation of information within a text are dictated by its genre . cross - lingual methods promise such benefits to languages which lack genre - annotated training data . while there has been work on genre classification for over two decades , none has considered cross - lingual methods before the start of this project . my research aims to fill this gap . it follows previous approaches to monolingual genre classification that exploit simple , low - level text features , many of which can be extracted in different languages and have similar functions . this contrasts with work on cross - lingual topic or sentiment classification of texts that typically use word frequencies as features . these have been shown to have limited use when it comes to genres . many such methods also assume cross - lingual resources , such as machine translation , which limits the range of their application . a selection of these approaches are used as baselines in my experiments . i report the results of two semi - supervised methods for exploiting genre - labelled source language texts and unlabelled target language texts . the first is a relatively simple algorithm that bridges the language gap by exploiting cross - lingual features and then iteratively re - trains a classification model on previously predicted target texts . my results show that this approach works well where only few cross - lingual resources are available and texts are to be classified into broad genre categories . it is also shown that further improvements can be achieved through multi - lingual training or cross - lingual feature selection if genre - annotated texts are available in several source languages . the second is a variant of the label propagation algorithm . this graph - based classifier learns genre - specific feature set weights from both source and target language texts and uses them to adjust the propagation channels for each text . this allows further feature sets to be added as additional resources , such as part of speech taggers , become available . while the method performs well even with basic text features , it is shown to benefit from additional feature sets . results also indicate that it handles fine - grained genre classes better than the iterative re - labelling method .
RANK = 20; score = 0.27066455160172165; correct = False; id = 2b3edd7b9d7946e3261e945a5571f0e0432eec86
cross - lingual induction of selectional preferences with bilingual vector spaces we describe a cross - lingual method for the induction of selectional preferences for resourcepoor languages , where no accurate monolingual models are available . the method uses bilingual vector spaces to “ translate ” foreign language predicate - argument structures into a resource - rich language like english . the only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource - poor language , although the model can profit from ( even noisy ) syntactic knowledge . our experiments show that the cross - lingual predictions correlate well with human ratings , clearly outperforming monolingual baseline models .

RANKING 901
QUERY
f - score driven max margin neural network for named entity recognition in chinese social media we focus on named entity recognition ( ner ) for chinese social media . with massive unlabeled text and quite limited labelled corpus , we propose a semisupervised learning model based on blstm neural network . to take advantage of traditional methods in ner such as crf , we combine transition probability with deep learning in our model . to bridge the gap between label accuracy and f - score of ner , we construct a model which can be directly trained on f - score . when considering the instability of fscore driven method and meaningful information provided by label accuracy , we propose an integrated method to train on both f - score and label accuracy . our integrated model yields substantial improvement over previous state - of - the - art result .
First cited at 1
TOP CITED PAPERS
RANK 1
named entity recognition for chinese social media with jointly trained embeddings we consider the task of named entity recognition for chinese social media . the long line of work in chinese ner has focused on formal domains , and ner for social media has been largely restricted to english . we present a new corpus of weibo messages annotated for both name and nominal mentions . additionally , we evaluate three types of neural embeddings for representing chinese text . finally , we propose a joint training objective for the embeddings that makes use of both ( ner ) labeled and unlabeled raw text . our methods yield a 9 % improvement over a stateof - the - art baseline .
RANK 2
improving named entity recognition for chinese social media with word segmentation representation learning named entity recognition , and other information extraction tasks , frequently use linguistic features such as part of speech tags or chunkings . for languages where word boundaries are not readily identified in text , word segmentation is a key first step to generating features for an ner system . while using word boundary tags as features are helpful , the signals that aid in identifying these boundaries may provide richer information for an ner system . new state - of - the - art word segmentation systems use neural models to learn representations for predicting word boundaries . we show that these same representations , jointly trained with an ner system , yield significant improvements in ner for chinese social media . in our experiments , jointly training ner and word segmentation with an lstm - crf model yields nearly 5 % absolute improvement over previously published results .
RANK 19
learning dictionaries for named entity recognition using minimal supervision this paper describes an approach for automatic construction of dictionaries for named entity recognition ( ner ) using large amounts of unlabeled data and a few seed examples . we use canonical correlation analysis ( cca ) to obtain lower dimensional embeddings ( representations ) for candidate phrases and classify these phrases using a small number of labeled examples . our method achieves 16.5 % and 11.3 % f-1 score improvement over co - training on disease and virus ner respectively . we also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings .
TOP UNCITED PAPERS
RANK 3
named entity recognition in estonian the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names , organizations and locations . in this work , we address the problem of ner in estonian using supervised learning approach . we explore common issues related to building a ner system such as the usage of language - agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools . for system training and evaluation purposes , we create a gold standard ner corpus . on this corpus , our crf - based system achieves an overall f1-score of 87 % .
RANK 4
using n - best lists for named entity recognition from chinese speech we present the first known result for named entity recognition ( ner ) in realistic largevocabulary spoken chinese . we establish this result by applying a maximum entropy model , currently the single best known approach for textual chinese ner , to the recognition output of the bbn lvcsr system on chinese broadcast news utterances . our results support the claim that transferring ner approaches from text to spoken language is a significantly more difficult task for chinese than for english . we propose re - segmenting the asr hypotheses as well as applying postclassification to improve the performance . finally , we introduce a method of using n - best hypotheses that yields a small but nevertheless useful improvement ner accuracy . we use acoustic , phonetic , language model , ner and other scores as confidence measure . experimental results show an average of 6.7 % relative improvement in precision and 1.7 % relative improvement in f - measure .
RANK 5
named entity recognition with bilingual constraints different languages contain complementary cues about entities , which can be used to improve named entity recognition ( ner ) systems . we propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple integer linear program , which encourages entity tags to agree via bilingual constraints . bilingual ner experiments on the large ontonotes 4.0 chinese - english corpus show that the proposed method can improve strong baselines for both chinese and english . in particular , chinese performance improves by over 5 % absolute f1 score . we can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method , and add it as uptraining data to the original monolingual ner training corpus . the chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % f1 score .
TOP 20
RANK = 1; score = 0.43011763183776586; correct = True; id = 0962f2a4452d7232fb031699ae933dccc77e5da0
named entity recognition for chinese social media with jointly trained embeddings we consider the task of named entity recognition for chinese social media . the long line of work in chinese ner has focused on formal domains , and ner for social media has been largely restricted to english . we present a new corpus of weibo messages annotated for both name and nominal mentions . additionally , we evaluate three types of neural embeddings for representing chinese text . finally , we propose a joint training objective for the embeddings that makes use of both ( ner ) labeled and unlabeled raw text . our methods yield a 9 % improvement over a stateof - the - art baseline .
RANK = 2; score = 0.3652192137465909; correct = True; id = 36ea0a9710b9310ce9c6ce199af63b6a00eea480
improving named entity recognition for chinese social media with word segmentation representation learning named entity recognition , and other information extraction tasks , frequently use linguistic features such as part of speech tags or chunkings . for languages where word boundaries are not readily identified in text , word segmentation is a key first step to generating features for an ner system . while using word boundary tags as features are helpful , the signals that aid in identifying these boundaries may provide richer information for an ner system . new state - of - the - art word segmentation systems use neural models to learn representations for predicting word boundaries . we show that these same representations , jointly trained with an ner system , yield significant improvements in ner for chinese social media . in our experiments , jointly training ner and word segmentation with an lstm - crf model yields nearly 5 % absolute improvement over previously published results .
RANK = 3; score = 0.36198170508970273; correct = False; id = 4723b9401da52ca0962a169a9c6260908fa6c478
named entity recognition in estonian the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names , organizations and locations . in this work , we address the problem of ner in estonian using supervised learning approach . we explore common issues related to building a ner system such as the usage of language - agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools . for system training and evaluation purposes , we create a gold standard ner corpus . on this corpus , our crf - based system achieves an overall f1-score of 87 % .
RANK = 4; score = 0.3616175591037413; correct = False; id = 785eba2987f71cc34560ba1dcd3a5ffc133322e1
using n - best lists for named entity recognition from chinese speech we present the first known result for named entity recognition ( ner ) in realistic largevocabulary spoken chinese . we establish this result by applying a maximum entropy model , currently the single best known approach for textual chinese ner , to the recognition output of the bbn lvcsr system on chinese broadcast news utterances . our results support the claim that transferring ner approaches from text to spoken language is a significantly more difficult task for chinese than for english . we propose re - segmenting the asr hypotheses as well as applying postclassification to improve the performance . finally , we introduce a method of using n - best hypotheses that yields a small but nevertheless useful improvement ner accuracy . we use acoustic , phonetic , language model , ner and other scores as confidence measure . experimental results show an average of 6.7 % relative improvement in precision and 1.7 % relative improvement in f - measure .
RANK = 5; score = 0.3308671770776617; correct = False; id = 3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7
named entity recognition with bilingual constraints different languages contain complementary cues about entities , which can be used to improve named entity recognition ( ner ) systems . we propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple integer linear program , which encourages entity tags to agree via bilingual constraints . bilingual ner experiments on the large ontonotes 4.0 chinese - english corpus show that the proposed method can improve strong baselines for both chinese and english . in particular , chinese performance improves by over 5 % absolute f1 score . we can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method , and add it as uptraining data to the original monolingual ner training corpus . the chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % f1 score .
RANK = 6; score = 0.32670506254476606; correct = False; id = c38fd86b6c45927700fe19516c7243fd399c0020
named entity recognition for arabic social media the majority of research on arabic named entity recognition ( ner ) addresses the the task for newswire genre , where the language used is modern standard arabic ( msa ) , however , the need to study this task in social media is becoming more vital . social media is characterized by the use of both msa and dialectal arabic ( da ) , with often code switching between the two language varieties . despite some common characteristics between msa and da , there are significant differences between which result in poor performance when msa targeting systems are applied for ner in da . additionally , most ner systems rely primarily on gazetteers , which can be more challenging in a social media processing context due to an inherent low coverage . in this paper , we present a gazetteers - free ner system for dialectal data that yields an f1 score of 72.68 % which is an absolute improvement of ≈ 2 − 3 % over a comparable state - ofthe - art gazetteer based da - ner system .
RANK = 7; score = 0.31927272603427337; correct = False; id = b616c6a2c50100d49184a8c5676965f8fd9dc4eb
domain specific named entity recognition referring to the real world by deep neural networks in this paper , we propose a method for referring to the real world to improve named entity recognition ( ner ) specialized for a domain . our method adds a stacked autoencoder to a text - based deep neural network for ner . we first train the stacked auto - encoder only from the real world information , then the entire deep neural network from sentences annotated with nes and accompanied by real world information . in our experiments , we took japanese chess as the example . the dataset consists of pairs of a game state and commentary sentences about it annotated with gamespecific ne tags . we conducted ner experiments and showed that referring to the real world improves the ner accuracy .
RANK = 8; score = 0.31340853880447417; correct = False; id = a2caf046aa395483d53b50ca51f9e21c5e9ad970
chinese word segmentation and named entity recognition based on conditional random fields models this paper mainly describes a chinese named entity recognition ( ner ) system ner@iscas , which integrates text , part - of - speech and a small - vocabularycharacter - lists feature for msra ner open track under the framework of conditional random fields ( crfs ) model . the techniques used for the close ner and word segmentation tracks are also presented .
RANK = 9; score = 0.3113390943444195; correct = False; id = d7c9fe8e5c26be64854bf7d14a61f91158638f7f
improving multilingual named entity recognition with wikipedia entity type mapping the state - of - the - art named entity recognition ( ner ) systems are statistical machine learning models that have strong generalization capability ( i.e. , can recognize unseen entities that do not appear in training data ) based on lexical and contextual information . however , such a model could still make mistakes if its features favor a wrong entity type . in this paper , we utilize wikipedia as an open knowledge base to improve multilingual ner systems . central to our approach is the construction of high - accuracy , highcoverage multilingual wikipedia entity type mappings . these mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language - dependent knowledge involved . based on these mappings , we develop several approaches to improve an ner system . we evaluate the performance of the approaches via experiments on ner systems trained for 6 languages . experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities , especially when a system is applied to a new domain or it is trained with little training data ( up to 18.3 f1 score improvement ) .
RANK = 10; score = 0.28575268238727264; correct = False; id = 6938196e63ff09c25d1e1366aaec7135a6720216
boosting named entity recognition with neural character embeddings most state - of - the - art named entity recognition ( ner ) systems rely on handcrafted features and on the output of other nlp tasks such as part - of - speech ( pos ) tagging and text chunking . in this work we propose a language - independent ner system that uses automatically learned features only . our approach is based on the charwnn deep neural network , which uses word - level and character - level representations ( embeddings ) to perform sequential classification . we perform an extensive number of experiments using two annotated corpora in two different languages : harem i corpus , which contains texts in portuguese ; and the spa conll2002 corpus , which contains texts in spanish . our experimental results give evidence of the contribution of neural character embeddings for ner . moreover , we demonstrate that the same neural network which has been successfully applied to pos tagging can also achieve state - of - theart results for language - independet ner , using the same hyperparameters , and without any handcrafted features . for the harem i corpus , charwnn outperforms the state - of - the - art system by 7.9 points in the f1-score for the total scenario ( ten ne classes ) . for the spa conll-2002 corpus , charwnn outperforms the state - ofthe - art system by 0.8 point in the f1 .
RANK = 11; score = 0.2857190833278356; correct = False; id = d2e1a04836db23105aebc945521743233a63b8c2
incorporating speech recognition confidence into discriminative named entity recognition of speech data this paper proposes a named entity recognition ( ner ) method for speech recognition results that uses confidence on automatic speech recognition ( asr ) as a feature . the asr confidence feature indicates whether each word has been correctly recognized . the ner model is trained using asr results with named entity ( ne ) labels as well as the corresponding transcriptions with ne labels . in experiments using support vector machines ( svms ) and speech data from japanese newspaper articles , the proposed method outperformed a simple application of textbased ner to asr results in ner fmeasure by improving precision . these results show that the proposed method is effective in ner for noisy inputs .
RANK = 12; score = 0.2704787626338266; correct = False; id = a0ed80f553f3a77a5b17542507bfc062eaf2c900
tree representations in probabilistic models for extended named entities detection in this paper we deal with named entity recognition ( ner ) on transcriptions of french broadcast data . two aspects make the task more difficult with respect to previous ner tasks : i ) named entities annotated used in this work have a tree structure , thus the task can not be tackled as a sequence labelling task ; ii ) the data used are more noisy than data used for previous ner tasks . we approach the task in two steps , involving conditional random fields and probabilistic context - free grammars , integrated in a single parsing algorithm . we analyse the effect of using several tree representations . our system outperforms the best system of the evaluation campaign by a significant margin .
RANK = 13; score = 0.2692067549207618; correct = False; id = 32fce77f2683c6296e787f319663fe9d2cde2d07
joint word alignment and bilingual named entity recognition using dual decomposition translated bi - texts contain complementary language cues , and previous work on named entity recognition ( ner ) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages . however , most previous approaches to bilingual tagging assume word alignments are given as fixed input , which can cause cascading errors . we observe that ner label information can be used to correct alignment mistakes , and present a graphical model that performs bilingual ner tagging jointly with word alignment , by combining two monolingual tagging models with two unidirectional alignment models . we introduce additional cross - lingual edge factors that encourage agreements between tagging and alignment decisions . we design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and ner output space . experiments on the ontonotes dataset demonstrate that our method yields significant improvements in both ner and word alignment over state - of - the - art monolingual baselines .
RANK = 14; score = 0.2668634708971193; correct = False; id = 474453f8c7f447896bfeed67fb8c62bb3da23df7
chinese named entity recognition with a multi - phase model chinese named entity recognition is one of the difficult and challenging tasks of nlp . in this paper , we present a chinese named entity recognition system using a multi - phase model . first , we segment the text with a character - level crf model . then we apply three word - level crf models to the labeling person names , location names and organization names in the segmentation results , respectively . our systems participated in the ner tests on open and closed tracks of microsoft research ( msra ) . the actual evaluation results show that our system performs well on both the open tracks and closed tracks .
RANK = 15; score = 0.2654589541632101; correct = False; id = 8d8155a1623fccd3eaeafd972a8b9a20080b81c8
using non - local features to improve named entity recognition recall named entity recognition ( ner ) is always limited by its lower recall resulting from the asymmetric data distribution where the none class dominates the entity classes . this paper presents an approach that exploits non - local information to improve the ner recall . several kinds of non - local features encoding entity token occurrence , entity boundary and entity class are explored under conditional random fields ( crfs ) framework . experiments on sighan 2006 msra ( cityu ) corpus indicate that non - local features can effectively enhance the recall of the state - of - the - art ner systems . incorporating the non - local features into the ner systems using local features alone , our best system achieves a 23.56 % ( 25.26 % ) relative error reduction on the recall and 17.10 % ( 11.36 % ) relative error reduction on the f1 score ; the improved f1 score 89.38 % ( 90.09 % ) is significantly superior to the best ner system with f1 of 86.51 % ( 89.03 % ) participated in the closed track .
RANK = 16; score = 0.26400461489292865; correct = False; id = e1b11add582bed8cc77ac667139f56feb87f4a7e
joint entity recognition and disambiguation extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding . existing systems typically run a named entity recognition ( ner ) model to extract entity names first , then run an entity linking model to link extracted names to a knowledge base . ner and linking models are usually trained separately , and the mutual dependency between the two tasks is ignored . we propose jerl , joint entity recognition and linking , to jointly model ner and linking tasks and capture the mutual dependency between them . it allows the information from each task to improve the performance of the other . to the best of our knowledge , jerl is the first model to jointly optimize ner and linking tasks together completely . in experiments on the conll’03/aida data set , jerl outperforms state - of - art ner and linking systems , and we find improvements of 0.4 % absolute f1 for ner on conll’03 , and 0.36 % absolute precision@1 for linking on aida .
RANK = 17; score = 0.26381024317289864; correct = False; id = 1fc360befecd3ca2fa9bddd799e4c16211299fa3
exploiting wikipedia as external knowledge for named entity recognition we explore the use of wikipedia as external knowledge to improve named entity recognition ( ner ) . our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry , which can be thought of as a definition part . these category labels are used as features in a crf - based ne tagger . we demonstrate using the conll 2003 dataset that the wikipedia category labels extracted by such a simple method actually improve the accuracy of ner .
RANK = 18; score = 0.2620624813235562; correct = False; id = b3e9742953dc84eeffe4c5eedd78467f5a7f5374
rule - based named entity recognition in urdu named entity recognition or extraction ( ner ) is an important task for automated text processing for industries and academia engaged in the field of language processing , intelligence gathering and bioinformatics . in this paper we discuss the general problem of named entity recognition , more specifically the challenges in ner in languages that do not have language resources e.g. large annotated corpora . we specifically address the challenges for urdu ner and differentiate it from other south asian ( indic ) languages . we discuss the differences between hindi and urdu and conclude that the ner computational models for hindi can not be applied to urdu . a rule - based urdu ner algorithm is presented that outperforms the models that use statistical learning .
RANK = 19; score = 0.256375186975852; correct = True; id = 007f199a4b20000b263185fe0bb0a7f8903027c5
learning dictionaries for named entity recognition using minimal supervision this paper describes an approach for automatic construction of dictionaries for named entity recognition ( ner ) using large amounts of unlabeled data and a few seed examples . we use canonical correlation analysis ( cca ) to obtain lower dimensional embeddings ( representations ) for candidate phrases and classify these phrases using a small number of labeled examples . our method achieves 16.5 % and 11.3 % f-1 score improvement over co - training on disease and virus ner respectively . we also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings .
RANK = 20; score = 0.25341807005487954; correct = False; id = a063601d35146fd5f4fa92eeaf3aac01243fc852
description of the ncu chinese word segmentation and named entity recognition system for sighan bakeoff 2006 asian languages are far from most western - style in their non - separate word sequence especially chinese . the preliminary step of asian - like language processing is to find the word boundaries between words . in this paper , we present a general purpose model for both chinese word segmentation and named entity recognition . this model was built on the word sequence classification with probability model , i.e. , conditional random fields ( crf ) . we used a simple feature set for crf which achieves satisfactory classification result on the two tasks . our model achieved 91.00 in f rate in upuctreebank data , and 78.71 for ner task .

RANKING 2175
QUERY
masterprint : exploring the vulnerability of partial fingerprint - based authentication systems this paper investigates the security of partial fingerprint - based authentication systems , especially when multiple fingerprints of a user are enrolled . a number of consumer electronic devices , such as smartphones , are beginning to incorporate fingerprint sensors for user authentication . the sensors embedded in these devices are generally small and the resulting images are , therefore , limited in size . to compensate for the limited size , these devices often acquire multiple partial impressions of a single finger during enrollment to ensure that at least one of them will successfully match with the image obtained from the user during authentication . furthermore , in some cases , the user is allowed to enroll multiple fingers , and the impressions pertaining to multiple partial fingers are associated with the same identity ( i.e. , one user ) . a user is said to be successfully authenticated if the partial fingerprint obtained during authentication matches any one of the stored templates . this paper investigates the possibility of generating a “ masterprint , ” a synthetic or real partial fingerprint that serendipitously matches one or more of the stored templates for a significant number of users . our preliminary results on an optical fingerprint data set and a capacitive fingerprint data set indicate that it is indeed possible to locate or generate partial fingerprints that can be used to impersonate a large number of users . in this regard , we expose a potential vulnerability of partial fingerprint - based authentication systems , especially when multiple impressions are enrolled per finger .
First cited at 79
TOP CITED PAPERS
RANK 79
evidential value of automated latent fingerprint comparison : an empirical approach latent prints are routinely recovered from crime scenes and are compared with available databases of known fingerprints for identifying criminals . however , current procedures to compare latent prints to large databases of exemplar ( rolled or plain ) prints are prone to errors . this suggests caution in making conclusions about a suspect 's identity based on a latent fingerprint comparison . a number of attempts have been made to statistically model the utility of a fingerprint comparison in making a correct accept / reject decision or its evidential value . these approaches , however , either make unrealistic assumptions about the model or they lack simple interpretation . we argue that the posterior probability of two fingerprints belonging to different fingers given their match score , referred to as the nonmatch probability ( nmp ) , effectively captures any implicating evidence of the comparison . nmp is computed using state - of - the - art matchers and is easy to interpret . to incorporate the effect of image quality , number of minutiae , and size of the latent on nmp value , we compute the nmp vs. match score plots separately for image pairs ( latent and exemplar prints ) with different characteristics . given the paucity of latent fingerprint databases in public domain , we simulate latent prints using two exemplar print databases ( nist sd-14 and michigan state police ) by cropping regions of three different sizes . we appropriately validate this simulation using four latent databases ( nist sd-27 and three proprietary latent databases ) and two state - of - the - art fingerprint matchers to compute their respective match scores . we also discuss a practical scenario where a latent examiner uses the proposed framework to compute the evidential value of a latent - exemplar print pair comparison .
RANK 2568
the science of guessing : analyzing an anonymized corpus of 70 million passwords we report on the largest corpus of user - chosen passwords ever studied , consisting of anonymized password histograms representing almost 70 million yahoo ! users , mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics . this large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution . in place of previously used metrics such as shannon entropy and guessing entropy , which can not be estimated with any realistically sized sample , we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker 's desired success rate . our new metric is comparatively easy to approximate and directly relevant for security engineering . by comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack , we estimate that passwords provide fewer than 10 bits of security against an online , trawling attack , and only about 20 bits of security against an optimal offline dictionary attack . we find surprisingly little variation in guessing difficulty ; every identifiable group of users generated a comparably weak password distribution . security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality . even proactive efforts to nudge users towards better password choices with graphical feedback make little difference . more surprisingly , even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population - specific lists .
TOP UNCITED PAPERS
RANK 1
on mixing fingerprints this work explores the possibility of mixing two different fingerprints , pertaining to two different fingers , at the image level in order to generate a new fingerprint . to mix two fingerprints , each fingerprint pattern is decomposed into two different components , viz . , the continuous and spiral components . after prealigning the components of each fingerprint , the continuous component of one fingerprint is combined with the spiral component of the other fingerprint . experiments on the west virginia university ( wvu ) and fvc2002 datasets show that mixing fingerprints has several benefits : ( a ) it can be used to generate virtual identities from two different fingers ; ( b ) it can be used to obscure the information present in an individual 's fingerprint image prior to storing it in a central database ; and ( c ) it can be used to generate a cancelable fingerprint template , i.e. , the template can be reset if the mixed fingerprint is compromised .
RANK 2
fingerprint combination for privacy protection we propose here a novel system for protecting fingerprint privacy by combining two different fingerprints into a new identity . in the enrollment , two fingerprints are captured from two different fingers . we extract the minutiae positions from one fingerprint , the orientation from the other fingerprint , and the reference points from both fingerprints . based on this extracted information and our proposed coding strategies , a combined minutiae template is generated and stored in a database . in the authentication , the system requires two query fingerprints from the same two fingers which are used in the enrollment . a two - stage fingerprint matching process is proposed for matching the two query fingerprints against a combined minutiae template . by storing the combined minutiae template , the complete minutiae feature of a single fingerprint will not be compromised when the database is stolen . furthermore , because of the similarity in topology , it is difficult for the attacker to distinguish a combined minutiae template from the original minutiae templates . with the help of an existing fingerprint reconstruction approach , we are able to convert the combined minutiae template into a real - look alike combined fingerprint . thus , a new virtual identity is created for the two different fingerprints , which can be matched using minutiae - based fingerprint matching algorithms . the experimental results show that our system can achieve a very low error rate with frr = 0.4 % at far = 0.1 % . compared with the state - of - the - art technique , our work has the advantage in creating a better new virtual identity when the two different fingerprints are randomly chosen .
RANK 3
a comparative study of fingerprint image - quality estimation methods one of the open issues in fingerprint verification is the lack of robustness against image - quality degradation . poor - quality images result in spurious and missing features , thus degrading the performance of the overall system . therefore , it is important for a fingerprint recognition system to estimate the quality and validity of the captured fingerprint images . in this work , we review existing approaches for fingerprint image - quality estimation , including the rationale behind the published measures and visual examples showing their behavior under different quality conditions . we have also tested a selection of fingerprint image - quality estimation algorithms . for the experiments , we employ the biosec multimodal baseline corpus , which includes 19 200 fingerprint images from 200 individuals acquired in two sessions with three different sensors . the behavior of the selected quality measures is compared , showing high correlation between them in most cases . the effect of low - quality samples in the verification performance is also studied for a widely available minutiae - based fingerprint matching system .
TOP 20
RANK = 1; score = 0.3925551388970897; correct = False; id = 807bc6bc7395805b8c4e179cee9d7b05434e8dd6
on mixing fingerprints this work explores the possibility of mixing two different fingerprints , pertaining to two different fingers , at the image level in order to generate a new fingerprint . to mix two fingerprints , each fingerprint pattern is decomposed into two different components , viz . , the continuous and spiral components . after prealigning the components of each fingerprint , the continuous component of one fingerprint is combined with the spiral component of the other fingerprint . experiments on the west virginia university ( wvu ) and fvc2002 datasets show that mixing fingerprints has several benefits : ( a ) it can be used to generate virtual identities from two different fingers ; ( b ) it can be used to obscure the information present in an individual 's fingerprint image prior to storing it in a central database ; and ( c ) it can be used to generate a cancelable fingerprint template , i.e. , the template can be reset if the mixed fingerprint is compromised .
RANK = 2; score = 0.34880652095409864; correct = False; id = da67d999b555f2f782b1cd4a80765509215044ce
fingerprint combination for privacy protection we propose here a novel system for protecting fingerprint privacy by combining two different fingerprints into a new identity . in the enrollment , two fingerprints are captured from two different fingers . we extract the minutiae positions from one fingerprint , the orientation from the other fingerprint , and the reference points from both fingerprints . based on this extracted information and our proposed coding strategies , a combined minutiae template is generated and stored in a database . in the authentication , the system requires two query fingerprints from the same two fingers which are used in the enrollment . a two - stage fingerprint matching process is proposed for matching the two query fingerprints against a combined minutiae template . by storing the combined minutiae template , the complete minutiae feature of a single fingerprint will not be compromised when the database is stolen . furthermore , because of the similarity in topology , it is difficult for the attacker to distinguish a combined minutiae template from the original minutiae templates . with the help of an existing fingerprint reconstruction approach , we are able to convert the combined minutiae template into a real - look alike combined fingerprint . thus , a new virtual identity is created for the two different fingerprints , which can be matched using minutiae - based fingerprint matching algorithms . the experimental results show that our system can achieve a very low error rate with frr = 0.4 % at far = 0.1 % . compared with the state - of - the - art technique , our work has the advantage in creating a better new virtual identity when the two different fingerprints are randomly chosen .
RANK = 3; score = 0.34138869467312233; correct = False; id = 41303b6350d1695d84fc0cb1ff3ff443aa5647e4
a comparative study of fingerprint image - quality estimation methods one of the open issues in fingerprint verification is the lack of robustness against image - quality degradation . poor - quality images result in spurious and missing features , thus degrading the performance of the overall system . therefore , it is important for a fingerprint recognition system to estimate the quality and validity of the captured fingerprint images . in this work , we review existing approaches for fingerprint image - quality estimation , including the rationale behind the published measures and visual examples showing their behavior under different quality conditions . we have also tested a selection of fingerprint image - quality estimation algorithms . for the experiments , we employ the biosec multimodal baseline corpus , which includes 19 200 fingerprint images from 200 individuals acquired in two sessions with three different sensors . the behavior of the selected quality measures is compared , showing high correlation between them in most cases . the effect of low - quality samples in the verification performance is also studied for a widely available minutiae - based fingerprint matching system .
RANK = 4; score = 0.33759485324976407; correct = False; id = a6babe3c1c23633348329b05a83d4d14494570ad
mimo authentication via deliberate fingerprinting at the physical layer we consider authentication of a wireless multiple - input - multiple - output ( mimo ) system by deliberately introducing a stealthy fingerprint at the physical layer . the fingerprint is superimposed onto the data and uniquely conveys an authentication message as a function of the transmitted data and a shared secret key . a symbol synchronous approach to fingerprint embedding provides low complexity operation . in comparison with a conventional tag - based authentication approach , fingerprinting conveys much less information on the secret key to an eavesdropper . we study the trade - offs between stealth , security , and robustness , and show that very good operating points exist . we consider the cases when deterministic or statistical channel state information is available to the transmitter , and show how precoding and channel mode power allocation can be applied to both the data and the fingerprint in combination to enhance the authentication process .
RANK = 5; score = 0.3375339113622152; correct = False; id = 7c150c39bdc1395babd336026122be6f1059c504
learning fingerprint reconstruction : from minutiae to image the set of minutia points is considered to be the most distinctive feature for fingerprint representation and is widely used in fingerprint matching . it was believed that the minutiae set does not contain sufficient information to reconstruct the original fingerprint image from which minutiae were extracted . however , recent studies have shown that it is indeed possible to reconstruct fingerprint images from their minutiae representations . reconstruction techniques demonstrate the need for securing fingerprint templates , improving the template interoperability , and improving fingerprint synthesis . but , there is still a large gap between the matching performance obtained from original fingerprint images and their corresponding reconstructed fingerprint images . in this paper , the prior knowledge about fingerprint ridge structures is encoded in terms of orientation patch and continuous phase patch dictionaries to improve the fingerprint reconstruction . the orientation patch dictionary is used to reconstruct the orientation field from minutiae , while the continuous phase patch dictionary is used to reconstruct the ridge pattern . experimental results on three public domain databases ( fvc2002 db1_a , fvc2002 db2_a , and nist sd4 ) demonstrate that the proposed reconstruction algorithm outperforms the state - of - the - art reconstruction algorithms in terms of both : 1 ) spurious minutiae and 2 ) matching performance with respect to type - i attack ( matching the reconstructed fingerprint against the same impression from which minutiae set was extracted ) and type - ii attack ( matching the reconstructed fingerprint against a different impression of the same finger ) .
RANK = 6; score = 0.33272810888603926; correct = False; id = 3a1da964b7ca842f002a56a7da755e9df0725b86
sensor fingerprint identification through composite fingerprints and group testing the photo response non - uniformity noise associated with an imaging sensor has been shown to be a unique and persistent identifier that can be treated as the sensor 's digital fingerprint . the method for attributing an image to a particular camera , however , is not suitable for source identification due to efficiency considerations , which is a one - to - many matching of a single fingerprint against a database of fingerprints . to address this problem , we propose a group - testing approach based on the notion of composite fingerprints ( cfs ) , generated by combining many actual fingerprints together into a single fingerprint . our technique organizes a database of fingerprints into an unordered binary search tree , wherein each internal node is represented by a fingerprint composited from all the fingerprints at the leaf nodes in the subtree beneath that node . different search strategies are considered , and the performance is analyzed analytically and verified using numerical simulations as well as experimental results . our results are presented in comparison with the linear search - based approach that utilizes fingerprint digests for more effective computation . results obtained under the best achievable accuracy showed that the proposed method yields a lower overall computational cost . it is also shown that by complementary use of the fingerprint dimension reduction and cf - based search tree approaches , it is possible to further improve the search efficiency .
RANK = 7; score = 0.3154542912805921; correct = False; id = 990916828e67c66d12ab7f85c0710893482d27ae
fingerprint - quality index using gradient components fingerprint image - quality checking is one of the most important issues in fingerprint recognition because recognition is largely affected by the quality of fingerprint images . in the past , many related fingerprint - quality checking methods have typically considered the condition of input images . however , when using the preprocessing algorithm , ridge orientation may sometimes be extracted incorrectly . unwanted false minutiae can be generated or some true minutiae may be ignored , which can also affect recognition performance directly . therefore , in this paper , we propose a novel quality - checking algorithm which considers the condition of the input fingerprints and orientation estimation errors . in the experiments , the 2-d gradients of the fingerprint images were first separated into two sets of 1-d gradients . then , the shapes of the probability density functions of these gradients were measured in order to determine fingerprint quality . we used the fvc2002 database and synthetic fingerprint images to evaluate the proposed method in three ways : 1 ) estimation ability of quality ; 2 ) separability between good and bad regions ; and 3 ) verification performance . experimental results showed that the proposed method yielded a reasonable quality index in terms of the degree of quality degradation . also , the proposed method proved superior to existing methods in terms of separability and verification performance .
RANK = 8; score = 0.31173500786357655; correct = False; id = 745b64caf359c3686b1c63e9852ba724949b9f2f
an improved scheme for full fingerprint reconstruction different fingerprint recognition systems store minutiae - based fingerprint templates differently . some store them inside a small token ; some can be found in a server database . as the minutiae template is very compact , many take it for granted that the template does not contain sufficient information for reconstructing the original fingerprint . this paper proposes a scheme to reconstruct a full fingerprint image from the minutiae points based on the amplitude and frequency modulated ( am - fm ) fingerprint model . the scheme starts with generating a binary ridge pattern which has a similar ridge flow to that of the original fingerprint . the continuous phase is intuitively reconstructed by removing the spirals in the phase image estimated from the ridge pattern . to reduce the artifacts due to the discontinuity in the continuous phase , a refinement process is introduced for the reconstructed phase image , which is the combination of the continuous phase and the spiral phase ( corresponding to the minutiae ) . finally , the refined phase image is used to produce a thinned version of the fingerprint , from which a real - look alike gray - scale fingerprint image is reconstructed . the experimental results show that our proposed scheme performs better than the - state - of - the - art technique .
RANK = 9; score = 0.3110198541603304; correct = False; id = a0b923918456c9c67e78656689b5ae5fd0b2af86
a high performance fingerprint matching system for large databases based on gpu fingerprints are the biometric features most used for identification . they can be characterized through some particular elements called minutiae . the identification of a given fingerprint requires the matching of its minutiae against the minutiae of other fingerprints . hence , fingerprint matching is a key process . the efficiency of current matching algorithms does not allow their use in large fingerprint databases ; to apply them , a breakthrough in running performance is necessary . nowadays , the minutia cylinder - code ( mcc ) is the best performing algorithm in terms of accuracy . however , a weak point of this algorithm is its computational requirements . in this paper , we present a gpu fingerprint matching system based on mcc . the many - core computing framework provided by cuda on nvidia tesla and geforce hardware platforms offers an opportunity to enhance fingerprint matching . through a thorough and careful data structure , computation and memory transfer design , we have developed a system that keeps its accuracy and reaches a speed - up up to 100.8× compared with a reference sequential cpu implementation . a rigorous empirical study over captured and synthetic fingerprint databases shows the efficiency of our proposal . these results open up a whole new field of possibilities for reliable real time fingerprint identification in large databases .
RANK = 10; score = 0.3039948180295627; correct = False; id = 870527ec0dac8803a0420c4cb6e8a8c9d8af5679
efficient sensor fingerprint matching through fingerprint binarization it is now established that photo - response nonuniformity noise pattern can be reliably used as a fingerprint to identify an image sensor . the large size and random nature of sensor fingerprints , however , make them inconvenient to store . further , associated fingerprint matching method can be computationally expensive , especially for applications that involve large - scale databases . to address these limitations , we propose to represent sensor fingerprints in binary - quantized form . it is shown through both analytical study and simulations that the reduction in matching accuracy due to quantization is insignificant as compared to conventional approaches . experiments on actual sensor fingerprint data are conducted to confirm that only a slight increase occurred in the probability of error and to demonstrate the computational efficacy of the approach .
RANK = 11; score = 0.29878954283468623; correct = False; id = 12d3dbe11fb18c5c62b2ff15ae42efe9b04cfaf4
fingerprint - based fuzzy vault : implementation and performance reliable information security mechanisms are required to combat the rising magnitude of identity theft in our society . while cryptography is a powerful tool to achieve information security , one of the main challenges in cryptosystems is to maintain the secrecy of the cryptographic keys . though biometric authentication can be used to ensure that only the legitimate user has access to the secret keys , a biometric system itself is vulnerable to a number of threats . a critical issue in biometric systems is to protect the template of a user which is typically stored in a database or a smart card . the fuzzy vault construct is a biometric cryptosystem that secures both the secret key and the biometric template by binding them within a cryptographic framework . we present a fully automatic implementation of the fuzzy vault scheme based on fingerprint minutiae . since the fuzzy vault stores only a transformed version of the template , aligning the query fingerprint with the template is a challenging task . we extract high curvature points derived from the fingerprint orientation field and use them as helper data to align the template and query minutiae . the helper data itself do not leak any information about the minutiae template , yet contain sufficient information to align the template and query fingerprints accurately . further , we apply a minutiae matcher during decoding to account for nonlinear distortion and this leads to significant improvement in the genuine accept rate . we demonstrate the performance of the vault implementation on two different fingerprint databases . we also show that performance improvement can be achieved by using multiple fingerprint impressions during enrollment and verification .
RANK = 12; score = 0.2929618460915675; correct = False; id = 76193ed7cea1716ef1d912954b028169359f2a86
latent fingerprint enhancement via multi - scale patch based sparse representation latent fingerprint identification plays an important role for identifying and convicting criminals in law enforcement agencies . latent fingerprint images are usually of poor quality with unclear ridge structure and various overlapping patterns . although significant advances have been achieved on developing automated fingerprint identification system , it is still challenging to achieve reliable feature extraction and identification for latent fingerprints due to the poor image quality . prior to feature extraction , fingerprint enhancement is necessary to suppress various noises , and improve the clarity of ridge structures in latent fingerprints . motivated by the recent success of sparse representation in image denoising , this paper proposes a latent fingerprint enhancement algorithm by combining the total variation model and multiscale patch - based sparse representation . first , the total variation model is applied to decompose the latent fingerprint into cartoon and texture components . the cartoon component with most of the nonfingerprint patterns is removed as the structured noise , whereas the texture component consisting of the weak latent fingerprint is enhanced in the next stage . second , we propose a multiscale patch - based sparse representation method for the enhancement of the texture component . dictionaries are constructed with a set of gabor elementary functions to capture the characteristics of fingerprint ridge structure , and multiscale patch - based sparse representation is iteratively applied to reconstruct high - quality fingerprint image . the proposed algorithm can not only remove the overlapping structured noises , but also restore and enhance the corrupted ridge structures . in addition , we present an automatic method to segment the foreground of latent image with the sparse coefficients and orientation coherence . experimental results and comparisons on nist sd27 latent fingerprint database are presented to show the effectiveness of the proposed algorithm and its superiority over existing algorithms .
RANK = 13; score = 0.28792299227548085; correct = False; id = 46e5a989f3a342f04354f74b0153db2f3bf7a39a
on the operational quality of fingerprint scanners this paper addresses the problem of evaluating the ldquooperational qualityrdquo of fingerprint scanners , that is , the ability of acquiring images that maximize the accuracy of automated fingerprint recognition . the quality parameters commonly used to quantify the fidelity of a scanner in sensing the input pattern have been analyzed and a large experimentation has been carried out to understand their effects on fingerprint recognition accuracy . the experimental results show that some parameters have a strong impact , while others appear to be less relevant .
RANK = 14; score = 0.28351510325643214; correct = False; id = 1501e73d22ba003ef37c2087d4dda6d6e61c497e
assessing fingerprint individuality in presence of noisy minutiae fingerprint image quality is an important source of intraclass variability . when the underlying image quality is poor , human experts as well as automatic systems are more likely to make errors in minutiae detection and matching by either missing true features or detecting spurious ones . as a consequence , fingerprint individuality estimates change depending on the quality of the underlying images . the goal of this paper is to quantitatively study the effect of noise in minutiae detection and localization , resulting from varying image quality , on fingerprint individuality . the measure of fingerprint individuality is modeled as a function of image quality via a random effects model and methodology for the estimation of unknown parameters is developed in a bayesian framework . empirical results on two databases , one in - house and another publicly available , demonstrate how the measure of fingerprint individuality increases as image quality becomes poor . the measure corresponding to the ¿ 12-point match ¿ with 26 observed minutiae in the query and template fingerprints increases by several orders of magnitude when the fingerprint quality degrades from ¿ best ¿ to ¿ poor¿.
RANK = 15; score = 0.2831142566974344; correct = False; id = 0d30eea0d07e566dee59bcb7d431604edad70a1a
separating overlapped fingerprints fingerprint images generally contain either a single fingerprint ( e.g. , rolled images ) or a set of nonoverlapped fingerprints ( e.g. , slap fingerprints ) . however , there are situations where several fingerprints overlap on top of each other . such situations are frequently encountered when latent ( partial ) fingerprints are lifted from crime scenes or residue fingerprints are left on fingerprint sensors . overlapped fingerprints constitute a serious challenge to existing fingerprint recognition algorithms , since these algorithms are designed under the assumption that fingerprints have been properly segmented . in this paper , a novel algorithm is proposed to separate overlapped fingerprints into component or individual fingerprints . the basic idea is to first estimate the orientation field of the given image with overlapped fingerprints and then separate it into component orientation fields using a relaxation labeling technique . we also propose an algorithm to utilize fingerprint singularity information to further improve the separation performance . experimental results indicate that the algorithm leads to good separation of overlapped fingerprints that leads to a significant improvement in the matching accuracy .
RANK = 16; score = 0.27739231937644965; correct = False; id = bca849ba0592d26458af6f3a746d99bca7f50298
distal - interphalangeal - crease - based user authentication system touchless - based fingerprint recognition technology is thought to be an alternative to touch - based systems to solve problems of hygienic , latent fingerprints , and maintenance . however , there are few studies about touchless fingerprint recognition systems due to the lack of a large database and the intrinsic drawback of low ridge - valley contrast of touchless fingerprint images . this paper proposes an end - to - end solution for user authentication systems based on touchless fingerprint images in which a multiview strategy is adopted to collect images and the robust fingerprint feature of touchless image is extracted for matching with high recognition accuracy . more specifically , a touchless multiview fingerprint capture device is designed to generate three views of raw images followed by preprocessing steps including region of interest ( roi ) extraction and image correction . the distal interphalangeal crease ( dip)-based feature is then extracted and matched to recognize the human 's identity in which part selection is introduced to improve matching efficiency . experiments are conducted on two sessions of touchless multiview fingerprint image database with 541 fingers acquired about two weeks apart . an eer of ~ 1.7 % can be achieved by using the proposed dip - based feature , which is much better than touchless fingerprint recognition by using scale invariant feature transformation ( sift ) and minutiae features . the given fusion results show that it is effective to combine the dip - based feature , minutiae , and sift feature for touchless fingerprint recognition systems . the eer is as low as ~ 0.5 % .
RANK = 17; score = 0.27345475735885616; correct = False; id = 4eee3f80b4af8da12b69917f4325260d99d7fc9d
using fingerprint authentication to reduce system security : an empirical study choosing the security architecture and policies for a system is a demanding task that must be informed by an understanding of user behavior . we investigate the hypothesis that adding visible security features to a system increases user confidence in the security of a system and thereby causes users to reduce how much effort they spend in other security areas . in our study , 96 volunteers each created a pair of accounts , one secured only by a password and one secured by both a password and a fingerprint reader . our results strongly support our hypothesis -- on average . when using the fingerprint reader , users created passwords that would take one three - thousandth as long to break , thereby potentially negating the advantage two - factor authentication could have offered .
RANK = 18; score = 0.27095766372998065; correct = False; id = 67a8fae1a9b83d1035dadeab55de1e616e6d4bb1
an analysis of lightweight encryption schemes for fingerprint images two lightweight encryption schemes for fingerprint images based on a bit - plane representation of the data are assessed . we demonstrate a low complexity attack against a scheme recently proposed in literature which exploits one of several weaknesses found . a second scheme is evaluated with respect to two fingerprint recognition systems and recommendations for its safe use are given .
RANK = 19; score = 0.26576253573914205; correct = False; id = 42788c11eae69c700b3683b75240a62b686eb13e
fingerprint liveness detection from single image using low - level features and shape analysis fingerprint - based authentication systems have developed rapidly in the recent years . however , current fingerprint - based biometric systems are vulnerable to spoofing attacks . moreover , single feature - based static approach does not perform equally over different fingerprint sensors and spoofing materials . in this paper , we propose a static software approach . we propose to combine low - level gradient features from speeded - up robust features , pyramid extension of the histograms of oriented gradient and texture features from gabor wavelet using dynamic score level integration . we extract these features from a single fingerprint image to overcome the issues faced in dynamic software approaches , which require user cooperation and longer computational time . a experimental analysis done on livdet 2011 data produced an average equal error rate ( eer ) of 3.95 % over four databases . the result outperforms the existing best average eer of 9.625 % . we also performed experiments with livdet 2013 database and achieved an average classification error rate of 2.27 % in comparison with 12.87 % obtained by the livdet 2013 competition winner .
RANK = 20; score = 0.2634218629906522; correct = False; id = 47eda60ec46f948c6c823ea277dd4c5a907102e4
improvement of fingerprint retrieval by a statistical classifier the topics of fingerprint classification , indexing , and retrieval have been studied extensively in the past decades . one problem faced by researchers is that in all publicly available fingerprint databases , only a few fingerprint samples from each individual are available for training and testing , making it inappropriate to use sophisticated statistical methods for recognition . hence most of the previous works resorted to simple k - nearest neighbor ( k - nn ) classification . however , the k - nn classifier has the drawbacks of being comparatively slow and less accurate . in this paper , we tackle this problem by first artificially expanding the set of training samples using our previously proposed spatial modeling technique . with the expanded training set , we are then able to employ a more sophisticated classifier such as the bayes classifier for recognition . we apply the proposed method to the problem of one - to - n fingerprint identification and retrieval . the accuracy and speed are evaluated using the benchmarking fvc 2000 , fvc 2002 , and nist-4 databases , and satisfactory retrieval performance is achieved .

RANKING 374
QUERY
duth at semeval-2017 task 4 : a voting classification approach for twitter sentiment analysis this report describes our participation to semeval-2017 task 4 : sentiment analysis in twitter , specifically in subtasks a , b , and c. the approach for text sentiment classification is based on a majority vote scheme and combined supervised machine learning methods with classical linguistic resources , including bag - of - words and sentiment lexicon features .
First cited at 7
TOP CITED PAPERS
RANK 7
twise at semeval-2016 task 4 : twitter sentiment classification this paper describes the participation of the team “ twise ” in the semeval 2016 challenge . specifically , we participated in task 4 , namely “ sentiment analysis in twitter ” for which we implemented sentiment classification systems for subtasks a , b , c and d. our approach consists of two steps . in the first step , we generate and validate diverse feature sets for twitter sentiment evaluation , inspired by the work of participants of previous editions of such challenges . in the second step , we focus on the optimization of the evaluation measures of the different subtasks . to this end , we examine different learning strategies by validating them on the data provided by the task organisers . for our final submissions we used an ensemble learning approach ( stacked generalization ) for subtask a and single linear models for the rest of the subtasks . in the official leaderboard we were ranked 9/35 , 8/19 , 1/11 and 2/14 for subtasks a , b , c and d respectively . the code can be found at https://github.com/ balikasg / semeval2016-twitter _ sentiment_evaluation .
RANK 92
sentiment analysis in social media texts this paper presents a method for sentiment analysis specifically designed to work with twitter data ( tweets ) , taking into account their structure , length and specific language . the approach employed makes it easily extendible to other languages and makes it able to process tweets in near real time . the main contributions of this work are : a ) the pre - processing of tweets to normalize the language and generalize the vocabulary employed to express sentiment ; b ) the use minimal linguistic processing , which makes the approach easily portable to other languages ; c ) the inclusion of higher order n - grams to spot modifications in the polarity of the sentiment expressed ; d ) the use of simple heuristics to select features to be employed ; e ) the application of supervised learning using a simple support vector machines linear classifier on a set of realistic data . we show that using the training models generated with the method described we can improve the sentiment classification performance , irrespective of the domain and distribution of the test sets .
RANK 234
recognizing contextual polarity in phrase - level sentiment analysis this paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions . with this approach , the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions , achieving results that are significantly better than baseline .
TOP UNCITED PAPERS
RANK 1
sentisys at semeval-2016 task 4 : feature - based system for sentiment analysis in twitter this paper describes our sentiment analysis system which has been built for sentiment analysis in twitter task of semeval-2016 . we have used a logistic regression classifier with different groups of features . this system is an improvement to our previous system lsislif in semeval-2015 after removing some features and adding new features extracted from a new automatic constructed sentiment lexicon .
RANK 2
su - sentilab : a classification system for sentiment analysis in twitter sentiment analysis refers to automatically extracting the sentiment present in a given natural language text . we present our participation to the semeval2013 competition , in the sentiment analysis of twitter and sms messages . our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the final system . both subsystems use supervised learning using features based on various polarity lexicons .
RANK 3
semeval-2015 task 10 : sentiment analysis in twitter in this paper , we describe the 2015 iteration of the semeval shared task on sentiment analysis in twitter . this was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years . this year ’s shared task competition consisted of five sentiment prediction subtasks . two were reruns from previous years : ( a ) sentiment expressed by a phrase in the context of a tweet , and ( b ) overall sentiment of a tweet . we further included three new subtasks asking to predict ( c ) the sentiment towards a topic in a single tweet , ( d ) the overall sentiment towards a topic in a set of tweets , and ( e ) the degree of prior polarity of a phrase .
TOP 20
RANK = 1; score = 0.4366629563400032; correct = False; id = 9f192a2395081ede063376e76df5cecbcbe48d5f
sentisys at semeval-2016 task 4 : feature - based system for sentiment analysis in twitter this paper describes our sentiment analysis system which has been built for sentiment analysis in twitter task of semeval-2016 . we have used a logistic regression classifier with different groups of features . this system is an improvement to our previous system lsislif in semeval-2015 after removing some features and adding new features extracted from a new automatic constructed sentiment lexicon .
RANK = 2; score = 0.43481291002412076; correct = False; id = 2b1f358315314d3531f210977080e53eda2088a8
su - sentilab : a classification system for sentiment analysis in twitter sentiment analysis refers to automatically extracting the sentiment present in a given natural language text . we present our participation to the semeval2013 competition , in the sentiment analysis of twitter and sms messages . our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the final system . both subsystems use supervised learning using features based on various polarity lexicons .
RANK = 3; score = 0.41563250707572413; correct = False; id = 8208d5644984a8ef5d10c9bc2f898f539d189263
semeval-2015 task 10 : sentiment analysis in twitter in this paper , we describe the 2015 iteration of the semeval shared task on sentiment analysis in twitter . this was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years . this year ’s shared task competition consisted of five sentiment prediction subtasks . two were reruns from previous years : ( a ) sentiment expressed by a phrase in the context of a tweet , and ( b ) overall sentiment of a tweet . we further included three new subtasks asking to predict ( c ) the sentiment towards a topic in a single tweet , ( d ) the overall sentiment towards a topic in a set of tweets , and ( e ) the degree of prior polarity of a phrase .
RANK = 4; score = 0.401583733491602; correct = False; id = e76bb1efe31ece10965a7c6b8b393a58283b1437
[ lvic - limsi ] : using syntactic features and multi - polarity words for sentiment analysis in twitter this paper presents the contribution of our team at task 2 of semeval 2013 : sentiment analysis in twitter . we submitted a constrained run for each of the two subtasks . in the contextual polarity disambiguation subtask , we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers . in the message polarity classification subtask , we focus on the influence of domain information on sentiment classification .
RANK = 5; score = 0.38914762501066513; correct = False; id = 4949cea065dce681464862799d7a98b0128da4f8
kea : expression - level sentiment analysis from twitter data this paper describes an expression - level sentiment detection system that participated in the subtask a of semeval-2013 task 2 : sentiment analysis in twitter . our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive , negative or neutral . the proposed approach helps to understand the relevant features that contribute most in this classification task .
RANK = 6; score = 0.38886440224186813; correct = False; id = f4ee6d64e16fe5d5123bed5198848b9867bc2a9b
sentiment lexicon interpolation and polarity estimation of objective and out - of - vocabulary words to improve sentiment classification on microblogging sentiment analysis has become an important classification task because a large amount of user - generated content is published over the internet . sentiment lexicons have been used successfully to classify the sentiment of user review datasets . more recently , microblogging services such as twitter have become a popular data source in the domain of sentiment analysis . however , analyzing sentiments on tweets is still difficult because tweets are very short and contain slang , informal expressions , emoticons , mistyping and many words not found in a dictionary . in addition , more than 90 percent of the words in public sentiment lexicons , such as sentiwordnet , are objective words , which are often considered less important in a classification module . in this paper , we introduce a hybrid approach that incorporates sentiment lexicons into a machine learning approach to improve sentiment classification in tweets . we automatically construct an add - on lexicon that compiles the polarity scores of objective words and out - ofvocabulary ( oov ) words from tweet corpora . we also introduce a novel feature weighting method by interpolating sentiment lexicon score into uni - gram vectors in the support vector machine ( svm ) . results of our experiment show that our method is effective and significantly improves the sentiment classification accuracy compared to a baseline unigram model .
RANK = 7; score = 0.38547605392986595; correct = True; id = 2d5321cf92547cd4db15b9b6995e09621777802b
twise at semeval-2016 task 4 : twitter sentiment classification this paper describes the participation of the team “ twise ” in the semeval 2016 challenge . specifically , we participated in task 4 , namely “ sentiment analysis in twitter ” for which we implemented sentiment classification systems for subtasks a , b , c and d. our approach consists of two steps . in the first step , we generate and validate diverse feature sets for twitter sentiment evaluation , inspired by the work of participants of previous editions of such challenges . in the second step , we focus on the optimization of the evaluation measures of the different subtasks . to this end , we examine different learning strategies by validating them on the data provided by the task organisers . for our final submissions we used an ensemble learning approach ( stacked generalization ) for subtask a and single linear models for the rest of the subtasks . in the official leaderboard we were ranked 9/35 , 8/19 , 1/11 and 2/14 for subtasks a , b , c and d respectively . the code can be found at https://github.com/ balikasg / semeval2016-twitter _ sentiment_evaluation .
RANK = 8; score = 0.3845646841259338; correct = False; id = 151a54ce5fcaaf6f62838999cca59a4a9e197885
ntnusenteval at semeval-2016 task 4 : combining general classifiers for fast twitter sentiment analysis . the paper describes experiments on sentiment classification of microblog messages using an architecture allowing general machine learning classifiers to be combined either sequentially to form a multi - step classifier , or in parallel , creating an ensemble classifier . the system achieved very competitive results in the shared task on sentiment analysis in twitter , in particular on non - twitter social media data , that is , input it was not specifically tailored to .
RANK = 9; score = 0.3799457079971691; correct = False; id = e6aef043a873d32106129b65ad3a180cd8a5fda2
teragram : rule - based detection of sentiment phrases using sas sentiment analysis for semeval-2013 task 2 , a and b ( sentiment analysis in twitter ) , we use a rulebased pattern matching system that is based on an existing ‘ domain independent’ sentiment taxonomy for english , essentially a highly phrasal sentiment lexicon . we have made some modifications to our set of rules , based on what we found in the annotated training data that was made available for the task . the resulting system scores competitively , especially on task b.
RANK = 10; score = 0.3789071308817813; correct = False; id = 670629e414dc085fbabded69586e1ac5c03fbeb5
insight-1 at semeval-2016 task 4 : convolutional neural networks for sentiment classification and quantification this paper describes our deep learning - based approach to sentiment analysis in twitter as part of semeval-2016 task 4 . we use a convolutional neural network to determine sentiment and participate in all subtasks , i.e. two - point , three - point , and five - point scale sentiment classification and two - point and five - point scale sentiment quantification . we achieve competitive results for two - point scale sentiment classification and quantification , ranking fifth and a close fourth ( third and second by alternative metrics ) respectively despite using only pre - trained embeddings that contain no sentiment information . we achieve good performance on three - point scale sentiment classification , ranking eighth out of 35 , while performing poorly on fivepoint scale sentiment classification and quantification . an error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information . we propose improvements in order to address these and other issues .
RANK = 11; score = 0.3762946456971942; correct = False; id = bb0baab42046a6d705d89f62c160db730e3d0769
ecnu at semeval-2016 task 5 : extracting effective features from relevant fragments in sentence for aspect - based sentiment analysis in reviews this paper describes our systems submitted to the sentence - level and text - level aspectbased sentiment analysis ( absa ) task ( i.e. , task 5 ) in semeval-2016 . the task involves two phases , namely , aspect detection phase and sentiment polarity classification phase . we participated in the second phase of both subtasks in laptop and restaurant domains , which focuses on the sentiment analysis based on the given aspect . in this task , we extracted four types of features ( i.e. , sentiment lexicon features , linguistic features , topic model features and word2vec feature ) from certain fragments related to aspect rather than the whole sentence . then the proposed features are fed into supervised classifiers for sentiment analysis . our submissions rank above average .
RANK = 12; score = 0.37503752064960233; correct = False; id = 8797763a3dedce3f40e4ee69efdb3b55d6f985b8
aueb.twitter.sentiment at semeval-2016 task 4 : a weighted ensemble of svms for twitter sentiment analysis this paper describes the system with which we participated in semeval-2016 task 4 ( sentiment analysis in twitter ) and specifically the message polarity classification subtask . our system is a weighted ensemble of two systems . the first one is based on a previous sentiment analysis system and uses manually crafted features . the second system of our ensemble uses features based on word embeddings . our ensemble was ranked 5th among 34 teams . the source code of our system is publicly available .
RANK = 13; score = 0.366780934706883; correct = False; id = adfd48db819ce54375d9647b32b7596a3107d57e
improving twitter sentiment analysis with topic - based mixture modeling and semi - supervised training in this paper , we present multiple approaches to improve sentiment analysis on twitter data . we first establish a state - of - the - art baseline with a rich feature set . then we build a topic - based sentiment mixture model with topic - specific data in a semi - supervised training framework . the topic information is generated through topic modeling based on an efficient implementation of latent dirichlet allocation ( lda ) . the proposed sentiment model outperforms the top system in the task of sentiment analysis in twitter in semeval-2013 in terms of averaged f scores .
RANK = 14; score = 0.36627011884722127; correct = False; id = 27ee75e2047bac3ae9fc3b4f09aa610d14104f5c
learning sentiment - specific word embedding for twitter sentiment classification we present a method that learns word embedding for twitter sentiment classification in this paper . most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text . this is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors . we address this issue by learning sentimentspecific word embedding ( sswe ) , which encodes sentiment information in the continuous representation of words . specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions . to obtain large scale training corpora , we learn the sentiment - specific word embedding from massive distant - supervised tweets collected by positive and negative emoticons . experiments on applying sswe to a benchmark twitter sentiment classification dataset in semeval 2013 show that ( 1 ) the sswe feature performs comparably with hand - crafted features in the top - performed system ; ( 2 ) the performance is further improved by concatenating sswe with existing feature set .
RANK = 15; score = 0.3613293047246562; correct = False; id = 6ad42df65db04e8b0365c354048659dd1de4c99e
a joint segmentation and classification framework for sentiment analysis in this paper , we propose a joint segmentation and classification framework for sentiment analysis . existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as “ not bad ” and “ a great deal of ” . we address this issue by developing a joint segmentation and classification framework ( jsc ) , which simultaneously conducts sentence segmentation and sentence - level sentiment classification . specifically , we use a log - linear model to score each segmentation candidate , and exploit the phrasal information of top - ranked segmentations as features to build the sentiment classifier . a marginal log - likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance . the joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations . experiments on a benchmark twitter sentiment classification dataset in semeval 2013 show that , our joint model performs comparably with the state - of - the - art methods .
RANK = 16; score = 0.3588374109845166; correct = False; id = 689bf2d64bd37c36e3d9db1f57e6bdd6809451c3
put at semeval-2016 task 4 : the abc of twitter sentiment analysis this paper describes a classification system that participated in semeval-2016 task 4 : sentiment analysis in twitter . the proposed approach competed in subtasks a , b , and c , which involved tweet polarity classification , tweet classification according to a two - point scale , and tweet classification according to a five - point scale . our system is based on an ensemble consisting of random forests , svms , and gradient boosting trees , and involves the use of a wide range of features including : ngrams , brown clustering , sentiment lexicons , wordnet , and part - of - speech tagging . the proposed system achieved 14th , 6th , and 3rd place in subtasks a , b , and c , respectively .
RANK = 17; score = 0.35662322535730734; correct = False; id = b2ad3c5f784ace1566fc37f29ed13ba0c235586f
uir - pku : twitter - opinminer system for sentiment analysis in twitter at semeval 2015 microblogs are considered as we - media information with many real - time opinions . this paper presents a twitter - opinminer system for twitter sentiment analysis evaluation at semeval 2015 . our approach stems from two different angles : topic detection for discovering the sentiment distribution on different topics and sentiment analysis based on a variety of features . moreover , we also implemented intra - sentence discourse relations for polarity identification . we divided the discourse relations into 4 predefined categories , including continuation , contrast , condition , and cause . these relations could facilitate us to eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing . based on the semeval 2014 and semeval 2015 twitter sentiment analysis task datasets , the experimental results show that the performance of twitter - opinminer could effectively recognize opinionated messages and identify the polarities .
RANK = 18; score = 0.34991146563826575; correct = False; id = c22093adfff5bf1a55fa9a79ee0122d4e44d55bb
ecnu at semeval-2016 task 4 : an empirical investigation of traditional nlp features and word embedding features for sentence - level and topic - level sentiment analysis in twitter this paper reports our submissions to task 4 , i.e. , sentiment analysis in twitter ( sat ) , in semeval 2016 , which consists of five subtasks grouped into two levels : ( 1 ) sentence level , i.e. , message polarity classification ( subtask a ) , and ( 2 ) topic level , i.e. , tweet classification and quantification according to two - point scale ( subtask b and d ) or five - point scale ( subtask c and e ) . we participated in all these five subtasks . to address these subtasks , we investigated several traditional natural language processing ( nlp ) features including sentiment lexicon , linguistic and domain specific features , and word embedding features together with supervised machine learning methods . officially released results showed that our systems rank above average .
RANK = 19; score = 0.3484142782228351; correct = False; id = 577621509af4f8e343a4d94504a52fb32e3fe882
klue : simple and robust methods for polarity classification this paper describes our approach to the semeval-2013 task on “ sentiment analysis in twitter ” . we use simple bag - of - words models , a freely available sentiment dictionary automatically extended with distributionally similar terms , as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms . the resulting system is resource - lean , making it relatively independent of a specific language . despite its simplicity , the system achieves competitive accuracies of 0.70–0.72 in detecting the sentiment of text messages . we also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message .
RANK = 20; score = 0.34677382233091664; correct = False; id = 8ca1fd4e167f3e26ff5a76ceb637ac1db423a7d6
twitterhawk : a feature bucket based approach to sentiment analysis this paper describes twitterhawk , a system for sentiment analysis of tweets which participated in the semeval-2015 task 10 , subtasks a through d. the system performed competitively , most notably placing 1 in topicbased sentiment classification ( subtask c ) and ranking 4 out of 40 in identifying the sentiment of sarcastic tweets . our submissions in all four subtasks used a supervised learning approach to perform three - way classification to assign positive , negative , or neutral labels . our system development efforts focused on text pre - processing and feature engineering , with a particular focus on handling negation , integrating sentiment lexicons , parsing hashtags , and handling expressive word modifications and emoticons . two separate classifiers were developed for phrase - level and tweetlevel sentiment classification . our success in aforementioned tasks came in part from leveraging the subtask b data and building a single tweet - level classifier for subtasks b , c and d.

RANKING 2272
QUERY
universal dependencies for arabic we describe the process of creating nudar , a universal dependency treebank for arabic . we present the conversion from the penn arabic treebank to the universal dependency syntactic representation through an intermediate dependency representation . we discuss the challenges faced in the conversion of the trees , the decisions we made to solve them , and the validation of our conversion . we also present initial parsing results on nudar .
First cited at 5
TOP CITED PAPERS
RANK 5
universal dependency annotation for multilingual parsing we present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : german , english , swedish , spanish , french and korean . to show the usefulness of such a resource , we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before . this ‘ universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1
RANK 124
catib : the columbia arabic treebank the columbia arabic treebank ( catib ) is a database of syntactic analyses of arabic sentences . catib contrasts with previous approaches to arabic treebanking in its emphasis on speed with some constraints on linguistic richness . two basic ideas inspire the catib approach : no annotation of redundant information and using representations and terminology inspired by traditional arabic syntax . we describe catib ’s representation and annotation procedure , and report on interannotator agreement and speed .
RANK 139
automatic morphological enrichment of a morphologically underspecified treebank in this paper , we study the problem of automatic enrichment of a morphologically underspecified treebank for arabic , a morphologically rich language . we show that we can map from a tagset of size six to one with 485 tags at an accuracy rate of 94%-95 % . we can also identify the unspecified lemmas in the treebank with an accuracy over 97 % . furthermore , we demonstrate that using our automatic annotations improves the performance of a state - of - the - art arabic morphological tagger . our approach combines a variety of techniques from corpus - based statistical models to linguistic rules that target specific phenomena . these results suggest that the cost of treebanking can be reduced by designing underspecified treebanks that can be subsequently enriched automatically .
TOP UNCITED PAPERS
RANK 1
better automatic treebank conversion using a feature - based approach for the task of automatic treebank conversion , this paper presents a feature - based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard . experiments on two chinese treebanks show that our approach improves conversion accuracy by 1.31 % over a strong baseline .
RANK 2
converting russian dependency treebank to stanford typed dependencies representation in this paper , we describe the process of rulebased conversion of russian dependency treebank into the stanford dependency ( sd ) schema . the motivation behind this project is the expansion of the number of languages that have treebank resources available in one consistent annotation schema . conversion includes creation of russian - specific sd guidelines , defining conversion rules from the original treebank schema into the sd model and evaluation of the conversion results . the converted treebank becomes part of a multilingual resource for nlp purposes .
RANK 3
universal decompositional semantics on universal dependencies we present a framework for augmenting data sets from the universal dependencies project with universal decompositional semantics . where the universal dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard , our extension has similar aims for semantic annotation . we describe results from annotating the english universal dependencies treebank , dealing with word senses , semantic roles , and event properties .
TOP 20
RANK = 1; score = 0.39723487692769244; correct = False; id = b5f55f6166132746c1d54466382fb2fc9495c61a
better automatic treebank conversion using a feature - based approach for the task of automatic treebank conversion , this paper presents a feature - based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard . experiments on two chinese treebanks show that our approach improves conversion accuracy by 1.31 % over a strong baseline .
RANK = 2; score = 0.39133582197126526; correct = False; id = 1a3ad40e8d6aba07a590982d2c1eafc8d6e2f3e9
converting russian dependency treebank to stanford typed dependencies representation in this paper , we describe the process of rulebased conversion of russian dependency treebank into the stanford dependency ( sd ) schema . the motivation behind this project is the expansion of the number of languages that have treebank resources available in one consistent annotation schema . conversion includes creation of russian - specific sd guidelines , defining conversion rules from the original treebank schema into the sd model and evaluation of the conversion results . the converted treebank becomes part of a multilingual resource for nlp purposes .
RANK = 3; score = 0.3742944641006875; correct = False; id = 00dc74ca39fc6630bef824a4768dd214bbd81927
universal decompositional semantics on universal dependencies we present a framework for augmenting data sets from the universal dependencies project with universal decompositional semantics . where the universal dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard , our extension has similar aims for semantic annotation . we describe results from annotating the english universal dependencies treebank , dealing with word senses , semantic roles , and event properties .
RANK = 4; score = 0.3640809146711894; correct = False; id = 0b20ae6d044b432eb8f85698e1577568da7a68a1
multiple - step treebank conversion : from dependency to penn format whilst the degree to which a treebank subscribes to a specific linguistic theory limits the usefulness of the resource , the availability of more formats for the same resource plays a crucial role both in nlp and linguistics . conversion tools and multi - format treebanks are useful for investigating portability of nlp systems and validity of annotation . unfortunately , conversion is a quite complex task since it involves grammatical rules and linguistic knowledge to be incorporated into the converter program . the paper focusses on a methodology for treebank conversion which consists in splitting the process in steps corresponding to the kinds of information that have to be converted , i.e. morphological , structural or relational syntactic . the advantage is the generation of a set of parallel treebanks featuring progressively differentiated formats . an application to the case of an italian dependency - based treebank in a penn like format is described .
RANK = 5; score = 0.34286969200735967; correct = True; id = 5808b096982bc76512be5c3ce2e9a02cc32a4919
universal dependency annotation for multilingual parsing we present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : german , english , swedish , spanish , french and korean . to show the usefulness of such a resource , we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before . this ‘ universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1
RANK = 6; score = 0.34262472445608977; correct = False; id = c08662f21ba1efe36e8493e275e017297ad3ae4b
an automatic treebank conversion algorithm for corpus sharing an automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank . a new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research communities . the simple algorithm achieves conversion accuracy of 96.4 % when tested on 8,867 sentences between two major grammar revisions of a large mt system .
RANK = 7; score = 0.3330074747639957; correct = False; id = 5e2d3027a6181c326b0ac93719d08a9148d290e1
using supertags and encoded annotation principles for improved dependency to phrase structure conversion we investigate the problem of automatically converting from a dependency representation to a phrase structure representation , a key aspect of understanding the relationship between these two representations for nlp work . we implement a new approach to this problem , based on a small number of supertags , along with an encoding of some of the underlying principles of the penn treebank guidelines . the resulting system significantly outperforms previous work in such automatic conversion . we also achieve comparable results to a system using a phrase - structure parser for the conversion . a comparison with our system using either the part - of - speech tags or the supertags provides some indication of what the parser is contributing .
RANK = 8; score = 0.32985534630158503; correct = False; id = c305e3314c0853b14911f704c68b04cfc9ea7aa1
conversion from paninian karakas to universal dependencies for hindi dependency treebank universal dependencies ( ud ) are gaining much attention of late for systematic evaluation of cross - lingual techniques for crosslingual dependency parsing . in this paper we present our work in line with ud . our contribution to this is manifold . we extend ud to indian languages through conversion of pānịnian dependencies to ud for the hindi dependency treebank ( hdtb ) . we discuss the differences in annotation in both the schemes , present parsing experiments for both the formalisms and empirically evaluate their weaknesses and strengths for hindi . we produce an automatically converted hindi treebank conforming to the international standard ud scheme , making it useful as a resource for multilingual language technology .
RANK = 9; score = 0.31702018395859166; correct = False; id = aa532a1602d34e5b148af72d0f3f2cd8b944c2f5
converting syntagrus dependency treebank into penn treebank style this paper presents the conversion of syntagrus dependency structures into penn treebank style phrase structures , whose resulting data will be used to train a statistical constituency parser for russian and create a large - scale constituency - parsed corpus . the implemented conversion includes various innovative features in order to create phrase structure trees that are closest to penn treebank style while optimally preserving information of the original dependency structure annotations . we believe the newly converted phrase structure treebank will be not only an adequate training dataset for our ongoing project but also a valuable resource for traditional and computational linguistic research .
RANK = 10; score = 0.3134395363349322; correct = False; id = da068a697b02135871b570c8c8c22b45cb2cd3c6
incorporating statistical information of lexical dependency into a rule - based parser this paper presents a method to incorporate statistical information into a rulebased parser to resolve syntactic ambiguities . we extract the statistical information from the penn treebank , and apply the information to the rule - based parser . for the extraction of the statistical information the tag conversion is needed because of the disagreement of the tags and the bracketing style . we will show the effect of the tag conversion with experiments . the final result shows about 7 % error rate reduction in the dependency evaluation . we will also show how much each type of statistical information affects the parsing performance .
RANK = 11; score = 0.3129260943355083; correct = False; id = fa72a6ad519f5064cec40fdb17e22e354b7967cd
down - stream effects of tree - to - dependency conversions dependency analysis relies on morphosyntactic evidence , as well as semantic evidence . in some cases , however , morphosyntactic evidence seems to be in conflict with semantic evidence . for this reason dependency grammar theories , annotation guidelines and tree - to - dependency conversion schemes often differ in how they analyze various syntactic constructions . most experiments for which constituent - based treebanks such as the penn treebank are converted into dependency treebanks rely blindly on one of four - five widely used tree - to - dependency conversion schemes . this paper evaluates the down - stream effect of choice of conversion scheme , showing that it has dramatic impact on end results .
RANK = 12; score = 0.30839353615247805; correct = False; id = d363441769bf88cd03781c57ef2ed7d6197dcc0f
semeval-2007 task 18 : arabic semantic labeling in this paper , we present the details of the arabic semantic labeling task . we describe some of the features of arabic that are relevant for the task . the task comprises two subtasks : arabic word sense disambiguation and arabic semantic role labeling . the task focuses on modern standard arabic .
RANK = 13; score = 0.3021644117732843; correct = False; id = cc91b4439d1e15fcd10bb0a2f04c6bb8f4909915
a corpus for modeling morpho - syntactic agreement in arabic : gender , number and rationality we present an enriched version of the penn arabic treebank ( maamouri et al . , 2004 ) , where latent features necessary for modeling morpho - syntactic agreement in arabic are manually annotated . we describe our process for efficient annotation , and present the first quantitative analysis of arabic morphosyntactic phenomena .
RANK = 14; score = 0.283686830392197; correct = False; id = 4b0f1f9c2f0839082ebdef9a9dbee67ae66801bc
on the unification of syntactic annotations under the stanford dependency scheme : a case study on bioinfer and genia several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction . the recently introduced stanford dependency scheme has been suggested to be a suitable unifying syntax formalism . in this paper , we present a step towards such unification by creating a conversion from the link grammar to the stanford scheme . further , we create a version of the bioinfer corpus with syntactic annotation in this scheme . we present an application - oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of bioinfer and the genia treebank . we find that a highly reliable conversion is both feasible to create and practical , increasing the applicability of both the parser and the corpus to information extraction .
RANK = 15; score = 0.280604918490319; correct = False; id = 445ddeb2d786ab4f9a4c6a14a1f14ebd06db481d
arabizi detection and conversion to arabic arabizi is arabic text that is written using latin characters . arabizi is used to present both modern standard arabic ( msa ) or arabic dialects . it is commonly used in informal settings such as social networking sites and is often with mixed with english . in this paper we address the problems of : identifying arabizi in text and converting it to arabic characters . we used word and sequence - level features to identify arabizi that is mixed with english . we achieved an identification accuracy of 98.5 % . as for conversion , we used transliteration mining with language modeling to generate equivalent arabic text . we achieved 88.7 % conversion accuracy , with roughly a third of errors being spelling and morphological variants of the forms in ground truth .
RANK = 16; score = 0.27350111509738906; correct = False; id = f6df98852ded88a2188e1676ad8b3fc8af9e5f8d
integrating morpho - syntactic features in english - arabic statistical machine translation this paper presents a hybrid approach to the enhancement of english to arabic statistical machine translation quality . machine translation has been defined as the process that utilizes computer software to translate text from one natural language to another . arabic , as a morphologically rich language , is a highly flexional language , in that the same root can lead to various forms according to its context . statistical machine translation ( smt ) engines often show poor syntax processing especially when the language used is morphologically rich such as arabic . in this paper , to overcome these shortcomings , we describe our hybrid approach which integrates knowledge of the arabic language into statistical machine translation . in this framework , we propose the use of a featured language model sflm ( smaïli et al . , 2004 ) to be able to integrate syntactic and grammatical knowledge about each word . in this paper , we first discuss some challenges in translating from english to arabic and we explore various techniques to improve performance on this task . we apply a morphological segmentation step for arabic words and we present our hybrid approach by identifying morpho - syntactic class of each segmented word to build up our statistical feature language model . we propose the scheme for recombining the segmented arabic word , and describe their effect on translation .
RANK = 17; score = 0.27199048767055106; correct = False; id = 39d6db85b34319e9327ef69b0ec1fcdf95d93b32
indonesian dependency treebank : annotation and parsing we introduce and describe ongoing work in our indonesian dependency treebank . we described characteristics of the source data as well as describe our annotation guidelines for creating the dependency structures . reported within are the results from the start of the indonesian dependency treebank . we also show ensemble dependency parsing and self training approaches applicable to under - resourced languages using our manually annotated dependency structures . we show that for an under - resourced language , the use of tuning data for a meta classifier is more effective than using it as additional training data for individual parsers . this meta - classifier creates an ensemble dependency parser and increases the dependency accuracy by 4.92 % on average and 1.99 % over the best individual models on average . as the data sizes grow for the the under - resourced language a meta classifier can easily adapt . to the best of our knowledge this is the first full implementation of a dependency parser for indonesian . using self - training in combination with our ensemble svm parser we show aditional improvement . using this parsing model we plan on expanding the size of the corpus by using a semi - supervised approach by applying the parser and correcting the errors , reducing the amount of annotation time needed .
RANK = 18; score = 0.2714716446302354; correct = False; id = 282a147a5197d33afac20f26ea9a5903b5066a0f
multilingual dependency parsing using bayes point machines we develop dependency parsers for arabic , english , chinese , and czech using bayes point machines , a training algorithm which is as easy to implement as the perceptron yet competitive with large margin methods . we achieve results comparable to state - of - the - art in english and czech , and report the first directed dependency parsing accuracies for arabic and chinese . given the multilingual nature of our experiments , we discuss some issues regarding the comparison of dependency parsers for different languages .
RANK = 19; score = 0.2706065828542535; correct = False; id = b969803114b8406f253c6ec93d636ba35280fc8f
a fast , accurate , non - projective , semantically - enriched parser dependency parsers are critical components within many nlp systems . however , currently available dependency parsers each exhibit at least one of several weaknesses , including high running time , limited accuracy , vague dependency labels , and lack of nonprojectivity support . furthermore , no commonly used parser provides additional shallow semantic interpretation , such as preposition sense disambiguation and noun compound interpretation . in this paper , we present a new dependency - tree conversion of the penn treebank along with its associated fine - grain dependency labels and a fast , accurate parser trained on it . we explain how a non - projective extension to shift - reduce parsing can be incorporated into non - directional easy - first parsing . the parser performs well when evaluated on the standard test section of the penn treebank , outperforming several popular open source dependency parsers ; it is , to the best of our knowledge , the first dependency parser capable of parsing more than 75 sentences per second at over 93 % accuracy .
RANK = 20; score = 0.26545054758447734; correct = False; id = f89724bb41f971950cc6fc84861ffc9272821aaa
on wordnet semantic classes and dependency parsing this paper presents experiments with wordnet semantic classes to improve dependency parsing . we study the effect of semantic classes in three dependency parsers , using two types of constituencyto - dependency conversions of the english penn treebank . overall , we can say that the improvements are small and not significant using automatic pos tags , contrary to previously published results using gold pos tags ( agirre et al . , 2011 ) . in addition , we explore parser combinations , showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented lth treebank conversion .

RANKING 5
QUERY
phrase - level combination of smt and tm using constrained word lattice constrained translation has improved statistical machine translation ( smt ) by combining it with translation memory ( tm ) at sentence - level . in this paper , we propose using a constrained word lattice , which encodes input phrases and tm constraints together , to combine smt and tm at phrase - level . experiments on english– chinese and english – french show that our approach is significantly better than previous combination methods , including sentence - level constrained translation and a recent phrase - level combination .
First cited at 1
TOP CITED PAPERS
RANK 1
integrating translation memory into phrase - based machine translation during decoding since statistical machine translation ( smt ) and translation memory ( tm ) complement each other in matched and unmatched regions , integrated models are proposed in this paper to incorporate tm information into phrase - based smt . unlike previous multi - stage pipeline approaches , which directly merge tm result into the final output , the proposed models refer to the corresponding tm information associated with each phrase at smt decoding . on a chinese – english tm database , our experiments show that the proposed integrated model - iii is significantly better than either the smt or the tm systems when the fuzzy match score is above 0.4 . furthermore , integrated model - iii achieves overall 3.48 bleu points improvement and 2.62 ter points reduction in comparison with the pure smt system . besides , the proposed models also outperform previous approaches significantly .
RANK 2
bridging smt and tm with translation recommendation we propose a translation recommendation framework to integrate statistical machine translation ( smt ) output with translation memory ( tm ) systems . the framework recommends smt outputs to a tm user when it predicts that smt outputs are more suitable for post - editing than the hits provided by the tm . we describe an implementation of this framework using an svm binary classifier . we exploit methods to fine - tune the classifier and investigate a variety of features of different types . we rely on automatic mt evaluation metrics to approximate human judgements in our experiments . experimental results show that our system can achieve 0.85 precision at 0.89 recall , excluding exact matches . furthermore , it is possible for the end - user to achieve a desired balance between precision and recall by adjusting confidence levels .
RANK 3
consistent translation using discriminative learning - a translation memory - inspired approach we present a discriminative learning method to improve the consistency of translations in phrase - based statistical machine translation ( smt ) systems . our method is inspired by translation memory ( tm ) systems which are widely used by human translators in industrial settings . we constrain the translation of an input sentence using the most similar ‘ translation example’ retrieved from the tm . differently from previous research which used simple fuzzy match thresholds , these constraints are imposed using discriminative learning to optimise the translation performance . we observe that using this method can benefit the smt system by not only producing consistent translations , but also improved translation outputs . we report a 0.9 point improvement in terms of bleu score on english – chinese technical documents .
TOP UNCITED PAPERS
RANK 4
phrase training based adaptation for statistical machine translation we present a novel approach for translation model ( tm ) adaptation using phrase training . the proposed adaptation procedure is initialized with a standard general - domain tm , which is then used to perform phrase training on a smaller in - domain set . this way , we bias the probabilities of the general tm towards the in - domain distribution . experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones . additionally , we compare our results to mixture modeling , where we report gains when using the suggested phrase training adaptation method .
RANK 5
translation model based cross - lingual language model adaptation : from word models to phrase models in this paper , we propose a novel translation model ( tm ) based cross - lingual data selection model for language model ( lm ) adaptation in statistical machine translation ( smt ) , from word models to phrase models . given a source sentence in the translation task , this model directly estimates the probability that a sentence in the target lm training corpus is similar . compared with the traditional approaches which utilize the first pass translation hypotheses , cross - lingual data selection model avoids the problem of noisy proliferation . furthermore , phrase tm based cross - lingual data selection model is more effective than the traditional approaches based on bag - ofwords models and word - based tm , because it captures contextual information in modeling the selection of phrase as a whole . experiments conducted on large - scale data sets demonstrate that our approach significantly outperforms the state - of - the - art approaches on both lm perplexity and smt performance .
RANK 6
tmtprime : a recommender system for mt and tm integration tmtprime is a recommender system that facilitates the effective use of both translation memory ( tm ) and machine translation ( mt ) technology within industrial language service providers ( lsps ) localization workflows . lsps have long used translation memory ( tm ) technology to assist the translation process . recent research shows how mt systems can be combined with tms in computer aided translation ( cat ) systems , selecting either tm or mt output based on sophisticated translation quality estimation without access to a reference . however , to date there are no commercially available frameworks for this . tmtprime takes confidence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of mt with legacy tm systems to provide the most effective ( least effort / cost ) translation options to human translators , based on the tmtprime confidence score .
TOP 20
RANK = 1; score = 0.5402487505520888; correct = True; id = 326344e68b2cffd45705548c1d2085093507f27d
integrating translation memory into phrase - based machine translation during decoding since statistical machine translation ( smt ) and translation memory ( tm ) complement each other in matched and unmatched regions , integrated models are proposed in this paper to incorporate tm information into phrase - based smt . unlike previous multi - stage pipeline approaches , which directly merge tm result into the final output , the proposed models refer to the corresponding tm information associated with each phrase at smt decoding . on a chinese – english tm database , our experiments show that the proposed integrated model - iii is significantly better than either the smt or the tm systems when the fuzzy match score is above 0.4 . furthermore , integrated model - iii achieves overall 3.48 bleu points improvement and 2.62 ter points reduction in comparison with the pure smt system . besides , the proposed models also outperform previous approaches significantly .
RANK = 2; score = 0.4382588699345298; correct = True; id = f041d13447a04ebe3f9d71502915f548e7d2613b
bridging smt and tm with translation recommendation we propose a translation recommendation framework to integrate statistical machine translation ( smt ) output with translation memory ( tm ) systems . the framework recommends smt outputs to a tm user when it predicts that smt outputs are more suitable for post - editing than the hits provided by the tm . we describe an implementation of this framework using an svm binary classifier . we exploit methods to fine - tune the classifier and investigate a variety of features of different types . we rely on automatic mt evaluation metrics to approximate human judgements in our experiments . experimental results show that our system can achieve 0.85 precision at 0.89 recall , excluding exact matches . furthermore , it is possible for the end - user to achieve a desired balance between precision and recall by adjusting confidence levels .
RANK = 3; score = 0.38539341873635635; correct = True; id = 543672155570ecbdf9d0716d4b68df6544f25472
consistent translation using discriminative learning - a translation memory - inspired approach we present a discriminative learning method to improve the consistency of translations in phrase - based statistical machine translation ( smt ) systems . our method is inspired by translation memory ( tm ) systems which are widely used by human translators in industrial settings . we constrain the translation of an input sentence using the most similar ‘ translation example’ retrieved from the tm . differently from previous research which used simple fuzzy match thresholds , these constraints are imposed using discriminative learning to optimise the translation performance . we observe that using this method can benefit the smt system by not only producing consistent translations , but also improved translation outputs . we report a 0.9 point improvement in terms of bleu score on english – chinese technical documents .
RANK = 4; score = 0.3659400319184306; correct = False; id = aa5297783d70669b28b78805b4405b876757e110
phrase training based adaptation for statistical machine translation we present a novel approach for translation model ( tm ) adaptation using phrase training . the proposed adaptation procedure is initialized with a standard general - domain tm , which is then used to perform phrase training on a smaller in - domain set . this way , we bias the probabilities of the general tm towards the in - domain distribution . experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones . additionally , we compare our results to mixture modeling , where we report gains when using the suggested phrase training adaptation method .
RANK = 5; score = 0.3570059506074118; correct = False; id = 151ff93e5d189bd810165dd90b1cbcb6194e8594
translation model based cross - lingual language model adaptation : from word models to phrase models in this paper , we propose a novel translation model ( tm ) based cross - lingual data selection model for language model ( lm ) adaptation in statistical machine translation ( smt ) , from word models to phrase models . given a source sentence in the translation task , this model directly estimates the probability that a sentence in the target lm training corpus is similar . compared with the traditional approaches which utilize the first pass translation hypotheses , cross - lingual data selection model avoids the problem of noisy proliferation . furthermore , phrase tm based cross - lingual data selection model is more effective than the traditional approaches based on bag - ofwords models and word - based tm , because it captures contextual information in modeling the selection of phrase as a whole . experiments conducted on large - scale data sets demonstrate that our approach significantly outperforms the state - of - the - art approaches on both lm perplexity and smt performance .
RANK = 6; score = 0.31122537501790715; correct = False; id = b829763f1ca5ea2899e9029120e7e3a0ec457306
tmtprime : a recommender system for mt and tm integration tmtprime is a recommender system that facilitates the effective use of both translation memory ( tm ) and machine translation ( mt ) technology within industrial language service providers ( lsps ) localization workflows . lsps have long used translation memory ( tm ) technology to assist the translation process . recent research shows how mt systems can be combined with tms in computer aided translation ( cat ) systems , selecting either tm or mt output based on sophisticated translation quality estimation without access to a reference . however , to date there are no commercially available frameworks for this . tmtprime takes confidence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of mt with legacy tm systems to provide the most effective ( least effort / cost ) translation options to human translators , based on the tmtprime confidence score .
RANK = 7; score = 0.29466450094322266; correct = False; id = fe620306c0278e8c9ccd9519377ce26a70400aaf
a comparison of pivot methods for phrase - based statistical machine translation we compare two pivot strategies for phrase - based statistical machine translation ( smt ) , namelyphrase translation and sentence translation . the phrase translation strategy means that we directly construct a phrase translation table ( phrase - table ) of the source and target language pair from two phrase - tables ; one constructed from the source language and english and one constructed from english and the target language . we then use that phrase - table in a phrase - based smt system . the sentence translation strategy means that we first translate a source language sentence into n english sentences and then translate these n sentences into target language sentences separately . then , we select the highest scoring sentence from these target sentences . we conducted controlled experiments using the europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained smt systems . the phrase translation strategy significantly outperformed the sentence translation strategy . its relative performance was 0.92 to 0.97 compared to directly trained smt systems .
RANK = 8; score = 0.2862710431931371; correct = False; id = ce7ebbbed98e905f3a86111d1d4cc4f196ddcd1e
syntax - aware phrase - based statistical machine translation : system description we present a variant of phrase - based smt that uses source - side parsing and a constituent reordering model based on word alignments in the word - aligned training corpus to predict hierarchical block - wise reordering of the input . multiple possible translation orders are represented compactly in a source order lattice . this source order lattice is then annotated with phrase - level translations to form a lattice of tokens in the target language . various feature functions are combined in a log - linear fashion to evaluate paths through that lattice .
RANK = 9; score = 0.28622495423679994; correct = False; id = 043473b4e39a395093363ac1ef88cd80e858d79d
semantics , discourse and statistical machine translation in the past decade , statistical machine translation ( smt ) has been advanced from word - based smt to phraseand syntax - based smt . although this advancement produces significant improvements in bleu scores , crucial meaning errors and lack of cross - sentence connections at discourse level still hurt the quality of smt - generated translations . more recently , we have witnessed two active movements in smt research : one towards combining semantics and smt in attempt to generate not only grammatical but also meaningpreserved translations , and the other towards exploring discourse knowledge for document - level machine translation in order to capture intersentence dependencies . the emergence of semantic smt are due to the combination of two factors : the necessity of semantic modeling in smt and the renewed interest of designing models tailored to relevant nlp / smt applications in the semantics community . the former is represented by recent numerous studies on exploring word sense disambiguation , semantic role labeling , bilingual semantic representations as well as semantic evaluation for smt . the latter is reflected in conll shared tasks , semeval and seneval exercises in recent years . the need of capturing cross - sentence dependencies for document - level smt triggers the resurgent interest of modeling translation from the perspective of discourse . discourse phenomena , such as coherent relations , discourse topics , lexical cohesion that are beyond the scope of conventional sentence - level n - grams , have been recently considered and explored in the context of smt . this tutorial aims at providing a timely and combined introduction of such recent work along these two trends as discourse is inherently connected with semantics . the tutorial has three parts . the first part critically reviews the phraseand syntax - based smt . the second part is devoted to the lines of research oriented to semantic smt , including a brief introduction of semantics , lexical and shallow semantics tailored to smt , semantic representations in smt , semantically motivated evaluation as well as advanced topics on deep semantic learning for smt . the third part is dedicated to recent work on smt with discourse , including a brief review on discourse studies from linguistics and computational viewpoints , discourse research from monolingual to multilingual , discourse - based smt and a few advanced topics . the tutorial is targeted for researchers in the smt , semantics and discourse communities . in particular , the expected audience comes from two groups : 1 ) researchers and students in the smt community who want to design cutting - edge models and algorithms for semantic smt with various semantic knowledge and representations , and who would like to advance smt from sentence - bysentence translation to document - level translation with discourse information ; 2 ) researchers and students from the semantics and discourse community who are interested in developing models and methods and adapting them to smt .
RANK = 10; score = 0.28141536016517976; correct = False; id = 61d9dadc3911d7fc159cb70069cfe37d51783838
a topic similarity model for hierarchical phrase - based translation previous work using topic model for statistical machine translation ( smt ) explore topic information at the word level . however , smt has been advanced from word - based paradigm to phrase / rule - based paradigm . we therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase - based translation . we associate each synchronous rule with a topic distribution , and select desirable rules according to the similarity of their topic distributions with given documents . we show that our model significantly improves the translation performance over the baseline on nist chinese - to - english translation experiments . our model also achieves a better performance and a faster speed than previous approaches that work at the word level .
RANK = 11; score = 0.28007470122345957; correct = False; id = 07d459e9d3aefdc729a344e0746cb31e8465f399
the universität karlsruhe translation system for the eacl - wmt 2009 in this paper we describe the statistical machine translation system of the universität karlsruhe developed for the translation task of the fourth workshop on statistical machine translation . the state - ofthe - art phrase - based smt system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modifications . we participate in the constrained condition of german - english and english - german as well as in the constrained condition of french - english and english - french .
RANK = 12; score = 0.2779489280169517; correct = False; id = 45a7ee6d3b138827c155c902eea73b173b163756
combining word - level and character - level models for machine translation between closely - related languages we propose several techniques for improving statistical machine translation between closely - related languages with scarce resources . we use character - level translation trained on n - gram - character - aligned bitexts and tuned using word - level bleu , which we further augment with character - based transliteration at the word level and combine with a word - level translation model . the evaluation on macedonian - bulgarian movie subtitles shows an improvement of 2.84 bleu points over a phrase - based word - level baseline .
RANK = 13; score = 0.2777684277412664; correct = False; id = 2bfd42435b576a552ffebe597406ff0760203cc5
an unsupervised method for automatic translation memory cleaning we address the problem of automatically cleaning a large - scale translation memory ( tm ) in a fully unsupervised fashion , i.e. without human - labelled data . we approach the task by : i ) designing a set of features that capture the similarity between two text segments in different languages , ii ) use them to induce reliable training labels for a subset of the translation units ( tus ) contained in the tm , and iii ) use the automatically labelled data to train an ensemble of binary classifiers . we apply our method to clean a test set composed of 1,000 tus randomly extracted from the english - italian version of mymemory , the world ’s largest public tm . our results show competitive performance not only against a strong baseline that exploits machine translation , but also against a state - of - the - art method that relies on human - labelled data .
RANK = 14; score = 0.2776718503887401; correct = False; id = 476445fa5bbf71f1b501c7522591dedc313a94e4
adaptation of reordering models for statistical machine translation previous research on domain adaptation ( da ) for statistical machine translation ( smt ) has mainly focused on the translation model ( tm ) and the language model ( lm ) . to the best of our knowledge , there is no previous work on reordering model ( rm ) adaptation for phrasebased smt . in this paper , we demonstrate that mixture model adaptation of a lexicalized rm can significantly improve smt performance , even when the system already contains a domain - adapted tm and lm . we find that , surprisingly , different training corpora can vary widely in their reordering characteristics for particular phrase pairs . furthermore , particular training corpora may be highly suitable for training the tm or the lm , but unsuitable for training the rm , or vice versa , so mixture weights for these models should be estimated separately . an additional contribution of the paper is to propose two improvements to mixture model adaptation : smoothing the in - domain sample , and weighting instances by document frequency . applied to mixture rms in our experiments , these techniques ( especially smoothing ) yield significant performance improvements .
RANK = 15; score = 0.2736160383615489; correct = False; id = 8ce9385101267a8efd39a181c58be55c4c43fa95
handling phrase reorderings for machine translation we propose a distance phrase reordering model ( dpr ) for statistical machine translation ( smt ) , where the aim is to capture phrase reorderings using a structure learning framework . on both the reordering classification and a chinese - to - english translation task , we show improved performance over a baseline smt system .
RANK = 16; score = 0.27327506140528945; correct = False; id = 8da2a86b830c1a732ed323001244ad47b003f391
a markov model of machine translation using non - parametric bayesian inference most modern machine translation systems use phrase pairs as translation units , allowing for accurate modelling of phraseinternal translation and reordering . however phrase - based approaches are much less able to model sentence level effects between different phrase - pairs . we propose a new model to address this imbalance , based on a word - based markov model of translation which generates target translations left - to - right . our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical pitman - yor process prior to provide dynamic adaptive smoothing . this mechanism implicitly supports not only traditional phrase pairs , but also gapping phrases which are non - consecutive in the source . our experiments on chinese to english and arabic to english translation show consistent improvements over competitive baselines , of up to + 3.4 bleu .
RANK = 17; score = 0.26841359670919446; correct = False; id = fbbd30000228279868c431a37e9d988e21792bee
on statistical machine translation and translation theory the translation process in statistical machine translation ( smt ) is shaped by technical constraints and engineering considerations . smt explicitly models translation as search for a target - language equivalent of the input text . this perspective on translation had wide currency in mid-20th century translation studies , but has since been superseded by approaches arguing for a more complex relation between source and target text . in this paper , we show how traditional assumptions of translational equivalence are embodied in smt through the concepts of word alignment and domain and discuss some limitations arising from the word - level / corpus - level dichotomy inherent in these concepts .
RANK = 18; score = 0.267298692300254; correct = False; id = 08ea34bd8ba05ae576ed1dfd564e60ea3a3d4f78
improving statistical machine translation with monolingual collocation this paper proposes to use monolingual collocations to improve statistical machine translation ( smt ) . we make use of the collocation probabilities , which are estimated from monolingual corpora , in two aspects , namely improving word alignment for various kinds of smt systems and improving phrase table for phrase - based smt . the experimental results show that our method improves the performance of both word alignment and translation quality significantly . as compared to baseline systems , we achieve absolute improvements of 2.40 bleu score on a phrase - based smt system and 1.76 bleu score on a parsing - based smt system .
RANK = 19; score = 0.2644425752395875; correct = False; id = b87036a4178e58cf327b901c330bec9a35d784fa
a joint rule selection model for hierarchical phrase - based translation lattice decoding in statistical machine translation ( smt ) is useful in speech translation and in the translation of german because it can handle input ambiguities such as speech recognition ambiguities and german word segmentation ambiguities . we show that lattice decoding is also useful for handling input variations . given an input sentence , we build a lattice which represents paraphrases of the input sentence . we call this a paraphrase lattice . then , we give the paraphrase lattice as an input to the lattice decoder . the decoder selects the best path for decoding . using these paraphrase lattices as inputs , we obtained significant gains in bleu scores for iwslt and europarl datasets .
RANK = 20; score = 0.26400696138888474; correct = True; id = 8e8f1e4a284ab36272fac7894beb9932fe200208
generalizing word lattice translation word lattice decoding has proven useful in spoken language translation ; we argue that it provides a compelling model for translation of text genres , as well . we extend lattice decoding to hierarchical phrase - based models , providing a unified treatment with phrase - based decoding by treating lattices as a case of weighted finite - state automata . in the process , we resolve a significant complication that lattice representations introduce in reordering models . our experiments evaluating the approach demonstrate substantial gains for chinese - english and arabic - english translation .

RANKING 406
QUERY
extension breakdown : security analysis of browsers extension resources control policies all major web browsers support browser extensions to add new features and extend their functionalities . nevertheless , browser extensions have been the target of several attacks due to their tight relation with the browser environment . as a consequence , extensions have been abused in the past for malicious tasks such as private information gathering , browsing history retrieval , or passwords theft — leading to a number of severe targeted attacks . even though no protection techniques existed in the past to secure extensions , all browsers now implement defensive countermeasures that , in theory , protect extensions and their resources from third party access . in this paper , we present two attacks that bypass these control techniques in every major browser family , enabling enumeration attacks against the list of installed extensions . in particular , we present a timing side - channel attack against the access control settings and an attack that takes advantage of poor programming practice , affecting a large number of safari extensions . due to the harmful nature of our findings , we also discuss possible countermeasures against our own attacks and reported our findings and countermeasures to the different actors involved . we believe that our study can help secure current implementations and help developers to avoid similar attacks in the future .
First cited at 1
TOP CITED PAPERS
RANK 1
verified security for browser extensions popup blocking , form filling , and many other features of modern web browsers were first introduced as third - party extensions . new extensions continue to enrich browsers in unanticipated ways . however , powerful extensions require capabilities , such as cross - domain network access and local storage , which , if used improperly , pose a security risk . several browsers try to limit extension capabilities , but an empirical survey we conducted shows that many extensions are over - privileged under existing mechanisms . this paper presents \ibex , a new framework for authoring , analyzing , verifying , and deploying secure browser extensions . our approach is based on using type - safe , high - level languages to program extensions against an api providing access to a variety of browser features . we propose using data log to specify fine - grained access control and dataflow policies to limit the ways in which an extension can use this api , thus restricting its privilege over security - sensitive web content and browser resources . we formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance . additionally , we provide visualization tools to assist with policy analysis , and compilers to translate extension source code to either . net byte code or javascript , facilitating cross - browser deployment of extensions . we evaluate our work by implementing and verifying~\numext extensions with a diverse set of features and security policies . we deploy our extensions in internet explorer , chrome , fire fox , and a new experimental html5 platform called c3 . in so doing , we demonstrate the versatility and effectiveness of our approach .
RANK 2
an evaluation of the google chrome extension security architecture vulnerabilities in browser extensions put users at risk by providing a way for website and network attackers to gain access to users’ private data and credentials . extensions can also introduce vulnerabilities into the websites that they modify . in 2009 , google chrome introduced a new extension platform with several features intended to prevent and mitigate extension vulnerabilities : strong isolation between websites and extensions , privilege separation within an extension , and an extension permission system . we performed a security review of 100 chrome extensions and found 70 vulnerabilities across 40 extensions . given these vulnerabilities , we evaluate how well each of the security mechanisms defends against extension vulnerabilities . we find that the mechanisms mostly succeed at preventing direct web attacks on extensions , but new security mechanisms are needed to protect users from network attacks on extensions , website metadata attacks on extensions , and vulnerabilities that extensions add to websites . we propose and evaluate additional defenses , and we conclude that banning http scripts and inline scripts would prevent 47 of the 50 most severe vulnerabilities with only modest impact on developers .
RANK 5
hulk : eliciting malicious behavior in browser extensions we present hulk , a dynamic analysis system that detects malicious behavior in browser extensions by monitoring their execution and corresponding network activity . hulk elicits malicious behavior in extensions in two ways . first , hulk leverages honeypages , which are dynamic pages that adapt to an extension ’s expectations in web page structure and content . second , hulk employs a fuzzer to drive the numerous event handlers that modern extensions heavily rely upon . we analyzed 48 k extensions from the chrome web store , driving each with over 1 m urls . we identify a number of malicious extensions , including one with 5.5 million affected users , stressing the risks that extensions pose for today ’s web security ecosystem , and the need to further strengthen browser security to protect user data and privacy .
TOP UNCITED PAPERS
RANK 3
vex : vetting browser extensions for security vulnerabilities the browser has become the de facto platform for everyday computation . among the many potential attacks that target or exploit browsers , vulnerabilities in browser extensions have received relatively little attention . currently , extensions are vetted by manual inspection , which does not scale well and is subject to human error . in this paper , we present vex , a framework for highlighting potential security vulnerabilities in browser extensions by applying static information - flow analysis to the javascript code used to implement extensions . we describe several patterns of flows as well as unsafe programming practices that may lead to privilege escalations in firefox extensions . vex analyzes firefox extensions for such flow patterns using high - precision , context - sensitive , flow - sensitive static analysis . we analyze thousands of browser extensions , and vex finds six exploitable vulnerabilities , three of which were previously unknown . vex also finds hundreds of examples of bad programming practices that may lead to security vulnerabilities . we show that compared to current mozilla extension review tools , vex greatly reduces the human burden for manually vetting extensions when looking for key types of dangerous flows .
RANK 4
secure web browsing with the op web browser current web browsers are plagued with vulnerabilities , providing hackers with easy access to computer systems via browser - based attacks . browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed . to enable more secure web browsing , we design and implement a new browser , called the op web browser , that attempts to improve the state - of - the - art in browser security . our overall design approach is to combine operating system design principles with formal methods to design a more secure web browser by drawing on the expertise of both communities . our overall design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit . at the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features . to show the utility of our browser architecture , we design and implement three novel security features . first , we develop novel and flexible security policies that allows us to include plugins within our security framework . our policy removes the burden of security from plugin writers , and gives plugins the flexibility to use innovative network architectures to deliver content while still maintaining the confidentiality and integrity of our browser , even if attackers compromise the plugin . second , we use formal methods to prove that the address bar displayed within our browser user interface always shows the correct address for the current web page . third , we design and implement a browser - level information - flow tracking system to enable post - mortem analysis of browser - based attacks . if an attacker is able to compromise our browser , we highlight the subset of total activity that is causally related to the attack , thus allowing users and system administrators to determine easily which web site lead to the compromise and to assess the damage of a successful attack . to evaluate our design , we implemented op and tested both performance and filesystem impact . to test performance , we measure latency to verify op 's performance penalty from security features are be minimal from a users perspective . our experiments show that on average the speed of the op browser is comparable to firefox and the audit log occupies around 80 kb per page on average .
RANK 6
cookieless monster : exploring the ecosystem of web - based device fingerprinting the web has become an essential part of our society and is currently the main medium of information delivery . billions of users browse the web on a daily basis , and there are single websites that have reached over one billion user accounts . in this environment , the ability to track users and their online habits can be very lucrative for advertising companies , yet very intrusive for the privacy of users . in this paper , we examine how web - based device fingerprinting currently works on the internet . by analyzing the code of three popular browser - fingerprinting code providers , we reveal the techniques that allow websites to track users without the need of client - side identifiers . among these techniques , we show how current commercial fingerprinting approaches use questionable practices , such as the circumvention of http proxies to discover a user 's real ip address and the installation of intrusive browser plugins . at the same time , we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser - identifying techniques . with so many different vendors involved in browser development , we demonstrate how one can use diversions in the browsers ' implementation to distinguish successfully not only the browser - family , but also specific major and minor versions . browser extensions that help users spoof the user - agent of their browsers are also evaluated . we show that current commercial approaches can bypass the extensions , and , in addition , take advantage of their shortcomings by using them as additional fingerprinting features .
TOP 20
RANK = 1; score = 0.4977939440259837; correct = True; id = 19d74776ad8065eb8967347b9eab408a944798d6
verified security for browser extensions popup blocking , form filling , and many other features of modern web browsers were first introduced as third - party extensions . new extensions continue to enrich browsers in unanticipated ways . however , powerful extensions require capabilities , such as cross - domain network access and local storage , which , if used improperly , pose a security risk . several browsers try to limit extension capabilities , but an empirical survey we conducted shows that many extensions are over - privileged under existing mechanisms . this paper presents \ibex , a new framework for authoring , analyzing , verifying , and deploying secure browser extensions . our approach is based on using type - safe , high - level languages to program extensions against an api providing access to a variety of browser features . we propose using data log to specify fine - grained access control and dataflow policies to limit the ways in which an extension can use this api , thus restricting its privilege over security - sensitive web content and browser resources . we formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance . additionally , we provide visualization tools to assist with policy analysis , and compilers to translate extension source code to either . net byte code or javascript , facilitating cross - browser deployment of extensions . we evaluate our work by implementing and verifying~\numext extensions with a diverse set of features and security policies . we deploy our extensions in internet explorer , chrome , fire fox , and a new experimental html5 platform called c3 . in so doing , we demonstrate the versatility and effectiveness of our approach .
RANK = 2; score = 0.38253884229939566; correct = True; id = 470c5dfa56ef0e5623b5902f589c3a0c3f0eef5c
an evaluation of the google chrome extension security architecture vulnerabilities in browser extensions put users at risk by providing a way for website and network attackers to gain access to users’ private data and credentials . extensions can also introduce vulnerabilities into the websites that they modify . in 2009 , google chrome introduced a new extension platform with several features intended to prevent and mitigate extension vulnerabilities : strong isolation between websites and extensions , privilege separation within an extension , and an extension permission system . we performed a security review of 100 chrome extensions and found 70 vulnerabilities across 40 extensions . given these vulnerabilities , we evaluate how well each of the security mechanisms defends against extension vulnerabilities . we find that the mechanisms mostly succeed at preventing direct web attacks on extensions , but new security mechanisms are needed to protect users from network attacks on extensions , website metadata attacks on extensions , and vulnerabilities that extensions add to websites . we propose and evaluate additional defenses , and we conclude that banning http scripts and inline scripts would prevent 47 of the 50 most severe vulnerabilities with only modest impact on developers .
RANK = 3; score = 0.35458444940401185; correct = False; id = a4ebb757ea215851146b7a4f78b79278f6d42b33
vex : vetting browser extensions for security vulnerabilities the browser has become the de facto platform for everyday computation . among the many potential attacks that target or exploit browsers , vulnerabilities in browser extensions have received relatively little attention . currently , extensions are vetted by manual inspection , which does not scale well and is subject to human error . in this paper , we present vex , a framework for highlighting potential security vulnerabilities in browser extensions by applying static information - flow analysis to the javascript code used to implement extensions . we describe several patterns of flows as well as unsafe programming practices that may lead to privilege escalations in firefox extensions . vex analyzes firefox extensions for such flow patterns using high - precision , context - sensitive , flow - sensitive static analysis . we analyze thousands of browser extensions , and vex finds six exploitable vulnerabilities , three of which were previously unknown . vex also finds hundreds of examples of bad programming practices that may lead to security vulnerabilities . we show that compared to current mozilla extension review tools , vex greatly reduces the human burden for manually vetting extensions when looking for key types of dangerous flows .
RANK = 4; score = 0.32965259830762444; correct = False; id = 431c2bbfb35f29eec1eff79dda1d67049605cddb
secure web browsing with the op web browser current web browsers are plagued with vulnerabilities , providing hackers with easy access to computer systems via browser - based attacks . browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed . to enable more secure web browsing , we design and implement a new browser , called the op web browser , that attempts to improve the state - of - the - art in browser security . our overall design approach is to combine operating system design principles with formal methods to design a more secure web browser by drawing on the expertise of both communities . our overall design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit . at the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features . to show the utility of our browser architecture , we design and implement three novel security features . first , we develop novel and flexible security policies that allows us to include plugins within our security framework . our policy removes the burden of security from plugin writers , and gives plugins the flexibility to use innovative network architectures to deliver content while still maintaining the confidentiality and integrity of our browser , even if attackers compromise the plugin . second , we use formal methods to prove that the address bar displayed within our browser user interface always shows the correct address for the current web page . third , we design and implement a browser - level information - flow tracking system to enable post - mortem analysis of browser - based attacks . if an attacker is able to compromise our browser , we highlight the subset of total activity that is causally related to the attack , thus allowing users and system administrators to determine easily which web site lead to the compromise and to assess the damage of a successful attack . to evaluate our design , we implemented op and tested both performance and filesystem impact . to test performance , we measure latency to verify op 's performance penalty from security features are be minimal from a users perspective . our experiments show that on average the speed of the op browser is comparable to firefox and the audit log occupies around 80 kb per page on average .
RANK = 5; score = 0.32483092629366217; correct = True; id = 2494382813fa0b7aa405c4cc0f1ef5be90ca2d79
hulk : eliciting malicious behavior in browser extensions we present hulk , a dynamic analysis system that detects malicious behavior in browser extensions by monitoring their execution and corresponding network activity . hulk elicits malicious behavior in extensions in two ways . first , hulk leverages honeypages , which are dynamic pages that adapt to an extension ’s expectations in web page structure and content . second , hulk employs a fuzzer to drive the numerous event handlers that modern extensions heavily rely upon . we analyzed 48 k extensions from the chrome web store , driving each with over 1 m urls . we identify a number of malicious extensions , including one with 5.5 million affected users , stressing the risks that extensions pose for today ’s web security ecosystem , and the need to further strengthen browser security to protect user data and privacy .
RANK = 6; score = 0.31493475308867946; correct = False; id = 0d2f693901fba451ede4d388724b0e3f57029cd3
cookieless monster : exploring the ecosystem of web - based device fingerprinting the web has become an essential part of our society and is currently the main medium of information delivery . billions of users browse the web on a daily basis , and there are single websites that have reached over one billion user accounts . in this environment , the ability to track users and their online habits can be very lucrative for advertising companies , yet very intrusive for the privacy of users . in this paper , we examine how web - based device fingerprinting currently works on the internet . by analyzing the code of three popular browser - fingerprinting code providers , we reveal the techniques that allow websites to track users without the need of client - side identifiers . among these techniques , we show how current commercial fingerprinting approaches use questionable practices , such as the circumvention of http proxies to discover a user 's real ip address and the installation of intrusive browser plugins . at the same time , we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser - identifying techniques . with so many different vendors involved in browser development , we demonstrate how one can use diversions in the browsers ' implementation to distinguish successfully not only the browser - family , but also specific major and minor versions . browser extensions that help users spoof the user - agent of their browsers are also evaluated . we show that current commercial approaches can bypass the extensions , and , in addition , take advantage of their shortcomings by using them as additional fingerprinting features .
RANK = 7; score = 0.2963187293677609; correct = False; id = 321a935b029a82fdacbba34274c801574a84ac09
on the incoherencies in web browser access control policies web browsers ' access control policies have evolved piecemeal in an ad - hoc fashion with the introduction of new browser features . this has resulted in numerous incoherencies . in this paper , we analyze three major access control flaws in today 's browsers : ( 1 ) principal labeling is different for different resources , raising problems when resources interplay , ( 2 ) runtime changes to principal identities are handled inconsistently , and ( 3)browsers mismanage resources belonging to the user principal . we show that such mishandling of principals leads to many access control incoherencies , presenting hurdles for web developers to construct secure web applications . a unique contribution of this paper is to identify the compatibility cost of removing these unsafe browser features . to do this , we have built webanalyzer , a crawler - based framework for measuring real - world usage of browser features , and used it to study the top 100,000 popular web sites ranked by alexa . our methodology and results serve as a guideline for browser designers to balance security and backward compatibility .
RANK = 8; score = 0.29235797270700753; correct = True; id = 1a2630e41f1519b701c799a1907f441e85496c62
securing script - based extensibility in web browsers securing script - based extensibility in web browsers vladan djeric master of applied science graduate department of electrical and computer engineering university of toronto 2009 web browsers are increasingly designed to be extensible to keep up with the web ’s rapid pace of change . this extensibility is typically implemented using script - based extensions . script extensions have access to sensitive browser apis and content from untrusted web pages . unfortunately , this powerful combination creates the threat of privilege escalation attacks that grant web page scripts the full privileges of script extensions and control over the entire browser process . this thesis describes the pitfalls of script - based extensibility based on our study of the firefox web browser , and is the first to offer a classification of script - based privilege escalation vulnerabilities . we propose a taint - based system to track the spread of untrusted data in the browser and to detect the characteristic signatures of privilege escalation attacks . we show that this approach is effective by testing our system against exploits in the firefox bug database and finding that it detects the vast majority of attacks with no false alarms .
RANK = 9; score = 0.291859818524805; correct = False; id = 9f7d68de19e54dbc1d03bc56defea35154f7da07
performance signatures of mobile phone browsers several fingerprinting techniques for computer browsers have been proposed to make it possible to link together different browser sessions and possibly tie them to a user identity . as most of these techniques depend on static browser characteristics and user - installed plugins , the resulting fingerprints are not suitable for mobile browsers because of the similarity of browser characteristics on similar mobile device products in spite of the differences in software and hardware . moreover , mobile devices are shipped with pre - installed plugins that can not be modified , which limits browser uniqueness . therefore , we propose a dynamic mobile browser fingerprinting technique that records the browser 's behavior and execution characteristics by running background customized browser scripts . our dynamic technique is based on the use of javascript , html5 , flash , and other scripts that are used to generate performance signatures of mobile browsers to detect the browser used , the operating system version , and device type . our browser detection technique compares the active browser session signature with existing signatures through three detection methods : ( 1 ) euclidean distance , ( 2 ) cosine similarity , and ( 3 ) voting system . in this paper we compare the detection rates of these methods and their accuracy in determining the mobile browser in use .
RANK = 10; score = 0.2858590685832382; correct = False; id = 73e3dd2b8fba372316b780fdb81bcb7686c15bae
the spy in the sandbox : practical cache attacks in javascript and their implications we present a micro - architectural side - channel attack that runs entirely in the browser . in contrast to previous work in this genre , our attack does not require the attacker to install software on the victim 's machine ; to facilitate the attack , the victim needs only to browse to an untrusted webpage that contains attacker - controlled content . this makes our attack model highly scalable , and extremely relevant and practical to today 's web , as most desktop browsers currently used to access the internet are affected by such side channel threats . our attack , which is an extension to the last - level cache attacks of liu et al . , allows a remote adversary to recover information belonging to other processes , users , and even virtual machines running on the same physical host with the victim web browser . we describe the fundamentals behind our attack , and evaluate its performance characteristics . in addition , we show how it can be used to compromise user privacy in a common setting , letting an attacker spy after a victim that uses private browsing . defending against this side channel is possible , but the required countermeasures can exact an impractical cost on benign uses of the browser .
RANK = 11; score = 0.2829815372572371; correct = False; id = 03667897fd22a31eb96232e14b7e08e26a5b9ad7
an analysis of private browsing modes in modern browsers we study the security and privacy of private browsing modes recently added to all major browsers . we first propose a clean definition of the goals of private browsing and survey its implementation in different browsers . we conduct a measurement study to determine how often it is used and on what categories of sites . our results suggest that private browsing is used differently from how it is marketed . we then describe an automated technique for testing the security of private browsing modes and report on a few weaknesses found in the firefox browser . finally , we show that many popular browser extensions and plugins undermine the security of private browsing . we propose and experiment with a workable policy that lets users safely run extensions in private browsing mode .
RANK = 12; score = 0.28249817298768215; correct = False; id = 57f891b7213282bd58dc61230919fb531b0e4fde
non - control - data attacks are realistic threats most memory corruption attacks and internet worms follow a familiar pattern known as the control - data attack . hence , many defensive techniques are designed to protect program control flow integrity . although earlier work did suggest the existence of attacks that do not alter control flow , such attacks are generally believed to be rare against real - world software . the key contribution of this paper is to show that non - control - data attacks are realistic . we demonstrate that many real - world applications , including ftp , ssh , telnet , and http servers , are vulnerable to such attacks . in each case , the generated attack results in a security compromise equivalent to that due to the controldata attack exploiting the same security bug . non - control - data attacks corrupt a variety of application data including user identity data , configuration data , user input data , and decision - making data . the success of these attacks and the variety of applications and target data suggest that potential attack patterns are diverse . attackers are currently focused on control - data attacks , but it is clear that when control flow protection techniques shut them down , they have incentives to study and employ non - control - data attacks . this paper emphasizes the importance of future research efforts to address this realistic threat .
RANK = 13; score = 0.27996802634661844; correct = False; id = 17e8416d8a71275f05ace6c74d67dfe05db64efc
language - based defenses against untrusted browser origins we present new attacks and robust countermeasures for security - sensitive components , such as single sign - on apis and client - side cryptographic libraries , that need to be safely deployed on untrusted web pages . we show how failing to isolate such components leaves them vulnerable to attacks both from the hosting website and other components running on the same page . these attacks are not prevented by browser security mechanisms alone , because they are caused by code interacting within the same origin . to mitigate these attacks , we propose to combine fine - grained component isolation at the javascript level with cryptographic mechanisms . we present defensive javascript ( djs ) , a subset of the language that guarantees the behavior integrity of scripts even when loaded in a hostile environment . we give a sound type system , type inference tool , and build defensive libraries for cryptography and data encodings . we show the effectiveness of our solution by implementing several applications using defensive patterns that fix some of our original attacks . we present a model extraction tool to analyze the security properties of our applications using a cryptographic protocol verifier . 1 defensive web components web users increasingly store sensitive data on servers spread across the web . the main advantage of this dispersal is that users can access their data from browsers on multiple devices , and easily share this data with friends and colleagues . the main drawback is that concentrating sensitive data on servers makes them tempting targets for cyber - criminals , who use increasingly sophisticated browser - based attacks to steal user data . in response to these concerns , web applications now offer users more control over who gets access to their data , using authorization protocols such as oauth [ 23 ] and application - level cryptography . these security mechanisms are often implemented as javascript components that may be included by any website , where they mediate a three - party interaction between the host website , the user ( represented by her browser ) , and a server that holds the sensitive data on behalf of the user . website ( w )
RANK = 14; score = 0.2776791165325566; correct = False; id = 0fd652356d1ecbc457504a17f594187ccab2fe76
ucognito : private browsing without tears while private browsing is a standard feature , its implementation has been inconsistent among the major browsers . more seriously , it often fails to provide the adequate or even the intended privacy protection . for example , as shown in prior research , browser extensions and add - ons often undermine the goals of private browsing . in this paper , we first present our systematic study of private browsing . we developed a technical approach to identify browser traces left behind by a private browsing session , and showed that chrome and firefox do not correctly clear some of these traces . we analyzed the source code of these browsers and discovered that the current implementation approach is to decide the behaviors of a browser based on the current browsing mode ( i.e. , private or public ) ; but such decision points are scattered throughout the code base . this implementation approach is very problematic because developers are prone to make mistakes given the complexities of browser components ( including extensions and add - ons ) . based on this observation , we propose a new and general approach to implement private browsing . the main idea is to overlay the actual filesystem with a sandbox filesystem when the browser is in private browsing mode , so that no unintended leakage is allowed and no persistent modification is stored . this approach requires no change to browsers and the os kernel because the layered sandbox filesystem is implemented by interposing system calls . we have implemented a prototype system called ucognito on linux . our evaluations show that ucognito , when applied to chrome and firefox , stops all known privacy leaks identified by prior work and our current study . more importantly , ucognito incurs only negligible performance overhead : e.g. , 0%-2.5 % in benchmarks for standard javascript and webpage loading .
RANK = 15; score = 0.27672988796771963; correct = True; id = 70f340e80468832b7a293da8a4f1d08ed2786448
timing attacks on web privacy we describe a class of attacks that can compromise the privacy of users’ web - browsing histories . the attacks allow a malicious web site to determine whether or not the user has recently visited some other , unrelated web page . the malicious page can determine this information by measuring the time the user ’s browser requires to perform certain operations . since browsers perform various forms of caching , the time required for operations depends on the user ’s browsing history ; this paper shows that the resulting time variations convey enough information to compromise users’ privacy . this attack method also allows other types of information gathering by web sites , such as a more invasive form of web “ cookies ” . the attacks we describe can be carried out without the victim ’s knowledge , and most “ anonymous browsing ” tools fail to prevent them . other simple countermeasures also fail to prevent these attacks . we describe a way of reengineering browsers to prevent most of them .
RANK = 16; score = 0.27050050351079574; correct = False; id = bba5b4d1ec0e9f254d82b7a755596e825a975ca0
trends and lessons from three years fighting malicious extensions in this work we expose wide - spread efforts by criminals to abuse the chrome web store as a platform for distributing malicious extensions . a central component of our study is the design and implementation of webeval , the first system that broadly identifies malicious extensions with a concrete , measurable detection rate of 96.5 % . over the last three years we detected 9,523 malicious extensions : nearly 10 % of every extension submitted to the store . despite a short window of operation — we removed 50 % of malware within 25 minutes of creation— a handful of under 100 extensions escaped immediate detection and infected over 50 million chrome users . our results highlight that the extension abuse ecosystem is drastically different from malicious binaries : miscreants profit from web traffic and user tracking rather than email spam or banking theft .
RANK = 17; score = 0.25857803777235344; correct = True; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 18; score = 0.24749950874232912; correct = True; id = fe2f4faec5cf209ae7d8a73100db9cce46ce53d4
beauty and the beast : diverting modern web browsers to build unique browser fingerprints worldwide , the number of people and the time spent browsing the web keeps increasing . accordingly , the technologies to enrich the user experience are evolving at an amazing pace . many of these evolutions provide for a more interactive web ( e.g. , boom of javascript libraries , weekly innovations in html5 ) , a more available web ( e.g. , explosion of mobile devices ) , a more secure web ( e.g. , flash is disappearing , npapi plugins are being deprecated ) , and a more private web ( e.g. , increased legislation against cookies , huge success of extensions such as ghostery and adblock ) . nevertheless , modern browser technologies , which provide the beauty and power of the web , also provide a darker side , a rich ecosystem of exploitable data that can be used to build unique browser fingerprints . our work explores the validity of browser fingerprinting in today 's environment . over the past year , we have collected 118,934 fingerprints composed of 17 attributes gathered thanks to the most recent web technologies . we show that innovations in html5 provide access to highly discriminating attributes , notably with the use of the canvas api which relies on multiple layers of the user 's system . in addition , we show that browser fingerprinting is as effective on mobile devices as it is on desktops and laptops , albeit for radically different reasons due to their more constrained hardware and software environments . we also evaluate how browser fingerprinting could stop being a threat to user privacy if some technological evolutions continue ( e.g. , disappearance of plugins ) or are embraced by browser vendors ( e.g. , standard http headers ) .
RANK = 19; score = 0.24552015707165664; correct = False; id = f5df2326de40b38d74526b036137026c52dcd742
partitioning attacks : or how to rapidly clone some gsm cards in this paper , we introduce a new class of side – channel attacks called partitioning attacks . we have successfully launched a version of the attack on several implementations of comp128 , the popular gsm authentication algorithm that has been deployed by different service providers in several types of sim cards , to retrieve the 128 bit key using as few as 8 chosen plaintexts . we show how partitioning attacks can be used effectively to attack implementations that have been equipped with ad hoc and inadequate countermeasures against side – channel attacks . such ad hoc countermeasures are systemic in implementations of cryptographic algorithms , such as comp128 , which require the use of large tables since there has been a mistaken belief that sound countermeasures require more resources than are available . to address this problem , we describe a new resource – efficient countermeasure for protecting table lookups in cryptographic implementations and justify its correctness rigorously .
RANK = 20; score = 0.2441769435063148; correct = False; id = 4af9cd5f263e00aa0c743749869a81988221134a
virtual browser : a web - level sandbox to secure third - party javascript without sacrificing functionality third - party javascript offers much more diversity to web and its applications but also introduces new threats . those scripts can not be completely trusted and executed with the privileges given to host web sites . due to incomplete virtualization and lack of tracking all the data flows , all the existing works in this area can secure only a subset of third - party javascript . at the same time , because of the existence of not so well documented browser quirks , attacks may be encoded in non standard html / javascript so that they can bypass existing approaches as these approaches will parse third - party javascript twice , at both server and client side . in this paper , we propose virtual browser , a completely virtualized environment within existing browsers for executing untrusted third - party code . we secure complete javascript , including all the hard - to - secure functions of javascript programs , such as with and eval . since this approach parses scripts only once , there is no possibility of attacks being executed through browser quirks . we first completely isolate virtual browser from the native browser components and then introduce communication by adding data flows carefully examined for security .

