RANKING 102
QUERY
document - based recommender system for job postings using dense representations job boards and professional social networks heavily use recommender systems in order to better support users in exploring job advertisements . detecting the similarity between job advertisements is important for job recommendation systems as it allows , for example , the application of item - to - item based recommendations . in this work , we research the usage of dense vector representations to enhance a large - scale job recommendation system and to rank german job advertisements regarding their similarity . we follow a two - folded evaluation scheme : ( 1 ) we exploit historic user interactions to automatically create a dataset of similar jobs that enables an offline evaluation . ( 2 ) in addition , we conduct an online a / b test and evaluate the best performing method on our platform reaching more than 1 million users . we achieve the best results by combining job titles with full - text job descriptions . in particular , this method builds dense document representation using words of the titles to weigh the importance of words of the full - text description . in the online evaluation , this approach allows us to increase the click - through rate on job recommendations for active users by 8.0 % .
First cited at 3994
TOP CITED PAPERS
RANK 3994
uofl at semeval-2016 task 4 : multi domain word2vec for twitter sentiment classification in this paper , we present a transfer learning system for twitter sentiment classification and compare its performance using different feature sets that include different word representation vectors . we utilized data from a different source domain to increase the performance of our system in the target domain . our approach was based on training various word2vec models on data from the source and target domains combined , then using these models to calculate the average word vector of all the word vectors in a tweet observation , then input the average word vector as a feature to our classifiers for training . we further developed one doc2vec model that was trained on the positive , negative and neutral tweets in the target domain only . we then used these models in calculating the average word vector for every tweet in the training set as a preprocessing step . the final evaluation results show that our approach gave a prediction accuracy on the twitter2016 test dataset that outperformed two teams that were among the top 10 in terms of avgf1 scores .
RANK 12012
dependency - based word embeddings while continuous word embeddings are gaining popularity , current models are based solely on linear contexts . in this work , we generalize the skip - gram model with negative sampling introduced by mikolov et al . to include arbitrary contexts . in particular , we perform experiments with dependency - based contexts , and show that they produce markedly different embeddings . the dependencybased embeddings are less topical and exhibit more functional similarity than the original skip - gram embeddings .
TOP UNCITED PAPERS
RANK 1
classifying relations by ranking with convolutional neural networks relation classification is an important semantic processing task for which state - ofthe - art systems still rely on costly handcrafted features . in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking ( cr - cnn ) . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes . we perform experiments using the the semeval-2010 task 8 dataset , which is designed for the task of classifying the relationship between two nominals marked in a sentence . using crcnn , we outperform the state - of - the - art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features . additionally , our experimental results show that : ( 1 ) our approach is more effective than cnn followed by a softmax classifier ; ( 2 ) omitting the representation of the artificial class other improves both precision and recall ; and ( 3 ) using only word embeddings as input features is enough to achieve state - of - the - art results if we consider only the text between the two target nominals .
RANK 2
ualacant : using online machine translation for cross - lingual textual entailment this paper describes a new method for crosslingual textual entailment ( clte ) detection based on machine translation ( mt ) . we use sub - segment translations from different mt systems available online as a source of crosslingual knowledge . in this work we describe and evaluate different features derived from these sub - segment translations , which are used by a support vector machine classifier to detect cltes . we presented this system to the semeval 2012 task 8 obtaining an accuracy up to 59.8 % on the english – spanish test set , the second best performing approach in the contest .
RANK 3
lexpagerank : prestige in multi - document text summarization multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document . centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo - sentence . we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality ( prestige ) that we call lexpagerank . in this model , a sentence connectivity matrix is constructed based on cosine similarity . if the cosine similarity between two sentences exceeds a particular predefined threshold , a corresponding edge is added to the connectivity matrix . we provide an evaluation of our method on duc 2004 data . the results show that our approach outperforms centroid - based summarization and is quite successful compared to other summarization systems .
TOP 20
RANK = 1; score = 0.20812182741116753; correct = False; id = 63cf3fd2994747f781ccd2e1e488165d7e060682
classifying relations by ranking with convolutional neural networks relation classification is an important semantic processing task for which state - ofthe - art systems still rely on costly handcrafted features . in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking ( cr - cnn ) . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes . we perform experiments using the the semeval-2010 task 8 dataset , which is designed for the task of classifying the relationship between two nominals marked in a sentence . using crcnn , we outperform the state - of - the - art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features . additionally , our experimental results show that : ( 1 ) our approach is more effective than cnn followed by a softmax classifier ; ( 2 ) omitting the representation of the artificial class other improves both precision and recall ; and ( 3 ) using only word embeddings as input features is enough to achieve state - of - the - art results if we consider only the text between the two target nominals .
RANK = 2; score = 0.20588235294117646; correct = False; id = 0a9c1946ca521804a20b898953c212bad058086d
ualacant : using online machine translation for cross - lingual textual entailment this paper describes a new method for crosslingual textual entailment ( clte ) detection based on machine translation ( mt ) . we use sub - segment translations from different mt systems available online as a source of crosslingual knowledge . in this work we describe and evaluate different features derived from these sub - segment translations , which are used by a support vector machine classifier to detect cltes . we presented this system to the semeval 2012 task 8 obtaining an accuracy up to 59.8 % on the english – spanish test set , the second best performing approach in the contest .
RANK = 3; score = 0.2046783625730994; correct = False; id = e6a4c9c918a4d8154b08bbdc3673da6a6eaeba3c
lexpagerank : prestige in multi - document text summarization multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document . centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo - sentence . we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality ( prestige ) that we call lexpagerank . in this model , a sentence connectivity matrix is constructed based on cosine similarity . if the cosine similarity between two sentences exceeds a particular predefined threshold , a corresponding edge is added to the connectivity matrix . we provide an evaluation of our method on duc 2004 data . the results show that our approach outperforms centroid - based summarization and is quite successful compared to other summarization systems .
RANK = 4; score = 0.20408163265306123; correct = False; id = 9a3cdd8b6c3a9d5d2fd90d18a5463e3f0741511a
identifying patterns for short answer scoring using graph - based lexico - semantic text matching short answer scoring systems typically use regular expressions , templates or logic expressions to detect the presence of specific terms or concepts among student responses . previous work has shown that manually developed regular expressions can provide effective scoring , however manual development can be quite time consuming . in this work we present a new approach that uses word - order graphs to identify important patterns from humanprovided rubric texts and top - scoring student answers . the approach also uses semantic metrics to determine groups of related words , which can represent alternative answers . we evaluate our approach on two datasets : ( 1 ) the kaggle short answer dataset ( asap - sas , 2012 ) , and ( 2 ) a short answer dataset provided by mohler et al . ( 2011 ) . we show that our automated approach performs better than the best performing kaggle entry and generalizes as a method to the mohler dataset .
RANK = 5; score = 0.203125; correct = False; id = 33793d860add7ce6bbcdaaa056ba5e973663c116
high - order low - rank tensors for semantic role labeling this paper introduces a tensor - based approach to semantic role labeling ( srl ) . the motivation behind the approach is to automatically induce a compact feature representation for words and their relations , tailoring them to the task . in this sense , our dimensionality reduction method provides a clear alternative to the traditional feature engineering approach used in srl . to capture meaningful interactions between the argument , predicate , their syntactic path and the corresponding role label , we compress each feature representation first to a lower dimensional space prior to assessing their interactions . this corresponds to using an overall cross - product feature representation and maintaining associated parameters as a four - way low - rank tensor . the tensor parameters are optimized for the srl performance using standard online algorithms . our tensor - based approach rivals the best performing system on the conll-2009 shared task . in addition , we demonstrate that adding the representation tensor to a competitive tensorfree model yields 2 % absolute increase in fscore.1
RANK = 6; score = 0.20238095238095238; correct = False; id = d55824c7799f2d6ddf6abc73270007a6ecb9cb20
latent vector weighting for word meaning in context this paper presents a novel method for the computation of word meaning in context . we make use of a factorization model in which words , together with their window - based context words and their dependency relations , are linked to latent dimensions . the factorization model allows us to determine which dimensions are important for a particular context , and adapt the dependency - based feature vector of the word accordingly . the evaluation on a lexical substitution task – carried out for both english and french – indicates that our approach is able to reach better results than state - of - the - art methods in lexical substitution , while at the same time providing more accurate meaning representations .
RANK = 7; score = 0.20118343195266272; correct = False; id = a2218c7c0044552047d706317210633cb293307b
combining heterogeneous models for measuring relational similarity in this work , we study the problem of measuring relational similarity between two word pairs ( e.g. , silverware : fork and clothing : shirt ) . due to the large number of possible relations , we argue that it is important to combine multiple models based on heterogeneous information sources . our overall system consists of two novel general - purpose relational similarity models and three specific word relation models . when evaluated in the setting of a recently proposed semeval-2012 task , our approach outperforms the previous best system substantially , achieving a 54.1 % relative increase in spearman ’s rank correlation .
RANK = 8; score = 0.20105820105820105; correct = False; id = 749fc5a1c9b8bb1561f31ef108b3678a6e87155a
unsupervised language model adaptation incorporating named entity information language model ( lm ) adaptation is important for both speech and language processing . it is often achieved by combining a generic lm with a topic - specific model that is more relevant to the target document . unlike previous work on unsupervised lm adaptation , this paper investigates how effectively using named entity ( ne ) information , instead of considering all the words , helps lm adaptation . we evaluate two latent topic analysis approaches in this paper , namely , clustering and latent dirichlet allocation ( lda ) . in addition , a new dynamically adapted weighting scheme for topic mixture models is proposed based on lda topic analysis . our experimental results show that the ne - driven lm adaptation framework outperforms the baseline generic lm . the best result is obtained using the lda - based approach by expanding the named entities with syntactically filtered words , together with using a large number of topics , which yields a perplexity reduction of 14.23 % compared to the baseline generic lm .
RANK = 9; score = 0.2; correct = False; id = 4bf72e92628ebacd2e1cfd533c63d67f0fa47edc
adapting phrase - based machine translation to normalise medical terms in social media messages previous studies have shown that health reports in social media , such as dailystrength and twitter , have potential for monitoring health conditions ( e.g. adverse drug reactions , infectious diseases ) in particular communities . however , in order for a machine to understand and make inferences on these health conditions , the ability to recognise when laymen ’s terms refer to a particular medical concept ( i.e. text normalisation ) is required . to achieve this , we propose to adapt an existing phrase - based machine translation ( mt ) technique and a vector representation of words to map between a social media phrase and a medical concept . we evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions . our experimental results show that the combination of a phrase - based mt technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55 % .
RANK = 10; score = 0.1989247311827957; correct = False; id = b1d09188ea6cb4fc7822f771aab442e4f0e8d0ef
uniba - core : combining strategies for semantic textual similarity this paper describes the uniba participation in the semantic textual similarity ( sts ) core task 2013 . we exploited three different systems for computing the similarity between two texts . a system is used as baseline , which represents the best model emerged from our previous participation in sts 2012 . such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together . in addition , we investigated the use of two different learning strategies exploiting both syntactic and semantic features . the former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets . the latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose .
RANK = 11; score = 0.19791666666666666; correct = False; id = c887209b87aff1877420346dc89cdef64ca82910
improving text normalization via unsupervised model and discriminative reranking various models have been developed for normalizing informal text . in this paper , we propose two methods to improve normalization performance . first is an unsupervised approach that automatically identifies pairs of a non - standard token and proper word from a large unlabeled corpus . we use semantic similarity based on continuous word vector representation , together with other surface similarity measurement . second we propose a reranking strategy to combine the results from different systems . this allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case . both wordand sentence - level optimization schemes are explored in this study . we evaluate our approach on data sets used in prior studies , and demonstrate that our proposed methods perform better than the state - of - the - art systems .
RANK = 12; score = 0.1978021978021978; correct = False; id = 728aa52045cedce0ffb11975d880c7046abef3f2
learning hybrid representations to retrieve semantically equivalent questions retrieving similar questions in online q&a community sites is a difficult task because different users may formulate the same question in a variety of ways , using different vocabulary and structure . in this work , we propose a new neural network architecture to perform the task of semantically equivalent question retrieval . the proposed architecture , which we call bow - cnn , combines a bag - ofwords ( bow ) representation with a distributed vector representation created by a convolutional neural network ( cnn ) . we perform experiments using data collected from two stack exchange communities . our experimental results evidence that : ( 1 ) bow - cnn is more effective than bow based information retrieval methods such as tfidf ; ( 2 ) bow - cnn is more robust than the pure cnn for long texts .
RANK = 13; score = 0.1951219512195122; correct = False; id = 4404a5ba8e9b6619f88bca58dc6f281e4a0dde49
ricoh at semeval-2016 task 1 : ir - based semantic textual similarity estimation this paper describes our ir ( information retrieval ) based method for semeval 2016 task 1 , semantic textual similarity ( sts ) . the main feature of our approach is to extend a conventional ir - based scheme by incorporating word alignment information . this enables us to develop a more fine - grained similarity measurement . in the evaluation results , we have seen that the proposed method improves upon a conventional ir - based method on average . in addition , one of our submissions achieved the best performance for the “ postediting ” data set .
RANK = 14; score = 0.19270833333333334; correct = False; id = 5de25f171cfd569b12d7233d37b451e6ddca0d99
user participation prediction in online forums online community is an important source for latest news and information . accurate prediction of a user ’s interest can help provide better user experience . in this paper , we develop a recommendation system for online forums . there are a lot of differences between online forums and formal media . for example , content generated by users in online forums contains more noise compared to formal documents . content topics in the same forum are more focused than sources like news websites . some of these differences present challenges to traditional word - based user profiling and recommendation systems , but some also provide opportunities for better recommendation performance . in our recommendation system , we propose to ( a ) use latent topics to interpolate with content - based recommendation ; ( b ) model latent user groups to utilize information from other users . we have collected three types of forum data sets . our experimental results demonstrate that our proposed hybrid approach works well in all three types of forums .
RANK = 15; score = 0.19230769230769232; correct = False; id = 71de8d010bfa56edea9ef4c46bc05aaa100b0a28
using derivation trees for informative treebank inter - annotator agreement evaluation this paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter - annotator agreement . this system makes for a more informative iaa evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types . we evaluate the system on two corpora ( 1 ) a corpus of english web text , and ( 2 ) a corpus of modern british english .
RANK = 16; score = 0.19186046511627908; correct = False; id = bf1c5d2b47657906df421aeb19e0991825882a5f
a feature - enriched tree kernel for relation extraction tree kernel is an effective technique for relation extraction . however , the traditional syntactic tree representation is often too coarse or ambiguous to accurately capture the semantic relation information between two entities . in this paper , we propose a new tree kernel , called feature - enriched tree kernel ( ftk ) , which can enhance the traditional tree kernel by : 1 ) refining the syntactic tree representation by annotating each tree node with a set of discriminant features ; and 2 ) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration . experimental results show that our method can achieve a 5.4 % f - measure improvement over the traditional convolution tree kernel .
RANK = 17; score = 0.19161676646706588; correct = False; id = 459a4abea27d059b4e22fdce74b0d3439f80ef2d
detecting nasty comments from bbs posts we propose a method to detect japanese nasty comments from posts on bulletin board systems ( bbs ) . nasty comments can cause many social problem , because they express potentially harmful words and phrases . there are methods to recognize harmful words , but they are insufficient . therefore , we present a method for detecting such comments on a bbs with many posts using an n - gram model . in addition , we compared our method with a support vector machine ( svm ) that is based on nasty words . as a result , we detected nasty comments that are different to those by the svm . we also observe higher detection accuracy by combining two methods .
RANK = 18; score = 0.19161676646706588; correct = False; id = 00d19c8036d1a09bd9832ab0429013bba2da5151
user embedding for scholarly microblog recommendation nowadays , many scholarly messages are posted on chinese microblogs and more and more researchers tend to find scholarly information on microblogs . in order to exploit microblogging to benefit scientific research , we propose a scholarly microblog recommendation system in this study . it automatically collects and mines scholarly information from chinese microblogs , and makes personalized recommendations to researchers . we propose two different neural network models which learn the vector representations for both users and microblog texts . then the recommendation is accomplished based on the similarity between a user ’s vector and a microblog text ’s vector . we also build a dataset for this task . the two embedding models are evaluated on the dataset and show good results compared to several baselines .
RANK = 19; score = 0.18994413407821228; correct = False; id = 7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917
a comparison of vector - based representations for semantic composition in this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods . we experiment with several possible combinations of representation and composition , exhibiting varying degrees of sophistication . some are shallow while others operate over syntactic structure , rely on parameter learning , or require access to very large corpora . we find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests : ( 1 ) phrase similarity and ( 2 ) paraphrase detection . the sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method .
RANK = 20; score = 0.1896551724137931; correct = False; id = 72b88705083fda3df98e114507ad76094a0465ec
a dependency - based neural network for relation classification previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees . in this paper , we further explore how to make full use of the combination of these dependency information . we first propose a new structure , termed augmented dependency path ( adp ) , which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path . to exploit the semantic representation behind the adp structure , we develop dependency - based neural networks ( depnn ) : a recursive neural network designed to model the subtrees , and a convolutional neural network to capture the most important features on the shortest path . experiments on the semeval-2010 dataset show that our proposed method achieves state - of - art results .

RANKING 1762
QUERY
neural transductive learning and beyond : morphological generation in the minimal - resource setting neural state - of - the - art sequence - to - sequence ( seq2seq ) models often do not perform well for small training sets . we address paradigm completion , the morphological task of , given a partial paradigm , generating all missing forms . we propose two new methods for the minimalresource setting : ( i ) paradigm transduction : since we assume only few paradigms available for training , neural seq2seq models are able to capture relationships between paradigm cells , but are tied to the idiosyncracies of the training set . paradigm transduction mitigates this problem by exploiting the input subset of inflected forms at test time . ( ii ) source selection with high precision ( ship ) : multi - source models which learn to automatically select one or multiple sources to predict a target inflection do not perform well in the minimal - resource setting . ship is an alternative to identify a reliable source if training data is limited . on a 52-language benchmark dataset , we outperform the previous state of the art by up to 9.71 % absolute accuracy .
First cited at 46
TOP CITED PAPERS
RANK 46
single - model encoder - decoder with explicit morphological representation for reinflection morphological reinflection is the task of generating a target form given a source form , a source tag and a target tag . we propose a new way of modeling this task with neural encoder - decoder models . our approach reduces the amount of required training data for this architecture and achieves state - of - the - art results , making encoder - decoder models applicable to morphological reinflection even for lowresource languages . we further present a new automatic correction method for the outputs based on edit trees .
RANK 61
supervised learning of complete morphological paradigms we describe a supervised approach to predicting the set of all inflected forms of a lexical item . our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples , and then learns the contexts in which those transformations apply using a discriminative sequence model . because our approach is completely data - driven and the model is trained on examples extracted from wiktionary , our method can extend to new languages without change . our end - to - end system is able to predict complete paradigms with 86.1 % accuracy and individual inflected forms with 94.9 % accuracy , averaged across three languages and two parts of speech .
RANK 84
latent - variable modeling of string transductions with finite - state methods string - to - string transduction is a central problem in computational linguistics and natural language processing . it occurs in tasks as diverse as name transliteration , spelling correction , pronunciation modeling and inflectional morphology . we present a conditional loglinear model for string - to - string transduction , which employs overlapping features over latent alignment sequences , and which learns latent classes and latent string pair regions from incomplete training data . we evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results , even when trained on small data sets . on the task of generating morphological forms , we outperform a baseline method reducing the error rate by up to 48 % . on a lemmatization task , we reduce the error rates in wicentowski ( 2002 ) by 38–92 % .
TOP UNCITED PAPERS
RANK 1
modeling sentences in the latent space sentence similarity is the process of computing a similarity score between two sentences . previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences . in this paper , we show that by carefully handling words that are not in the sentences ( missing words ) , we can train a reliable latent variable model on sentences . in the process , we propose a new evaluation framework for sentence similarity : concept definition retrieval . the new framework allows for large scale tuning and testing of sentence similarity models . experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models . our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity .
RANK 2
addressee and response selection for multi - party conversation to create conversational systems working in actual situations , it is crucial to assume that they interact with multiple agents . in this work , we tackle addressee and response selection for multi - party conversation , in which systems are expected to select whom they address as well as what they say . the key challenge of this task is to jointly model who is talking about what in a previous context . for the joint modeling , we propose two modeling frameworks : 1 ) static modeling and 2 ) dynamic modeling . to show benchmark results of our frameworks , we created a multi - party conversation corpus . our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents .
RANK 3
an unsupervised method for automatic translation memory cleaning we address the problem of automatically cleaning a large - scale translation memory ( tm ) in a fully unsupervised fashion , i.e. without human - labelled data . we approach the task by : i ) designing a set of features that capture the similarity between two text segments in different languages , ii ) use them to induce reliable training labels for a subset of the translation units ( tus ) contained in the tm , and iii ) use the automatically labelled data to train an ensemble of binary classifiers . we apply our method to clean a test set composed of 1,000 tus randomly extracted from the english - italian version of mymemory , the world ’s largest public tm . our results show competitive performance not only against a strong baseline that exploits machine translation , but also against a state - of - the - art method that relies on human - labelled data .
TOP 20
RANK = 1; score = 0.20809248554913296; correct = False; id = 01976ebd5efbde258544de2f01776081e32afa52
modeling sentences in the latent space sentence similarity is the process of computing a similarity score between two sentences . previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences . in this paper , we show that by carefully handling words that are not in the sentences ( missing words ) , we can train a reliable latent variable model on sentences . in the process , we propose a new evaluation framework for sentence similarity : concept definition retrieval . the new framework allows for large scale tuning and testing of sentence similarity models . experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models . our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity .
RANK = 2; score = 0.2046783625730994; correct = False; id = 811e014002d1e4d1e185fc236cf9e3fafe2aade5
addressee and response selection for multi - party conversation to create conversational systems working in actual situations , it is crucial to assume that they interact with multiple agents . in this work , we tackle addressee and response selection for multi - party conversation , in which systems are expected to select whom they address as well as what they say . the key challenge of this task is to jointly model who is talking about what in a previous context . for the joint modeling , we propose two modeling frameworks : 1 ) static modeling and 2 ) dynamic modeling . to show benchmark results of our frameworks , we created a multi - party conversation corpus . our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents .
RANK = 3; score = 0.20441988950276244; correct = False; id = 2bfd42435b576a552ffebe597406ff0760203cc5
an unsupervised method for automatic translation memory cleaning we address the problem of automatically cleaning a large - scale translation memory ( tm ) in a fully unsupervised fashion , i.e. without human - labelled data . we approach the task by : i ) designing a set of features that capture the similarity between two text segments in different languages , ii ) use them to induce reliable training labels for a subset of the translation units ( tus ) contained in the tm , and iii ) use the automatically labelled data to train an ensemble of binary classifiers . we apply our method to clean a test set composed of 1,000 tus randomly extracted from the english - italian version of mymemory , the world ’s largest public tm . our results show competitive performance not only against a strong baseline that exploits machine translation , but also against a state - of - the - art method that relies on human - labelled data .
RANK = 4; score = 0.2033898305084746; correct = False; id = 1670f5c8e64d008bd7761df72856430361cdf32e
i2rntu at semeval-2016 task 4 : classifier fusion for polarity classification in twitter in this work , we apply classifier fusion to tweet polarity identification problem . the task is to predict whether the emotion hidden in a tweet is positive , neutral , or negative . an asymmetric simpls ( asimpls ) based classifier , which was proved to be able to identify the minority class well in imbalanced classification problems , is implemented . word embedding is also employed as a new feature . for each word , we obtain three word embedding vectors on positive , neutral , and negative tweet sets respectively . these vectors are used as features in the asimpls classifier . another three state - of - the - art systems are implemented also , and these four systems are fused together to further boost the performance . the fusion system achieved 59.63 % accuracy on the 2016 test set of semeval2016 task 4 , subtask a.
RANK = 5; score = 0.20261437908496732; correct = False; id = e3b31ab11bbcc8c9003604292471529a34804400
named entity recognition for novel types by transfer learning in named entity recognition , we often do n’t have a large in - domain training corpus or a knowledge base with adequate coverage to train a model directly . in this paper , we propose a method where , given training data in a related domain with similar ( but not identical ) named entity ( ne ) types and a small amount of in - domain training data , we use transfer learning to learn a domain - specific ne model . that is , the novelty in the task setup is that we assume not just domain mismatch , but also label mismatch .
RANK = 6; score = 0.20108695652173914; correct = False; id = 931db8aa08c8ad52fea78829438ea7a63e0e286d
vpctagger : detecting verb - particle constructions with syntax - based methods verb - particle combinations ( vpcs ) consist of a verbal and a preposition / particle component , which often have some additional meaning compared to the meaning of their parts . if a data - driven morphological parser or a syntactic parser is trained on a dataset annotated with extra information for vpcs , they will be able to identify vpcs in raw texts . in this paper , we examine how syntactic parsers perform on this task and we introduce vpctagger , a machine learning - based tool that is able to identify english vpcs in context . our method consists of two steps : it first selects vpc candidates on the basis of syntactic information and then selects genuine vpcs among them by exploiting new features like semantic and contextual ones . based on our results , we see that vpctagger outperforms state - of - the - art methods in the vpc detection task .
RANK = 7; score = 0.2; correct = False; id = 7fe65d3741ebdf983f80a8478d84e1501b98c393
data enhancement and selection strategies for the word - level quality estimation this paper describes the dcu - sheff word - level quality estimation ( qe ) system submitted to the qe shared task at wmt15 . starting from a baseline set of features and a crf algorithm to learn a sequence tagging model , we propose improvements in two ways : ( i ) by filtering out the training sentences containing too few errors , and ( ii ) by adding incomplete sequences to the training data to enrich the model with new information . we also experiment with considering the task as a classification problem , and report results using a subset of the features with random forest classifiers .
RANK = 8; score = 0.2; correct = False; id = 09f0885d1727a0b82300e94856e0be2f2f72561c
sentiment domain adaptation with multiple sources domain adaptation is an important research topic in sentiment analysis area . existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain . in this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains . we first extract both global and domain - specific sentiment knowledge from the data of multiple source domains using multi - task learning . then we transfer them to target domain with the help of words’ sentiment polarity relations extracted from the unlabeled target domain data . the similarities between target domain and different source domains are also incorporated into the adaptation process . experimental results on benchmark dataset show the effectiveness of our approach in improving cross - domain sentiment classification performance .
RANK = 9; score = 0.1977401129943503; correct = False; id = 9d66cee00ba9dffba34ba0f340ba45cbc0a2e4a3
cross - domain co - extraction of sentiment and topic lexicons extracting sentiment and topic lexicons is important for opinion mining . previous works have showed that supervised learning methods are superior for this task . however , the performance of supervised methods highly relies on manually labeled training data . in this paper , we propose a domain adaptation framework for sentimentand topiclexicon co - extraction in a domain of interest where we do not require any labeled data , but have lots of labeled data in another related domain . the framework is twofold . in the first step , we generate a few high - confidence sentiment and topic seeds in the target domain . in the second step , we propose a novel relational adaptive bootstrapping ( rap ) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words . experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation .
RANK = 10; score = 0.1977401129943503; correct = False; id = 593338fbbb79065891beaf985a95860bd39f52bf
semi - supervised learning for neural machine translation while end - to - end neural machine translation ( nmt ) has made remarkable progress recently , nmt systems only rely on parallel corpora for parameter estimation . since parallel corpora are usually limited in quantity , quality , and coverage , especially for low - resource languages , it is appealing to exploit monolingual corpora to improve nmt . we propose a semisupervised approach for training nmt models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data . the central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto - target and target - to - source translation models serve as the encoder and decoder , respectively . our approach can not only exploit the monolingual corpora of the target language , but also of the source language . experiments on the chineseenglish dataset show that our approach achieves significant improvements over state - of - the - art smt and nmt systems .
RANK = 11; score = 0.19704433497536947; correct = False; id = b4ecca89aacb66b163f76d32e6e0c18368441c0a
semi - supervised semantic tagging of conversational understanding using markov topic regression finding concepts in natural language utterances is a challenging task , especially given the scarcity of labeled data for learning semantic ambiguity . furthermore , data mismatch issues , which arise when the expected test ( target ) data does not exactly match the training data , aggravate this scarcity problem . to deal with these issues , we describe an efficient semisupervised learning ( ssl ) approach which has two components : ( i ) markov topic regression is a new probabilistic model to cluster words into semantic tags ( concepts ) . it can efficiently handle semantic ambiguity by extending standard topic models with two new features . first , it encodes word n - gram features from labeled source and unlabeled target data . second , by going beyond a bag - of - words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context . ( ii ) retrospective learner is a new learning technique that adapts to the unlabeled target data . our new ssl approach improves semantic tagging performance by 3 % absolute over the baseline models , and also compares favorably on semi - supervised syntactic tagging .
RANK = 12; score = 0.19642857142857142; correct = False; id = 418c5d3488698b81098b74c2b1fa275a3ece2769
exploiting latent information to predict diffusions of novel topics on social networks this paper brings a marriage of two seemly unrelated topics , natural language processing ( nlp ) and social network analysis ( sna ) . we propose a new task in sna which is to predict the diffusion of a new topic , and design a learning - based framework to solve this problem . we exploit the latent semantic information among users , topics , and social connections as features for prediction . our framework is evaluated on real data collected from public domain . the experiments show 16 % auc improvement over baseline methods . the source code and dataset are available at http://www.csie.ntu.edu.tw/~d97944007/dif fusion/
RANK = 13; score = 0.19597989949748743; correct = False; id = 013428172739ca8cff3d9f2fb96d32cdf3ae612e
learning to predict case markers in japanese japanese case markers , which indicate the grammatical relation of the complement np to the predicate , often pose challenges to the generation of japanese text , be it done by a foreign language learner , or by a machine translation ( mt ) system . in this paper , we describe the task of predicting japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the japanese sentence ; and ( ii ) b lingual , when also given information from a corresponding english source sentence in an mt context . we formulate the task after the well - studied task of english semantic role labelling , and explore features from a syntactic dependency structure of the sentence . for the monolingual task , we evaluated our models on the kyoto corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase . for the bilingual task , we achieved an accuracy of 92 % per phrase using a bilingual dataset from a technical domain . we show that in both settings , features that exploit dependency information , whether derived from gold - standard annotations or automatically assigned , contribute significantly to the prediction of case markers . 1
RANK = 14; score = 0.19597989949748743; correct = False; id = 111501ce052a6f4f323214f081b26f04528207b4
summarizing source code using a neural attention model high quality source code is often paired with high level summaries of the computation it performs , for example in code documentation or in descriptions posted in online forums . such summaries are extremely useful for applications such as code search but are expensive to manually author , hence only done for a small fraction of all code that is produced . in this paper , we present the first completely datadriven approach for generating high level summaries of source code . our model , code - nn , uses long short term memory ( lstm ) networks with attention to produce sentences that describe c # code snippets and sql queries . code - nn is trained on a new corpus that is automatically collected from stackoverflow , which we release . experiments demonstrate strong performance on two tasks : ( 1 ) code summarization , where we establish the first end - to - end learning results and outperform strong baselines , and ( 2 ) code retrieval , where our learned model improves the state of the art on a recently introduced c # benchmark by a large margin .
RANK = 15; score = 0.1956521739130435; correct = False; id = add066d420bb2cf59ba8d51647426d7167dd5ccc
thread - level information for comment classification in community question answering community question answering ( cqa ) is a new application of qa in social contexts ( e.g. , fora ) . it presents new interesting challenges and research directions , e.g. , exploiting the dependencies between the different comments of a thread to select the best answer for a given question . in this paper , we explored two ways of modeling such dependencies : ( i ) by designing specific features looking globally at the thread ; and ( ii ) by applying structure prediction models . we trained and evaluated our models on data from semeval-2015 task 3 on answer selection in cqa . our experiments show that : ( i ) the thread - level features consistently improve the performance for a variety of machine learning models , yielding state - of - the - art results ; and ( ii ) sequential dependencies between the answer labels captured by structured prediction models are not enough to improve the results , indicating that more information is needed in the joint model .
RANK = 16; score = 0.1951219512195122; correct = False; id = 83cf4b2f39bcc802b09fd59b69e23068447b26b7
multi - task learning for multiple language translation in this paper , we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages . our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem . we extend the neural machine translation to a multi - task learning framework which shares source language representation and separates the modeling of different target language translation . our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available . experiments show that our multi - task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available .
RANK = 17; score = 0.1945945945945946; correct = False; id = 26362bf123d93f1d2cc268b3086473ad7373119d
generating abbreviations for chinese named entities using recurrent neural network with dynamic dictionary chinese named entities occur frequently in formal and informal environments . various approaches have been formalized the problem as a sequence labelling task and utilize a character - based methodology , in which character is treated as the basic classification unit . one of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of chinese . to address this problem , we propose a novel neural network architecture to perform task . it combines recurrent neural network ( rnn ) with an architecture determining whether a given sequence of characters can be a word or not . for demonstrating the effectiveness of the proposed method , we evaluate it on chinese named entity generation and opinion target extraction tasks . experimental results show that the proposed method can achieve better performance than state - ofthe - art methods .
RANK = 18; score = 0.193717277486911; correct = False; id = 0854de0ac24222529709c4d4bb9c14eddded7ec6
regularized interlingual projections : evaluation on multilingual transliteration in this paper , we address the problem of building a multilingual transliteration system using an interlingual representation . our approach uses international phonetic alphabet ( ipa ) to learn the interlingual representation and thus allows us to use any word and its ipa representation as a training example . thus , our approach requires only monolingual resources : a phoneme dictionary that lists words and their ipa representations.1 by adding a phoneme dictionary of a new language , we can readily build a transliteration system into any of the existing previous languages , without the expense of all - pairs data or computation . we also propose a regularization framework for learning the interlingual representation , which accounts for language specific phonemic variability , and thus it can find better mappings between languages . experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29 % accuracy and an average improvement of 17 % accuracy compared to a state - of - the - art baseline system .
RANK = 19; score = 0.1932367149758454; correct = False; id = b90196005b0f1eb0953758c16869633d7ca3399d
a multi - pass sieve for coreference resolution most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features . this approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones . to overcome this problem , we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision . each tier builds on the previous tier ’s entity cluster output . further , our model propagates global information by sharing attributes ( e.g. , gender and number ) across mentions in the same cluster . this cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time . the framework is highly modular : new coreference modules can be plugged in without any change to the other modules . in spite of its simplicity , our approach outperforms many state - of - the - art supervised and unsupervised models on several standard corpora . this suggests that sievebased approaches could be applied to other nlp tasks .
RANK = 20; score = 0.19270833333333334; correct = False; id = 812965ddce635174b33621aaaa551e5f6199b6c0
if all you have is a bit of the bible : learning pos taggers for truly low - resource languages we present a simple method for learning part - of - speech taggers for languages like akawaio , aukan , or cakchiquel – languages for which nothing but a translation of parts of the bible exists . by aggregating over the tags from a few annotated languages and spreading them via wordalignment on the verses , we learn pos taggers for 100 languages , using the languages to bootstrap each other . we evaluate our cross - lingual models on the 25 languages where test sets exist , as well as on another 10 for which we have tag dictionaries . our approach performs much better ( 20 - 30 % ) than state - of - the - art unsupervised pos taggers induced from bible translations , and is often competitive with weakly supervised approaches that assume high - quality parallel corpora , representative monolingual corpora with perfect tokenization , and/or tag dictionaries . we make models for all 100 languages available .

RANKING 462
QUERY
cogcomptime : a tool for understanding time in natural language automatic extraction of temporal information is important for natural language understanding . it involves two basic tasks : ( 1 ) understanding time expressions that are mentioned explicitly in text ( e.g. , february 27 , 1998 or tomorrow ) , and ( 2 ) understanding temporal information that is conveyed implicitly via relations . this paper introduces cogcomptime , a system that has these two important functionalities . it incorporates the most recent progress , achieves state - of - the - art performance , and is publicly available.1 we believe that this demo will provide valuable insight for temporal understanding and be useful for multiple time - aware applications .
First cited at 3
TOP CITED PAPERS
RANK 3
a robust shallow temporal reasoning system this paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text : extraction , normalization to time intervals and comparison . our system makes use of an existing state - of - the - art temporal extraction system , on top of which we add several important novel contributions . in addition , we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations . experimental study shows that the system achieves excellent performance on all the tasks we address .
RANK 941
event detection and co - reference with minimal supervision an important aspect of natural language understanding involves recognizing and categorizing events and the relations among them . however , these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task , resulting in supervised systems that attempt to learn complex models from small amounts of data , which they over - fit . this paper addresses this challenge by developing an event detection and co - reference system with minimal supervision , in the form of a few event examples . we view these tasks as semantic similarity problems between event mentions or event mentions and an ontology of types , thus facilitating the use of large amounts of out of domain text data . notably , our semantic relatedness function exploits the structure of the text by making use of a semantic - role - labeling based representation of an event . we show that our approach to event detection is competitive with the top supervised methods . more significantly , we outperform stateof - the - art supervised methods for event coreference on benchmark data sets , and support significantly better transfer across domains .
RANK 1799
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
TOP UNCITED PAPERS
RANK 1
mplus : a probabilistic medical language understanding system this paper describes the basic philosophy and implementation of mplus ( m+ ) , a robust medical text analysis tool that uses a semantic model based on bayesian networks ( bns ) . bns provide a concise and useful formalism for representing semantic patterns in medical text , and for recognizing and reasoning over those patterns . bns are noise - tolerant , and facilitate the training of m+ .
RANK 2
edinburgh - ltg : tempeval-2 system description we describe the edinburgh information extraction system which we are currently adapting for analysis of newspaper text as part of the sync3 project . our most recent focus is geospatial and temporal grounding of entities and it has been useful to participate in tempeval-2 to measure the performance of our system and to guide further development . we took part in tasks a and b for english .
RANK 4
performance issues and error analysis in an open - domain question answering system this paper presents an in - depth analysis of a state - of - the - art question answering system . several scenarios are examined : ( 1 ) the performance of each module in a serial baseline system , ( 2 ) the impact of feedbacks and the insertion of a logic prover , and ( 3 ) the impact of various retrieval strategies and lexical resources . the main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding .
TOP 20
RANK = 1; score = 0.22857142857142856; correct = False; id = c0c6bd88e4b2c0273a4e6ba078b87ff8190765be
mplus : a probabilistic medical language understanding system this paper describes the basic philosophy and implementation of mplus ( m+ ) , a robust medical text analysis tool that uses a semantic model based on bayesian networks ( bns ) . bns provide a concise and useful formalism for representing semantic patterns in medical text , and for recognizing and reasoning over those patterns . bns are noise - tolerant , and facilitate the training of m+ .
RANK = 2; score = 0.22857142857142856; correct = False; id = 002cce922445d1513bd0e296b695925a9fd1af53
edinburgh - ltg : tempeval-2 system description we describe the edinburgh information extraction system which we are currently adapting for analysis of newspaper text as part of the sync3 project . our most recent focus is geospatial and temporal grounding of entities and it has been useful to participate in tempeval-2 to measure the performance of our system and to guide further development . we took part in tasks a and b for english .
RANK = 3; score = 0.22413793103448276; correct = True; id = 5de957a6bd6ef34a47147da2f11eb4b7d15bdb04
a robust shallow temporal reasoning system this paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text : extraction , normalization to time intervals and comparison . our system makes use of an existing state - of - the - art temporal extraction system , on top of which we add several important novel contributions . in addition , we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations . experimental study shows that the system achieves excellent performance on all the tasks we address .
RANK = 4; score = 0.21929824561403508; correct = False; id = 4a7f8987748828ec6010bff2a5dac6b33d4de659
performance issues and error analysis in an open - domain question answering system this paper presents an in - depth analysis of a state - of - the - art question answering system . several scenarios are examined : ( 1 ) the performance of each module in a serial baseline system , ( 2 ) the impact of feedbacks and the insertion of a logic prover , and ( 3 ) the impact of various retrieval strategies and lexical resources . the main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding .
RANK = 5; score = 0.21487603305785125; correct = False; id = 15463471d7de5913bea6ea669a8c9d68f3911437
improving nominal srl in chinese language with verbal srl information and automatic predicate recognition this paper explores chinese semantic role labeling ( srl ) for nominal predicates . besides those widely used features in verbal srl , various nominal srl - specific features are first included . then , we improve the performance of nominal srl by integrating useful features derived from a state - of - the - art verbal srl system . finally , we address the issue of automatic predicate recognition , which is essential for a nominal srl system . evaluation on chinese nombank shows that our research in integrating various features derived from verbal srl significantly improves the performance . it also shows that our nominal srl system much outperforms the state - of - the - art ones .
RANK = 6; score = 0.21359223300970873; correct = False; id = ad32f484eccfa6b72b2b43cf18cb222ab1a55bbc
sutime : evaluation in tempeval-3 we analyze the performance of sutime , a temporal tagger for recognizing and normalizing temporal expressions , on tempeval-3 task a for english . sutime is available as part of the stanford corenlp pipeline and can be used to annotate documents with temporal information . testing on the tempeval-3 evaluation corpus showed that this system is competitive with state - of - the - art techniques .
RANK = 7; score = 0.2125984251968504; correct = False; id = 5c552b358f4c478c64dbea9464de7fb8c7e7cee7
understanding temporal expressions in emails recent years have seen increasing research on extracting and using temporal information in natural language applications . however most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts . in this paper we report our work on anchoring temporal expressions in a novel genre , emails . the highly under - specified nature of these expressions fits well with our constraintbased representation of time , time calculus for natural language ( tcnl ) . we have developed and evaluated a temporal expression anchoror ( tea ) , and the result shows that it performs significantly better than the baseline , and compares favorably with some of the closely related work .
RANK = 8; score = 0.21153846153846154; correct = False; id = 23385241580e80036a901b762c5ad34f416fdeb8
automatic keyphrase extraction : a survey of the state of the art while automatic keyphrase extraction has been examined extensively , state - of - theart performance on this task is still much lower than that on many core natural language processing tasks . we present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .
RANK = 9; score = 0.21014492753623187; correct = False; id = c11ba1886b5c5f6d2a009a79067291f26bf0266d
applying machine learning to chinese temporal relation resolution temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language . this information is often inferred from a variety of interactive grammatical and lexical cues , especially in chinese . for this purpose , inter - clause relations ( temporal or otherwise ) in a multiple - clause sentence play an important role . in this paper , a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a chinese multiple - clause sentence . the model makes use of the fact that events are represented in different temporal structures . it takes into account the effects of linguistic features such as tense / aspect , temporal connectives , and discourse structures . a set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution .
RANK = 10; score = 0.20863309352517986; correct = False; id = 2e0264c4a6b31f1032a4079b7f259d07d13cf4d9
the acl anthology searchbench we describe a novel application for structured search in scientific digital libraries . the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology . the application provides search in both its bibliographic metadata and semantically analyzed full textual content . by combining these two features , very efficient and focused queries are possible . at the same time , the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology . the system currently indexes the textual content of 7,500 anthology papers from 2002–2009 with predicateargument - like semantic structures . it also provides useful search filters based on bibliographic metadata . it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques .
RANK = 11; score = 0.20863309352517986; correct = False; id = 7a5a0d80d918ec724c0aa32364c7171dd109c259
exploiting qualitative information from automatic word alignment for cross - lingual nlp tasks the use of automatic word alignment to capture sentence - level semantic relations is common to a number of cross - lingual nlp applications . despite its proved usefulness , however , word alignment information is typically considered from a quantitative point of view ( e.g. the number of alignments ) , disregarding qualitative aspects ( the importance of aligned terms ) . in this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity . focusing on the cross - lingual textual entailment task , we contribute with a novel method that : i ) significantly outperforms the state of the art , and ii ) is portable , with limited loss in performance , to language pairs where training data are not available .
RANK = 12; score = 0.20833333333333334; correct = False; id = 90f4659ad94b5cae3a57fe84e7878932cca07b85
classifiers and semantic type coercion : motivating a new classification of classifiers this paper argues that the traditional view that nouns refer only to classic individuals is inadequate . instead , we argue that nouns are coerced by different types of classifiers to refer to kinds and events as well as to individuals . this finding is important because 1 ) the semantics of nouns involves more than just individuals , and 2 ) it is the first time that the previously abstract semantic distinctions between kinds , individuals and events is found to be instantiated in a particular system of a natural language grammar , namely , the classifier system .
RANK = 13; score = 0.20833333333333334; correct = False; id = 88efb1ea3db42b155a7c3fd35e5e010c970453e5
open ie as an intermediate structure for semantic tasks semantic applications typically extract information from intermediate structures derived from sentences , such as dependency parse or semantic role labeling . in this paper , we study open information extraction ’s ( open ie ) output as an additional intermediate structure and find that for tasks such as text comprehension , word similarity and word analogy it can be very effective . specifically , for word analogy , open ie - based embeddings surpass the state of the art . we suggest that semantic applications will likely benefit from adding open ie format to their set of potential sentencelevel structures .
RANK = 14; score = 0.208; correct = False; id = d08ee8ae52fbf31d006b4cc1295ceb747837fc6f
decompounding query keywords from compounding languages splitting compound words has proved to be useful in areas such as machine translation , speech recognition or information retrieval ( ir ) . furthermore , real - time ir systems ( such as search engines ) need to cope with noisy data , as user queries are sometimes written quickly and submitted without review . in this paper we apply a state - of - the - art procedure for german decompounding to other compounding languages , and we show that it is possible to have a single decompounding model that is applicable across languages .
RANK = 15; score = 0.208; correct = False; id = 07c54adef9593b1d5163fe34f4cb24844a33a94b
automatic identification of bengali noun - noun compounds using random forest this paper presents a supervised machine learning approach that uses a machine learning algorithm called random forest for recognition of bengali noun - noun compounds as multiword expression ( mwe ) from bengali corpus . our proposed approach to mwe recognition has two steps : ( 1 ) extraction of candidate multi - word expressions using chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi - word expression as multi - word expression or not . a variety of association measures , syntactic and linguistic clues are used as features for identifying mwes . the proposed system is tested on a bengali corpus for identifying noun - noun compound mwes from the corpus .
RANK = 16; score = 0.2077922077922078; correct = False; id = 145b2d9f1ce3152b58cef3c413b3b9c08d916fdb
transformation based learning in the fast lane transformation - based learning has been successfully employed to solve many natural language processing problems . it achieves state - of - the - art performance on many natural language processing tasks and does not overtrain easily . however , it does have a serious drawback : the training time is often intorelably long , especially on the large corpora which are often used in nlp . in this paper , we present a novel and realistic method for speeding up the training time of a transformation - based learner without sacri cing performance . the paper compares and contrasts the training time needed and performance achieved by our modi ed learner with two other systems : a standard transformation - based learner , and the ica system ( hepple , 2000 ) . the results of these experiments show that our system is able to achieve a signi ca nt improvement in training time while still achieving the same performance as a standard transformation - based learner . this is a valuable contribution to systems and algorithms which utilize transformation - based learning at any part of the execution .
RANK = 17; score = 0.20689655172413793; correct = False; id = b2da31da228c48f0800fa1445dc219456655bb33
an ontological analysis of japanese and chinese kinship terms most languages have some expressions to refer to family members ( e.g. , those referring to ‘ father,’ ‘ brother,’ ‘ uncle,’ etc . ) . in this paper , we will provide an analysis of japanese and chinese kinship terms , under a framework whose representational system has an ontological nature . it will be shown that this framework is effective not only in figuring out similarities and differences among the kinship terms of a particular language , but also in comparing the kinship terms of different languages .
RANK = 18; score = 0.2066115702479339; correct = False; id = 94770c3ed3c30a4b90885fb7593fe8d0ffeb705c
extracting comparative entities and predicates from texts using comparative type classification the automatic extraction of comparative information is an important text mining problem and an area of increasing interest . in this paper , we study how to build a korean comparison mining system . our work is composed of two consecutive tasks : 1 ) classifying comparative sentences into different types and 2 ) mining comparative entities and predicates . we perform various experiments to find relevant features and learning techniques . as a result , we achieve outstanding performance enough for practical use .
RANK = 19; score = 0.2066115702479339; correct = False; id = 2e614550c2e130821c341cf1354fefa3e64f2fa3
a systematic exploration of diversity in machine translation this paper addresses the problem of producing a diverse set of plausible translations . we present a simple procedure that can be used with any statistical machine translation ( mt ) system . we explore three ways of using diverse translations : ( 1 ) system combination , ( 2 ) discriminative reranking with rich features , and ( 3 ) a novel post - editing scenario in which multiple translations are presented to users . we find that diversity can improve performance on these tasks , especially for sentences that are difficult for mt .
RANK = 20; score = 0.20634920634920634; correct = False; id = 3b1c29ce86c21acd22d6c2b48c944b62050fb4d1
extracting data records from unstructured biomedical full text in this paper , we address the problem of extracting data records and their attributes from unstructured biomedical full text . there has been little effort reported on this in the research community . we argue that semantics is important for record extraction or finer - grained language processing tasks . we derive a data record template including semantic language models from unstructured text and represent them with a discourse level conditional random fields ( crf ) model . we evaluate the approach from the perspective of information extraction and achieve significant improvements on system performance compared with other baseline systems .

RANKING 39
QUERY
a short answer grading system in chinese by support vector approach in this paper , we report a short answer grading system in chinese . we build a system based on standard machine learning approaches and test it with translated corpus from two publicly available corpus in english . the experiment results show similar results on two different corpus as in english .
First cited at 50
TOP CITED PAPERS
RANK 50
text - to - text semantic similarity for automatic short answer grading in this paper , we explore unsupervised techniques for the task of automatic short answer grading . we compare a number of knowledge - based and corpus - based measures of text similarity , evaluate the effect of domain and size on the corpus - based measures , and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers . overall , our system significantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past .
RANK 2313
learning to grade short answer questions using semantic similarity measures and dependency graph alignments in this work we address the task of computerassisted assessment of short student answers . we combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation . we also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers .
RANK 6628
semeval-2012 task 6 : a pilot on semantic textual similarity semantic textual similarity ( sts ) measures the degree of semantic equivalence between two texts . this paper presents the results of the sts pilot task in semeval . the training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources . the test data also comprised 2000 sentences pairs for those datasets , plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise . the similarity of pairs of sentences was rated on a 0 - 5 scale ( low to high similarity ) by human judges using amazon mechanical turk , with high pearson correlation scores , around 90 % . 35 teams participated in the task , submitting 88 runs . the best results scored a pearson correlation>80 % , well above a simple lexical baseline that only scored a 31 % correlation . this pilot task opens an exciting way ahead , although there are still open issues , specially the evaluation metric .
TOP UNCITED PAPERS
RANK 1
machine learning for coreference resolution : from local classification to global ranking in this paper , we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems . we propose a set of partition - based features to learn a ranking model for distinguishing good and bad partitions . our approach compares favorably to two state - of - the - art coreference systems when evaluated on three standard coreference data sets .
RANK 2
learning the scope of hedge cues in biomedical texts identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information . in this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts . the system is based on a similar system that finds the scope of negation cues . we show that the same scope finding approach can be applied to both negation and hedging . to investigate the robustness of the approach , the system is tested on the three subcorpora of the bioscope corpus that represent different text types .
RANK 3
extraction of definitions using grammar - enhanced machine learning in this paper we compare different approaches to extract definitions of four types using a combination of a rule - based grammar and machine learning . we collected a dutch text corpus containing 549 definitions and applied a grammar on it . machine learning was then applied to improve the results obtained with the grammar . two machine learning experiments were carried out . in the first experiment , a standard classifier and a classifier designed specifically to deal with imbalanced datasets are compared . the algorithm designed specifically to deal with imbalanced datasets for most types outperforms the standard classifier . in the second experiment we show that classification results improve when information on definition structure is included .
TOP 20
RANK = 1; score = 0.2564102564102564; correct = False; id = 38623c0273a0b2cec32fc797aaa99b8aef3c4472
machine learning for coreference resolution : from local classification to global ranking in this paper , we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems . we propose a set of partition - based features to learn a ranking model for distinguishing good and bad partitions . our approach compares favorably to two state - of - the - art coreference systems when evaluated on three standard coreference data sets .
RANK = 2; score = 0.25; correct = False; id = 3602a097a32e3bc8a01316cb813626d04afca58c
learning the scope of hedge cues in biomedical texts identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information . in this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts . the system is based on a similar system that finds the scope of negation cues . we show that the same scope finding approach can be applied to both negation and hedging . to investigate the robustness of the approach , the system is tested on the three subcorpora of the bioscope corpus that represent different text types .
RANK = 3; score = 0.25; correct = False; id = 3a92479189cf5a5183e0ebbc02d464941093d18e
extraction of definitions using grammar - enhanced machine learning in this paper we compare different approaches to extract definitions of four types using a combination of a rule - based grammar and machine learning . we collected a dutch text corpus containing 549 definitions and applied a grammar on it . machine learning was then applied to improve the results obtained with the grammar . two machine learning experiments were carried out . in the first experiment , a standard classifier and a classifier designed specifically to deal with imbalanced datasets are compared . the algorithm designed specifically to deal with imbalanced datasets for most types outperforms the standard classifier . in the second experiment we show that classification results improve when information on definition structure is included .
RANK = 4; score = 0.25; correct = False; id = 4a4968f5b284119fc086b935ed0936184f17aa14
a novel classifier based on quantum computation in this article , we propose a novel classifier based on quantum computation theory . different from existing methods , we consider the classification as an evolutionary process of a physical system and build the classifier by using the basic quantum mechanics equation . the performance of the experiments on two datasets indicates feasibility and potentiality of the quantum classifier .
RANK = 5; score = 0.24285714285714285; correct = False; id = 96863ce64bf65a0f6e07271fc180850992a6f0bd
investigations on event - based summarization we investigate independent and relevant event - based extractive mutli - document summarization approaches . in this paper , events are defined as event terms and associated event elements . with independent approach , we identify important contents by frequency of events . with relevant approach , we identify important contents by pagerank algorithm on the event map constructed from documents . experimental results are encouraging .
RANK = 6; score = 0.24175824175824176; correct = False; id = a9b741b6a6159866d60b7937ef245dd089db036c
coverage embedding models for neural machine translation in this paper , we enhance the attention - based neural machine translation ( nmt ) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in nmt . for each source word , our model starts with a full coverage embedding vector to track the coverage status , and then keeps updating it with neural networks as the translation goes . experiments on the large - scale chinese - to - english task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system .
RANK = 7; score = 0.24096385542168675; correct = False; id = 523cd901ebb80af926b10d17bbe6e9f0abd77a48
machine learning with semantic - based distances between sentences for textual entailment this paper describes our experiments on textual entailment in the context of the third pascal recognising textual entailment ( rte-3 ) evaluation challenge . our system uses a machine learning approach with support vector machines and adaboost to deal with the rte challenge . we perform a lexical , syntactic , and semantic analysis of the entailment pairs . from this information we compute a set of semanticbased distances between sentences . the results look promising specially for the qa entailment task .
RANK = 8; score = 0.24050632911392406; correct = False; id = 4ad3c39799a20f6089d47d4429bba145edc2b468
word alignment of english - chinese bilingual corpus based on chucks in this paper , a method for the word alignment of english - chinese corpus based on chunks is proposed . the chunks of english sentences are identified firstly . then the chunk boundaries of chinese sentences are predicted by the translations of english chunks and heuristic information . the ambiguities of chinese chunk boundaries are resolved by the coterminous words in english chunks . with the chunk aligned bilingual corpus , a translation relation probability is proposed to align words . finally , we evaluate our system by real corpus and present the experiment results .
RANK = 9; score = 0.234375; correct = False; id = effd64371cac50e05cdf0d65a6b5286b2ead0647
transliteration experiments on chinese and arabic we report the results of our transliteration experiments with language - specific adaptations in the context of two language pairs : english to chinese , and arabic to english . in particular , we investigate a syllable - based pinyin intermediate representation for chinese , and a letter mapping for arabic .
RANK = 10; score = 0.23376623376623376; correct = False; id = 28da04e1253641e14d3c721f4f80645d2c7b8b65
learning translations of named - entity phrases from parallel corpora we develop a new approach to learning phrase translations from parallel corpora , and show that it performs with very high coverage and accuracy in choosing french translations of english named - entity phrases in a test corpus of software manuals . analysis of a subset of our results suggests that the method should also perform well on more general phrase translation tasks .
RANK = 11; score = 0.23170731707317074; correct = False; id = 2d1178e50fe4b814d8b138e6b07bc3f27edd6ff1
experimental results on the native language identification shared task we present a system for automatically identifying the native language of a writer . we experiment with a large set of features and train them on a corpus of 9,900 essays written in english by speakers of 11 different languages . our system achieved an accuracy of 43 % on the test data , improved to 63 % with improved feature normalization . in this paper , we present the features used in our system , describe our experiments and provide an analysis of our results .
RANK = 12; score = 0.23157894736842105; correct = False; id = a3e39e183cf46a49320be761242c42e02e530e80
co - training for cross - lingual sentiment classification the lack of chinese sentiment corpora limits the research progress on chinese sentiment classification . however , there are many freely available english sentiment corpora on the web . this paper focuses on the problem of cross - lingual sentiment classification , which leverages an available english corpus for chinese sentiment classification by using the english corpus as training data . machine translation services are used for eliminating the language gap between the training set and test set , and english features and chinese features are considered as two independent views of the classification problem . we propose a cotraining approach to making use of unlabeled chinese data . experimental results show the effectiveness of the proposed approach , which can outperform the standard inductive classifiers and the transductive classifiers .
RANK = 13; score = 0.23076923076923078; correct = False; id = b4c4c48148f2df5e5ec99d4b5bcf4b2ba52a89dc
identification and resolution of chinese zero pronouns : a machine learning approach in this paper , we present a machine learning approach to the identification and resolution of chinese anaphoric zero pronouns . we perform both identification and resolution automatically , with two sets of easily computable features . experimental results show that our proposed learning approach achieves anaphoric zero pronoun resolution accuracy comparable to a previous state - ofthe - art , heuristic rule - based approach . to our knowledge , our work is the first to perform both identification and resolution of chinese anaphoric zero pronouns using a machine learning approach .
RANK = 14; score = 0.22826086956521738; correct = False; id = 45dd93d3169ef00786064016fbbdfa624175382b
a corpus - based quantitative study of nominalizations across chinese and british media english this paper reports on a corpus - based quantitative study of the use of nominalizations across china english and british english in two comparable media corpora . in contrast to previous corpus - based studies of nominalizations , we start by using a syntactic approach and proceed with some methodological innovations incorporating large lexical databases and syntactically annotated corpora . the data show that there are significant differences in the use of nominalizations across these two english varieties . it is hoped that this research will offer useful insights on variations in nominalization across different english varieties and also on the understanding of the two english varieties in question .
RANK = 15; score = 0.22784810126582278; correct = False; id = 6d375f8b45d8e5e961875bf74a89f32394759b89
exploiting semantic role labeling , wordnet and wikipedia for coreference resolution in this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources . these features represent knowledge mined from wordnet and wikipedia , as well as information about semantic role labels . we show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns .
RANK = 16; score = 0.22772277227722773; correct = False; id = 0a9c1946ca521804a20b898953c212bad058086d
ualacant : using online machine translation for cross - lingual textual entailment this paper describes a new method for crosslingual textual entailment ( clte ) detection based on machine translation ( mt ) . we use sub - segment translations from different mt systems available online as a source of crosslingual knowledge . in this work we describe and evaluate different features derived from these sub - segment translations , which are used by a support vector machine classifier to detect cltes . we presented this system to the semeval 2012 task 8 obtaining an accuracy up to 59.8 % on the english – spanish test set , the second best performing approach in the contest .
RANK = 17; score = 0.2261904761904762; correct = False; id = 4607017f6b07d394418220cea657d4c1cb4a24eb
distributional similarity vs. pu learning for entity set expansion distributional similarity is a classic technique for entity set expansion , where the system is given a set of seed entities of a particular class , and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds . this paper shows that a machine learning model called positive and unlabeled learning ( pu learning ) can model the set expansion problem better . based on the test results of 10 corpora , we show that a pu learning technique outperformed distributional similarity significantly .
RANK = 18; score = 0.2261904761904762; correct = False; id = 4ac80f8a837a7e0c15ea19e42310f1423edaebcd
advancements in reordering models for statistical machine translation in this paper , we propose a novel reordering model based on sequence labeling techniques . our model converts the reordering problem into a sequence labeling problem , i.e. a tagging task . results on five chinese - english nist tasks show that our model improves the baseline system by 1.32 bleu and 1.53 ter on average . results of comparative study with other seven widely used reordering models will also be reported .
RANK = 19; score = 0.22448979591836735; correct = False; id = cadaa934e33b66ac135a4418438381b7062ad960
chunking with max - margin markov networks in this paper , we apply max - margin markov networks ( m3ns ) to english base phrases chunking , which is a large margin approach combining both the advantages of graphical models(such as conditional random fields , crfs ) and kernel - based approaches ( such as support vector machines , svms ) to solve the problems of multi - label multi - class supervised classification . to show the efficiency of m3ns , we compare it with crfs and other relative systems on the data set of conll-2000 comprehensively . the experiment results show that m3ns achieves state - of - the - art performance with strong generalization ability , which is better than crfs .
RANK = 20; score = 0.22448979591836735; correct = False; id = 3b09a9e1523cee6e862912ae86f1e28ea4892319
domain and dialect adaptation for machine translation into egyptian arabic in this paper , we present a statistical machine translation system for english to dialectal arabic ( da ) , using modern standard arabic ( msa ) as a pivot . we create a core system to translate from english to msa using a large bilingual parallel corpus . then , we design two separate pathways for translation from msa into da : a two - step domain and dialect adaptation system and a one - step simultaneous domain and dialect adaptation system . both variants of the adaptation systems are trained on a 100k sentence tri - parallel corpus of english , msa , and egyptian arabic generated by a rule - based transformation . we test our systems on a held - out egyptian arabic test set from the 100k sentence corpus and we achieve our best performance using the two - step domain and dialect adaptation system with a bleu score of 42.9 .

RANKING 899
QUERY
a hybrid system for chinese grammatical error diagnosis and correction this paper introduces the dm nlp team ’s system for nlptea 2018 shared task of chinese grammatical error diagnosis ( cged ) , which can be used to detect and correct grammatical errors in texts written by chinese as a foreign language ( cfl ) learners . this task aims at not only detecting four types of grammatical errors including redundant words ( r ) , missing words ( m ) , bad word selection ( s ) and disordered words ( w ) , but also recommending corrections for errors of m and s types . we proposed a hybrid system including four models for this task with two stages : the detection stage and the correction stage . in the detection stage , we first used a bilstm - crf model to tag potential errors by sequence labeling , along with some handcraft features . then we designed three grammatical error correction ( gec ) models to generate corrections , which could help to tune the detection result . in the correction stage , candidates were generated by the three gec models and then merged to output the final corrections for m and s types . our system reached the highest precision in the correction subtask , which was the most challenging part of this shared task , and got top 3 on f1 scores for position detection of errors .
First cited at 4
TOP CITED PAPERS
RANK 4
chinese grammatical error diagnosis system based on hybrid model this paper describes our system in the chinese grammatical error diagnosis ( cged ) task for learning chinese as a foreign language ( cfl ) . our work adopts a hybrid model by integrating rulebased method and n - gram statistical method to detect chinese grammatical errors , identify the error type and point out the position of error in the input sentences . tri - gram is applied to disorder mistake . and the rest of mistakes are solved by the conservation rules sets . empirical evaluation results demonstrate the utility of our cged system .
RANK 395
correcting esl errors using phrasal smt techniques this paper presents a pilot study of the use of phrasal statistical machine translation ( smt ) techniques to identify and correct writing errors made by learners of english as a second language ( esl ) . using examples of mass noun errors found in the chinese learner error corpus ( clec ) to guide creation of an engineered training set , we show that application of the smt paradigm can capture errors not well addressed by widely - used proofing tools designed for native speakers . our system was able to correct 61.81 % of mistakes in a set of naturallyoccurring examples of mass noun errors found on the world wide web , suggesting that efforts to collect alignable corpora of preand post - editing esl writing samples offer can enable the development of smt - based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of esl learners .
RANK 857
effective approaches to attention - based neural machine translation an attentional mechanism has lately been used to improve neural machine translation ( nmt ) by selectively focusing on parts of the source sentence during translation . however , there has been little work exploring useful architectures for attention - based nmt . this paper examines two simple and effective classes of attentional mechanism : a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . we demonstrate the effectiveness of both approaches on the wmt translation tasks between english and german in both directions . with local attention , we achieve a significant gain of 5.0 bleu points over non - attentional systems that already incorporate known techniques such as dropout . our ensemble model using different attention architectures yields a new state - of - the - art result in the wmt’15 english to german translation task with 25.9 bleu points , an improvement of 1.0 bleu points over the existing best system backed by nmt and an n - gram reranker.1
TOP UNCITED PAPERS
RANK 1
chinese grammatical error diagnosis by conditional random fields this paper reports how to build a chinese grammatical error diagnosis system based on the conditional random fields ( crf ) . the system can find four types of grammatical errors in learners’ essays . the four types or errors are redundant words , missing words , bad word selection , and disorder words . our system presents the best false positive rate in 2015 nlp - tea-2 cged shared task , and also the best precision rate in three diagnosis levels .
RANK 2
condition random fields - based grammatical error detection for chinese as second language the foreign learners are not easy to learn chinese as a second language . because there are many special rules different from other languages in chinese . when the people learn chinese as a foreign language usually make some grammatical errors , such as missing , redundant , selection and disorder . in this paper , we proposed the conditional random fields ( crfs ) to detect the grammatical errors . the features based on statistical word and part - ofspeech ( pos ) pattern were adopted here . the relationships between words by part - of - speech are helpful for chinese grammatical error detection . finally , we according to crf determined which error types in sentences . according to the observation of experimental results , the performance of the proposed model is acceptable in precision and recall rates .
RANK 3
gwu - hasp-2015$@$qalb-2015 shared task : priming spelling candidates with probability in this paper , we describe our system hasp-2015 ( hybrid arabic spelling and punctuation corrector ) in which we introduce significant improvements over our previous version hasp-2014 and with which we participated in the qalb2015 second shared task on arabic error correction . our system utilizes probabilistic information on errors and their possible corrections in the training data and combine that with an open - source reference dictionary ( or word list ) for detecting errors and generating and filtering candidates . we enhance our system further by allowing it to generate candidates for common semantic and grammatical errors . eventually , an n - gram language model is used for selecting best candidates . we use a crf ( conditional random fields ) classifier for correcting punctuation errors in a two - pass process where first the system learns punctuation placement , and then it learns to identify punctuation types .
TOP 20
RANK = 1; score = 0.2746478873239437; correct = False; id = 108698733b27c518c6d8303ed57ba5cba08c5d9b
chinese grammatical error diagnosis by conditional random fields this paper reports how to build a chinese grammatical error diagnosis system based on the conditional random fields ( crf ) . the system can find four types of grammatical errors in learners’ essays . the four types or errors are redundant words , missing words , bad word selection , and disorder words . our system presents the best false positive rate in 2015 nlp - tea-2 cged shared task , and also the best precision rate in three diagnosis levels .
RANK = 2; score = 0.25595238095238093; correct = False; id = f61173d25d44ad4abb1a8c54a5e04637c180b4f8
condition random fields - based grammatical error detection for chinese as second language the foreign learners are not easy to learn chinese as a second language . because there are many special rules different from other languages in chinese . when the people learn chinese as a foreign language usually make some grammatical errors , such as missing , redundant , selection and disorder . in this paper , we proposed the conditional random fields ( crfs ) to detect the grammatical errors . the features based on statistical word and part - ofspeech ( pos ) pattern were adopted here . the relationships between words by part - of - speech are helpful for chinese grammatical error detection . finally , we according to crf determined which error types in sentences . according to the observation of experimental results , the performance of the proposed model is acceptable in precision and recall rates .
RANK = 3; score = 0.22404371584699453; correct = False; id = 1989781a343724e91f012c1e0d7999ff2b95a079
gwu - hasp-2015$@$qalb-2015 shared task : priming spelling candidates with probability in this paper , we describe our system hasp-2015 ( hybrid arabic spelling and punctuation corrector ) in which we introduce significant improvements over our previous version hasp-2014 and with which we participated in the qalb2015 second shared task on arabic error correction . our system utilizes probabilistic information on errors and their possible corrections in the training data and combine that with an open - source reference dictionary ( or word list ) for detecting errors and generating and filtering candidates . we enhance our system further by allowing it to generate candidates for common semantic and grammatical errors . eventually , an n - gram language model is used for selecting best candidates . we use a crf ( conditional random fields ) classifier for correcting punctuation errors in a two - pass process where first the system learns punctuation placement , and then it learns to identify punctuation types .
RANK = 4; score = 0.2129032258064516; correct = True; id = 463edc002d3b8c6e5cdb0ffdfe6bb16b366a81d4
chinese grammatical error diagnosis system based on hybrid model this paper describes our system in the chinese grammatical error diagnosis ( cged ) task for learning chinese as a foreign language ( cfl ) . our work adopts a hybrid model by integrating rulebased method and n - gram statistical method to detect chinese grammatical errors , identify the error type and point out the position of error in the input sentences . tri - gram is applied to disorder mistake . and the rest of mistakes are solved by the conservation rules sets . empirical evaluation results demonstrate the utility of our cged system .
RANK = 5; score = 0.21052631578947367; correct = False; id = f49fb8b79d9571be36ef74d415ab46e9a8fe4c12
gwu - hasp : hybrid arabic spelling and punctuation corrector in this paper , we describe our hybrid arabic spelling and punctuation corrector ( hasp ) . hasp was one of the systems participating in the qalb-2014 shared task on arabic error correction . the system uses a crf ( conditional random fields ) classifier for correcting punctuation errors , an open - source dictionary ( or word list ) for detecting errors and generating and filtering candidates , an n - gram language model for selecting the best candidates , and a set of deterministic rules for text normalization ( such as removing diacritics and kashida and converting hindi numbers into arabic numerals ) . we also experiment with word alignment for spelling correction at the character level and report some preliminary results .
RANK = 6; score = 0.20670391061452514; correct = False; id = 4a31ca8dd87fa2e8d3806d082ae211b0358510bd
techlimed$@$qalb - shared task 2015 : a hybrid arabic error correction system this paper reports on the participation of techlimed in the second shared task on automatic arabic error correction organized by the arabic natural language processing workshop . this year 's competition includes two tracks , and , in addition to errors produced by native speakers ( l1 ) , also includes correction of texts written by learners of arabic as a foreign language ( l2 ) . techlimed participated in the l1 track . for our participation in the l1 evaluation task , we developed two systems . the first one is based on the spellchecker hunspell with specific dictionaries . the second one is a hybrid system based on rules , morphology analysis and statistical machine translation . our results on the test set show that the hybrid system outperforms the lexicon driven approach with a precision of 71.2 % , a recall of 64.94 % and an f - measure of 67.93 % .
RANK = 7; score = 0.20606060606060606; correct = False; id = 48bd7b1bec3df233c2cd41845c9090ef30ab4ccd
sentence - level grammatical error identification as sequence - to - sequence correction we demonstrate that an attention - based encoder - decoder model can be used for sentence - level grammatical error identification for the automated evaluation of scientific writing ( aesw ) shared task 2016 . the attention - based encoder - decoder models can be used for the generation of corrections , in addition to error identification , which is of interest for certain end - user applications . we show that a character - based encoder - decoder model is particularly effective , outperforming other results on the aesw shared task on its own , and showing gains over a word - based counterpart . our final model — a combination of three character - based encoder - decoder models , one word - based encoder - decoder model , and a sentence - level cnn — is the highest performing system on the aesw 2016 binary prediction shared task .
RANK = 8; score = 0.20574162679425836; correct = False; id = 8b431c00444e2a88dcede60f8867f6d38b7baca3
an automatic chinese document revision system using bit and character mask approach the errors in chinese document are mainly caused in two stages input and editing . there are homonyms or homophones selection error , ambiguous pronunciation error , word segmentation error , similar shape character error , editing operation error and so on . in order to increase the quality of chinese text , the conventional chinese document revision system used the similar characters set and language model with some statistical date . nevertheless , there are the following problems : ( 1 ) the perfect similar character set is difficult to make ( 2 ) due to the copyright problem the large and balanced chinese corpus is very difficult to be obtained ( 3 ) the above editing errors can not be solved simultaneously ( 4 ) the average success revision rate is not over 75 % . in this paper we study the chinese features and phonetic - input - to - character conversion system for chinese . it is found that the chinese phonetic information and the related conversion algorithm are much help to detect and revise the input errors in chinese document . as to the editing errors , a special code structure of chinese pronunciation which has only one bit difference among similar pronunciations is proposed in addition , the bits and characters mask technology is also proposed respectively . the experimental result of the proposed system show that the average success revision rate of the proposed system is close to 87 % .
RANK = 9; score = 0.20238095238095238; correct = False; id = b58d9e1fc4b783ef8db0b7231d95e9a9943ba58a
naist at the hoo 2012 shared task this paper describes the nara institute of science and technology ( naist ) error correction system in the helping our own ( hoo ) 2012 shared task . our system targets preposition and determiner errors with spelling correction as a pre - processing step . the result shows that spelling correction improves the detection , correction , and recognition fscores for preposition errors . with regard to preposition error correction , f - scores were not improved when using the training set with correction of all but preposition errors . as for determiner error correction , there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an np were removed . our system ranked third in preposition and fourth in determiner error corrections .
RANK = 10; score = 0.2011173184357542; correct = False; id = c35a590b0e830e56ab6253043fb4ac011faa21a2
ku leuven at hoo-2012 : a hybrid approach to detection and correction of determiner and preposition errors in non - native english text in this paper we describe the technical implementation of our system that participated in the helping our own 2012 shared task ( hoo-2012 ) . the system employs a number of preprocessing steps and machine learning classifiers for correction of determiner and preposition errors in non - native english texts . we use maximum entropy classifiers trained on the provided hoo-2012 development data and a large high - quality english text collection . the system proposes a number of highlyprobable corrections , which are evaluated by a language model and compared with the original text . a number of deterministic rules are used to increase the precision and recall of the system . our system is ranked among the three best performing hoo-2012 systems with a precision of 31.15 % , recall of 22.08 % and f1score of 25.84 % for correction of determiner and preposition errors combined .
RANK = 11; score = 0.19879518072289157; correct = False; id = ff5edc4388725c79a4fed7ddbb00a01fe75ec60a
arib$@$qalb-2015 shared task : a hybrid cascade model for arabic spelling error detection and correction in this paper we present the arib system for arabic spelling error detection and correction as part of the second shared task on automatic arabic error correction . our system contains many components that address various types of spelling error and applies a combination of approaches including rule based , statistical based , and lexicon based in a cascade fashion . we also employed two core models , namely a probabilistic - based model and a distance - based model . our results on the development and test set indicate that using the correction components in cascaded way yields the best results . the overall recall of our system is 0.51 , with a precision of 0.67 and an f1 score of 0.58 .
RANK = 12; score = 0.19786096256684493; correct = False; id = 00a0a3e3afbb1839532bf0f6366098b7a2140f1b
a comparative study of parameter estimation methods for statistical natural language processing this paper presents a comparative study of five parameter estimation algorithms on four nlp tasks . three of the five algorithms are well - known in the computational linguistics community : maximum entropy ( me ) estimation with l2 regularization , the averaged perceptron ( ap ) , and boosting . we also investigate me estimation with l1 regularization using a novel optimization algorithm , and blasso , which is a version of boosting with lasso ( l1 ) regularization . we first investigate all of our estimators on two re - ranking tasks : a parse selection task and a language model ( lm ) adaptation task . then we apply the best of these estimators to two additional tasks involving conditional sequence models : a conditional markov model ( cmm ) for part of speech tagging and a conditional random field ( crf ) for chinese word segmentation . our experiments show that across tasks , three of the estimators — me estimation with l1 or l2 regularization , and ap — are in a near statistical tie for first place .
RANK = 13; score = 0.1956521739130435; correct = False; id = e0d1429717d6be45b64b66de28fd918d8cbe0355
a hybrid markov / semi - markov conditional random field for sequence segmentation markov order-1 conditional random fields ( crfs ) and semi - markov crfs are two popular models for sequence segmentation and labeling . both models have advantages in terms of the type of features they most naturally represent . we propose a hybrid model that is capable of representing both types of features , and describe efficient algorithms for its training and inference . we demonstrate that our hybrid model achieves error reductions of 18 % and 25 % over a standard order-1 crf and a semi - markov crf ( resp . ) on the task of chinese word segmentation . we also propose the use of a powerful feature for the semi - markov crf : the log conditional odds that a given token sequence constitutes a chunk according to a generative model , which reduces error by an additional 13 % . our best system achieves 96.8 % f - measure , the highest reported score on this test set .
RANK = 14; score = 0.195; correct = False; id = e9ec8a2ed6e0431211f1a3f32d3176a420dc1d99
lcc - wsd : system description for english coarse grained all words task at semeval 2007 this document describes the word sense disambiguation system used by language computer corporation at english coarse grained all word task at semeval 2007 . the system is based on two supervised machine learning algorithms : maximum entropy and support vector machines . these algorithms were trained on a corpus created from semcor , senseval 2 and 3 all words and lexical sample corpora and open mind word expert 1.0 corpus . we used topical , syntactic and semantic features . some semantic features were created using wordnet glosses with semantic relations tagged manually and automatically as part of extended wordnet project . we also tried to create more training instances from the disambiguated wordnet glosses found in xwn project ( xwn , 2003 ) . for words for which we could not build a sense classifier , we used first sense in wordnet as a back - off strategy in order to have coverage of 100 % . the precision and recall of the overall system is 81.446 % placing it in the top 5 systems .
RANK = 15; score = 0.19428571428571428; correct = False; id = 9152a1ca197d0e5862fde6352cdc0ecf0540c41b
codex : combining an svm classifier and character n - gram language models for sentiment analysis on twitter text this paper briefly reports our system for the semeval-2013 task 2 : sentiment analysis in twitter . we first used an svm classifier with a wide range of features , including bag of word features ( unigram , bigram ) , pos features , stylistic features , readability scores and other statistics of the tweet being analyzed , domain names , abbreviations , emoticons in the twitter text . then we investigated the effectiveness of these features . we also used character n - gram language models to address the problem of high lexical variation in twitter text and combined the two approaches to obtain the final results . our system is robust and achieves good performance on the twitter test data as well as the sms test data .
RANK = 16; score = 0.19148936170212766; correct = False; id = 360bd083d8a286b30a9045a8ba8ab6bb6469b914
automatic generation of related work sections in scientific papers : an optimization approach in this paper , we investigate a challenging task of automatic related work generation . given multiple reference papers as input , the task aims to generate a related work section for a target paper . the generated related work section can be used as a draft for the author to complete his or her final related work section . we propose our automatic related work generation system called arwg to address this task . it first exploits a plsa model to split the sentence set of the given papers into different topic - biased parts , and then applies regression models to learn the importance of the sentences . at last it employs an optimization framework to generate the related work section . our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed arwg system can generate related work sections with better quality . a user study is also performed to show arwg can achieve an improvement over generic multi - document summarization baselines .
RANK = 17; score = 0.1910828025477707; correct = False; id = a1cd86fd0cddefffa62330827600866936a776f4
ntou chinese grammar checker for cged shared task grammatical error diagnosis is an essential part in a language - learning tutoring system . participating in the second chinese grammar error detection task , we proposed a new system which measures the likelihood of sentences generated by deleting , inserting , or exchanging characters or words . two sentence likelihood functions were proposed based on frequencies of spaceremoved version of google n - grams . the best system achieved a precision of 23.4 % and a recall of 36.4 % in the identification level .
RANK = 18; score = 0.1896551724137931; correct = False; id = ce7b79c7ac49247efa3883ecb6411fb09284f0fd
error profiling : toward a model of english acquisition for deaf learners in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system , which has been designed for deaf users of american sign language . we explore the correlation between a corpus of error - tagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population . since errors made at lower levels ( and not at higher levels ) presumably represent constructions acquired before those on which errors are found only at higher levels , this should provide insight into the order of acquisition of english grammatical forms .
RANK = 19; score = 0.1896551724137931; correct = False; id = a063601d35146fd5f4fa92eeaf3aac01243fc852
description of the ncu chinese word segmentation and named entity recognition system for sighan bakeoff 2006 asian languages are far from most western - style in their non - separate word sequence especially chinese . the preliminary step of asian - like language processing is to find the word boundaries between words . in this paper , we present a general purpose model for both chinese word segmentation and named entity recognition . this model was built on the word sequence classification with probability model , i.e. , conditional random fields ( crf ) . we used a simple feature set for crf which achieves satisfactory classification result on the two tasks . our model achieved 91.00 in f rate in upuctreebank data , and 78.71 for ner task .
RANK = 20; score = 0.1895424836601307; correct = False; id = e4bac7805854dd180a883c9cae01bbff27f4092e
qcri$@$qalb-2015 shared task : correction of arabic text for native and non - native speakers ' errors this paper describes the error correction model that we used for the qalb2015 automatic correction of arabic text shared task . we employed a case - specific correction approach that handles specific error types such as dialectal word substitution and word splits and merges with the aid of a language model . we also applied corrections that are specific to second language learners that handle erroneous preposition selection , definiteness , and gender - number agreement .

RANKING 16
QUERY
solving feature sparseness in text classification using core - periphery decomposition feature sparseness is a problem common to cross - domain and short - text classification tasks . to overcome this feature sparseness problem , we propose a novel method based on graph decomposition to find candidate features for expanding feature vectors . specifically , we first create a feature - relatedness graph , which is subsequently decomposed into core - periphery ( cp ) pairs and use the peripheries as the expansion candidates of the cores . we expand both training and test instances using the computed related features and use them to train a text classifier . we observe that prioritising features that are common to both training and test instances as cores during the cp decomposition to further improve the accuracy of text classification . we evaluate the proposed cp - decomposition - based feature expansion method on benchmark datasets for cross - domain sentiment classification and short - text classification . our experimental results show that the proposed method consistently outperforms all baselines on short - text classification tasks , and perform competitively with pivot - based cross - domain sentiment classification methods .
First cited at 39
TOP CITED PAPERS
RANK 39
unsupervised cross - domain word representation learning meaning of a word varies from one domain to another . despite this important domain dependence in word semantics , existing word representation learning methods are bound to a single domain . given a pair of source - target domains , we propose an unsupervised method for learning domain - specific word representations that accurately capture the domainspecific aspects of word semantics . first , we select a subset of frequent words that occur in both domains as pivots . next , we optimize an objective function that enforces two constraints : ( a ) for both source and target domain documents , pivots that appear in a document must accurately predict the co - occurring non - pivots , and ( b ) word representations learnt for pivots must be similar in the two domains . moreover , we propose a method to perform domain adaptation using the learnt word representations . our proposed method significantly outperforms competitive baselines including the state - of - theart domain - insensitive word representations , and reports best sentiment classification accuracies for all domain - pairs in a benchmark dataset .
RANK 194
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK 348
a re - examination of query expansion using lexical resources query expansion is an effective technique to improve the performance of information retrieval systems . although hand - crafted lexical resources , such as wordnet , could provide more reliable related terms , previous studies showed that query expansion using only wordnet leads to very limited performance improvement . one of the main challenges is how to assign appropriate weights to expanded terms . in this paper , we re - examine this problem using recently proposed axiomatic approaches and find that , with appropriate term weighting strategy , we are able to exploit the information from lexical resources to significantly improve the retrieval performance . our empirical results on six trec collections show that query expansion using only hand - crafted lexical resources leads to significant performance improvement . the performance can be further improved if the proposed method is combined with query expansion using co - occurrence - based resources .
TOP UNCITED PAPERS
RANK 1
learning latent word representations for domain adaptation using supervised word clustering domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain . in this paper , we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features , which are generalizable across domains while informative to the prediction task . specifically , we propose a hierarchical multinomial naive bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains , and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation . we train this latent graphical model using a simple expectation - maximization ( em ) algorithm . we empirically evaluate the proposed method with both cross - domain document categorization tasks on reuters-21578 dataset and cross - domain sentiment classification tasks on amazon product review dataset . the experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods .
RANK 2
using multiple sources to construct a sentiment sensitive thesaurus for cross - domain sentiment classification we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains , designated as the source domains . we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains . the created thesaurus is then used to expand feature vectors to train a binary classifier . unlike previous cross - domain sentiment classification methods , our method can efficiently learn from multiple source domains . our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross - domain sentiment classification methods on a benchmark dataset containing amazon user reviews for different types of products .
RANK 3
lifelong learning for sentiment classification this paper proposes a novel lifelong learning ( ll ) approach to sentiment classification . ll mimics the human continuous learning process , i.e. , retaining the knowledge learned from past tasks and use it to help future learning . in this paper , we first discuss ll in general and then ll for sentiment classification in particular . the proposed ll approach adopts a bayesian optimization framework based on stochastic gradient descent . our experimental results show that the proposed method outperforms baseline methods significantly , which demonstrates that lifelong learning is a promising research direction .
TOP 20
RANK = 1; score = 0.2608695652173913; correct = False; id = 3b720c58d5ee838261ccec6d1bb9aa052e8f510b
learning latent word representations for domain adaptation using supervised word clustering domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain . in this paper , we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features , which are generalizable across domains while informative to the prediction task . specifically , we propose a hierarchical multinomial naive bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains , and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation . we train this latent graphical model using a simple expectation - maximization ( em ) algorithm . we empirically evaluate the proposed method with both cross - domain document categorization tasks on reuters-21578 dataset and cross - domain sentiment classification tasks on amazon product review dataset . the experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods .
RANK = 2; score = 0.24324324324324326; correct = False; id = 8639e396d1cdf07ef206dc1abff07b327f6cdb58
using multiple sources to construct a sentiment sensitive thesaurus for cross - domain sentiment classification we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains , designated as the source domains . we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains . the created thesaurus is then used to expand feature vectors to train a binary classifier . unlike previous cross - domain sentiment classification methods , our method can efficiently learn from multiple source domains . our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross - domain sentiment classification methods on a benchmark dataset containing amazon user reviews for different types of products .
RANK = 3; score = 0.23846153846153847; correct = False; id = 70a04587a53f3dd1923ec6bcb0f31bed033563af
lifelong learning for sentiment classification this paper proposes a novel lifelong learning ( ll ) approach to sentiment classification . ll mimics the human continuous learning process , i.e. , retaining the knowledge learned from past tasks and use it to help future learning . in this paper , we first discuss ll in general and then ll for sentiment classification in particular . the proposed ll approach adopts a bayesian optimization framework based on stochastic gradient descent . our experimental results show that the proposed method outperforms baseline methods significantly , which demonstrates that lifelong learning is a promising research direction .
RANK = 4; score = 0.23841059602649006; correct = False; id = a27e243d2ef62644e7a2a1fa51878fe7dbca4479
learning term embeddings for taxonomic relation identification using dynamic weighting neural network taxonomic relation identification aims to recognize the ‘ is - a’ relation between two terms . previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches , but the accuracy of these approaches is far from satisfactory . in this paper , we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings . for this purpose , we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms , but also the contextual information between them . we then apply such embeddings as features to identify taxonomic relations using a supervised method . the experimental results show that our proposed approach significantly outperforms other state - of - the - art methods by 9 % to 13 % in terms of accuracy for both general and specific domain datasets .
RANK = 5; score = 0.23776223776223776; correct = False; id = ddbdedf94222232f41cf1c0200d973f000568fe7
improving statistical machine translation accuracy using bilingual lexicon extractionwith paraphrases statistical machine translation ( smt ) suffers from the accuracy problem that the translation pairs and their feature scores in the translation model can be inaccurate . the accuracy problem is caused by the quality of the unsupervised methods used for translation model learning . previous studies propose estimating comparable features for the translation pairs in the translation model from comparable corpora , to improve the accuracy of the translation model . comparable feature estimation is based on bilingual lexicon extraction ( ble ) technology . however , ble suffers from the data sparseness problem , which makes the comparable features inaccurate . in this paper , we propose using paraphrases to address this problem . paraphrases are used to smooth the vectors used in comparable feature estimation with ble . in this way , we improve the quality of comparable features , which can improve the accuracy of the translation model thus improve smt performance . experiments conducted on chinese - english phrase - based smt ( pbsmt ) verify the effectiveness of our proposed method .
RANK = 6; score = 0.2361111111111111; correct = False; id = c65ecfd126f8218df9d589c5e4b6eca4dfd45ad4
cross - lingual sentiment classification with bilingual document representation learning cross - lingual sentiment classification aims to adapt the sentiment resource in a resource - rich language to a resource - poor language . in this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages . different from previous research which only gets bilingual word embedding , our bilingual document representation learning model bidrl directly learns document representations . both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space . the experiments are based on the multilingual multi - domain amazon review dataset . we use english as the source language and use japanese , german and french as the target languages . the experimental results show that bidrl outperforms the state - of - the - art methods for all the target languages .
RANK = 7; score = 0.2357142857142857; correct = False; id = 63cb5e89d1e875d3f0813e270e9aabdfaaefdde1
exploiting non - local features for spoken language understanding in this paper , we exploit non - local features as an estimate of long - distance dependencies to improve performance on the statistical spoken language understanding ( slu ) problem . the statistical natural language parsers trained on text perform unreliably to encode non - local information on spoken language . an alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm . we describe a light version of the inducer in which a simple modification is efficient and successful . we evaluate our method on an slu task and show an error reduction of up to 27 % over the base local model .
RANK = 8; score = 0.23563218390804597; correct = False; id = 0955fa03294e796eefe02a098d8839f7d6031112
a semi - supervised approach to improve classification of infrequent discourse relations using feature vector extension several recent discourse parsers have employed fully - supervised machine learning approaches . these methods require human annotators to beforehand create an extensive training corpus , which is a time - consuming and costly process . on the other hand , unlabeled data is abundant and cheap to collect . in this paper , we propose a novel semi - supervised method for discourse relation classification based on the analysis of cooccurring features in unlabeled data , which is then taken into account for extending the feature vectors given to a classifier . our experimental results on the rst discourse treebank corpus and penn discourse treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro - average f - score when small training datasets are used . for instance , with training sets of c.a . 1000 labeled instances , the proposed method brings improvements in accuracy and macro - average f - score up to 50 % compared to a baseline classifier . we believe that the proposed method is a first step towards detecting low - occurrence relations , which is useful for domains with a lack of annotated data .
RANK = 9; score = 0.23484848484848486; correct = False; id = ed2e60f027b03bec109d5cd047c9178b8f8259e1
using bilingual information for cross - language document summarization cross - language document summarization is defined as the task of producing a summary in a target language ( e.g. chinese ) for a set of documents in a source language ( e.g. english ) . existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language . in this study , we propose to use the bilingual information from both the source and translated documents for this task . two summarization methods ( simfusion and corank ) are proposed to leverage the bilingual information in the graph - based ranking framework for cross - language summary extraction . experimental results on the duc2001 dataset with manually translated reference chinese summaries show the effectiveness of the proposed methods .
RANK = 10; score = 0.23484848484848486; correct = False; id = 0c2df9ca8a83a5c24aff3d9090f56de046d0fe79
graph - based unsupervised learning of word similarities using heterogeneous feature types in this work , we propose a graph - based approach to computing similarities between words in an unsupervised manner , and take advantage of heterogeneous feature types in the process . the approach is based on the creation of two separate graphs , one for words and one for features of different types ( alignmentbased , orthographic , etc . ) . the graphs are connected through edges that link nodes in the feature graph to nodes in the word graph , the edge weights representing the importance of a particular feature for a particular word . high quality graphs are learned during training , and the proposed method outperforms experimental baselines .
RANK = 11; score = 0.23387096774193547; correct = False; id = 86d41542f25134dc1271ac417c416e3f56bda683
virtual examples for text classification with support vector machines we explore how virtual examples ( artificially created examples ) improve performance of text classification with support vector machines ( svms ) . we propose techniques to create virtual examples for text classification based on the assumption that the category of a document is unchanged even if a small number of words are added or deleted . we evaluate the proposed methods by reuters-21758 test set collection . experimental results show virtual examples improve the performance of text classification with svms , especially for small training sets .
RANK = 12; score = 0.23134328358208955; correct = False; id = 14885c99671318877d1273d3b5b20d02a8f697d9
modeling human inference process for textual entailment recognition this paper aims at understanding what human think in textual entailment ( te ) recognition process and modeling their thinking process to deal with this problem . we first analyze a labeled rte-5 test set and find that the negative entailment phenomena are very effective features for te recognition . then , a method is proposed to extract this kind of phenomena from text - hypothesis pairs automatically . we evaluate the performance of using the negative entailment phenomena on both the english rte-5 dataset and chinese ntcir-9 rite dataset , and conclude the same findings .
RANK = 13; score = 0.23129251700680273; correct = False; id = cf10fc1504bd3d261192def17a66dc5b1dd9b821
automatically building a corpus for sentiment analysis on indonesian tweets the popularity of the user generated content , such as twitter , has made it a rich source for the sentiment analysis and opinion mining tasks . this paper presents our study in automatically building a training corpus for the sentiment analysis on indonesian tweets . we start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the expectation and maximization ( em ) framework . we apply our automatically built corpus to perform two tasks , namely opinion tweet extraction and tweet polarity classification using various machine learning approaches . experiment result shows that a classifier model trained on our data , which is automatically constructed using our proposed method , outperforms the baseline system in terms of opinion tweet extraction and tweet polarity classification .
RANK = 14; score = 0.23076923076923078; correct = False; id = 5bf4897a199d51d9f5048e2f53a5a9edf8bfe8b7
a novel content enriching model for microblog using news corpus in this paper , we propose a novel model for enriching the content of microblogs by exploiting external knowledge , thus improving the data sparseness problem in short text classification . we assume that microblogs share the same topics with external knowledge . we first build an optimization model to infer the topics of microblogs by employing the topic - word distribution of the external knowledge . then the content of microblogs is further enriched by relevant words from external knowledge . experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods .
RANK = 15; score = 0.23076923076923078; correct = False; id = 2d38f7aab07d4435b2110602db4138ef20da4cc0
learning sentence embeddings with auxiliary tasks for cross - domain sentiment classification in this paper , we study cross - domain sentiment classification with neural network architectures . we borrow the idea from structural correspondence learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification . we also propose to jointly learn this sentence embedding together with the sentiment classifier itself . experiment results demonstrate that our proposed joint model outperforms several state - of - theart methods on five benchmark datasets .
RANK = 16; score = 0.2302158273381295; correct = False; id = fd458e7109e8ebac2f59d399981054957078c7a4
encoding temporal information for time - aware link prediction most existing knowledge base ( kb ) embedding methods solely learn from time - unknown fact triples but neglect the temporal information in the knowledge base . in this paper , we propose a novel time - aware kb embedding approach taking advantage of the happening time of facts . specifically , we use temporal order constraints to model transformation between time - sensitive relations and enforce the embeddings to be temporally consistent and more accurate . we empirically evaluate our approach in two tasks of link prediction and triple classification . experimental results show that our method outperforms other baselines on the two tasks consistently .
RANK = 17; score = 0.22981366459627328; correct = False; id = db22ce6894d2319132dd9a465262bd65f99c71af
sexual predator detection in chats with chained classifiers this paper describes a novel approach for sexual predator detection in chat conversations based on sequences of classifiers . the proposed approach divides documents into three parts , which , we hypothesize , correspond to the different stages that a predator employs when approaching a child . local classifiers are trained for each part of the documents and their outputs are combined by a chain strategy : predictions of a local classifier are used as extra inputs for the next local classifier . additionally , we propose a ring - based strategy , in which the chaining process is iterated several times , with the goal of further improving the performance of our method . we report experimental results on the corpus used in the first international competition on sexual predator identification ( pan’12 ) . experimental results show that the proposed method outperforms a standard ( global ) classification technique for the different settings we consider ; besides the proposed method compares favorably with most methods evaluated in the pan’12 competition .
RANK = 18; score = 0.22962962962962963; correct = False; id = 3b41bc34dde4284c174c27529c3707f95d8b35e6
language model weight adaptation based on cross - entropy for statistical machine translation in this paper , we investigate the language model ( lm ) adaptation issue for statistical machine translation ( smt ) . in order to overcome the weight bias on the lm obtained from the development data , a simple but effective method is proposed to adapt the lm for diverse test datasets by employing the cross entropy of translation hypotheses as a metric to measure the similarity between different datasets . experimental results show that the cross entropy of a test dataset is closely correlated with the bias in estimating the language models and our adaptation strategy significantly outperforms a strong baseline .
RANK = 19; score = 0.22962962962962963; correct = False; id = 01e812ad00b7743e9b24aa070a24023f05710b8b
a distributed representation based query expansion approach for image captioning in this paper , we propose a novel query expansion approach for improving transferbased automatic image captioning . the core idea of our method is to translate the given visual query into a distributional semantics based form , which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image . using three image captioning benchmark datasets , we show that our approach provides more accurate results compared to the state - of - theart data - driven methods in terms of both automatic metrics and subjective evaluation .
RANK = 20; score = 0.22916666666666666; correct = False; id = 58ad5b84b17fb70b353b26f41715197ecb77e5cf
improve statistical machine translation with context - sensitive bilingual semantic embedding model we investigate how to improve bilingual embedding which has been successfully used as a feature in phrase - based statistical machine translation ( smt ) . despite bilingual embedding ’s success , the contextual information , which is of critical importance to translation quality , was ignored in previous work . to employ the contextual information , we propose a simple and memory - efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account . bilingual translation scores generated from our proposed bilingual embedding model are used as features in our smt system . experimental results show that the proposed method achieves significant improvements on large - scale chinese - english translation task .

RANKING 2630
QUERY
toward more accurate matching of contactless palmprint images under less constrained environments contactless personal identification using biometrics characteristics brings multifaceted advantages with improved hygiene , user security , and the convenience . such imaging also generates deformation - free palmprint images which can lead to higher matching accuracy as the ground truth information is better preserved as compared with those from contact - based imaging . advancement of palmprint identification technologies for new domains requires research using larger palmprint databases that are acquired from more realistic populations , under contactless , ambient , and indoor and outdoor environments . this paper presents such a new contactless palmprint database acquired from 600 different subjects , which is the largest to date and is also made available in the public domain . unlike contactless fingerprints , contactless palmprint images often illustrate pose deformations along the optical axis of the camera , which also degrades the matching accuracy . this paper also introduces a new approach for matching contactless palmprint images using more accurate deformation alignment and matching . the experimental results are validated on three publicly available contactless palmprint databases . comparative experimental results presented in this paper indicate consistently outperforming results over competing methods in the literature and validate the effectiveness of the investigated approach . these results also serve as baseline performance to advance much needed further research using the most challenging and largest database introduced from this paper .
First cited at 1
TOP CITED PAPERS
RANK 1
suspecting less and doing better : new insights on palmprint identification for faster and more accurate matching this paper introduces a generalized palmprint identification framework to unify several state - of - art 2d and 3d palmprint methods . through this framework , we argue that the methods employing one - to - one matching strategy and binary representation for feature are more effective for palmprint identification . the analysis for the first argument is based on a statistical matching model and is supported by outperforming results on several publicly available 2d palmprpint databases . these two arguments are further evaluated for 3d palmprint matching and used to introduce a new method for encoding 3d palmprint feature . the proposed 3d feature is binary and more efficiently computed . it encodes the 3d shape of palmprint to either convex or concave . the experimental results on two publicly available , from contactless and contact - base 3d palmprint database of 177 and 200 subjects , respectively , outperform the state - of - the - art methods . this paper also provides our palmprint matching algorithm(s ) in public domain , unlike the previous work in this area , which will help to further advance research efforts in this area .
RANK 2
human identification using palm - vein images this paper presents two new approaches to improve the performance of palm - vein - based identification systems presented in the literature . the proposed approach attempts to more effectively accommodate the potential deformations , rotational and translational changes by encoding the orientation preserving features and utilizing a novel region - based matching scheme . we systematically compare the previously proposed palm - vein identification approaches with our proposed ones on two different databases that are acquired with the contactless and touch - based imaging setup . we evaluate the performance improvement in both verification and recognition scenarios and analyze the influence of enrollment size on the performance . in this context , the proposed approaches are also compared for its superiority using single image enrollment on two different databases . the rigorous experimental results presented in this paper , on the databases of 100 and 250 subjects , consistently conforms the superiority of the proposed approach in both the verification and recognition scenario .
TOP UNCITED PAPERS
RANK 3
a unified framework for contactless hand verification two - dimensional ( 2-d ) hand - geometry features carry limited discriminatory information and therefore yield moderate performance when utilized for personal identification . this paper investigates a new approach to achieve performance improvement by simultaneously acquiring and combining three - dimensional ( 3-d ) and 2-d features from the human hand . the proposed approach utilizes a 3-d digitizer to simultaneously acquire intensity and range images of the presented hands of the users in a completely contact - free manner . two new representations that effectively characterize the local finger surface features are extracted from the acquired range images and are matched using the proposed matching metrics . in addition , the characterization of 3-d palm surface using surfacecode is proposed for matching a pair of 3-d palms . the proposed approach is evaluated on a database of 177 users acquired in two sessions . the experimental results suggest that the proposed 3-d hand - geometry features have significant discriminatory information to reliably authenticate individuals . our experimental results demonstrate that consolidating 3-d and 2-d hand - geometry features results in significantly improved performance that can not be achieved with the traditional 2-d hand - geometry features alone . furthermore , this paper also investigates the performance improvement that can be achieved by integrating five biometric features , i.e. , 2-d palmprint , 3-d palmprint , finger texture , along with 3-d and 2-d hand - geometry features , that are simultaneously extracted from the user 's hand presented for authentication .
RANK 4
importance of being unique from finger dorsal patterns : exploring minor finger knuckle patterns in verifying human identities automated biometrics identification using finger knuckle images has increasingly generated interest among researchers with emerging applications in human forensics and biometrics . prior efforts in the biometrics literature have only investigated the major finger knuckle patterns that are formed on the finger surface joining proximal phalanx and middle phalanx bones . this paper investigates the possible use of minor finger knuckle patterns , which are formed on the finger surface joining distal phalanx and middle phalanx bones . the minor finger knuckle patterns can either be used as independent biometric patterns or employed to improve the performance from the major finger knuckle patterns . a completely automated approach for the minor finger knuckle identification is developed with key steps for region of interest segmentation , image normalization , enhancement , and robust matching to accommodate image variations . this paper also introduces a new or first publicly available database for minor ( also major ) finger knuckle images from 503 different subjects . the efforts to develop an automated minor finger knuckle pattern matching scheme achieve promising results and illustrate its simultaneous use to significantly improve the performance over the conventional finger knuckle identification . several open questions on the stability and uniqueness of finger knuckle patterns should be addressed before knuckle pattern / image evidence can be admissible as supportive evidence in a court of law . therefore , this paper also presents a study on the stability of finger knuckle patterns from images acquired with an interval of 4 - 7 years . the experimental results and the images presented in this paper provide new insights on the finger knuckle pattern and identify the need for further work to exploit finger knuckle patterns in forensics and biometrics applications .
RANK 5
on the use of discriminative cohort score normalization for unconstrained face recognition facial imaging has been largely addressed for automatic personal identification , in a variety of different environments . however , automatic face recognition becomes very challenging whenever the acquisition conditions are unconstrained . in this paper , a picture - specific cohort normalization approach , based on polynomial regression , is proposed to enhance the robustness of face matching under challenging conditions . a careful analysis is presented to better understand the actual discriminative power of a given cohort set . in particular , it is shown that the cohort polynomial regression alone conveys some discriminative information on the matching face pair , which is just marginally worse than the raw matching score . the influence of the cohort set size in the matching accuracy is also investigated . further , tests performed on the face recognition grand challenge ver 2 database and the labeled faces in the wild database allowed to determine the relation between the quality of the cohort samples and cohort normalization performance . experimental results obtained from the lfw data set demonstrate the effectiveness of the proposed approach to improve the recognition accuracy in unconstrained face acquisition scenarios .
TOP 20
RANK = 1; score = 0.24352331606217617; correct = True; id = 78918b5a467354461c446a10c92ea5987d61658f
suspecting less and doing better : new insights on palmprint identification for faster and more accurate matching this paper introduces a generalized palmprint identification framework to unify several state - of - art 2d and 3d palmprint methods . through this framework , we argue that the methods employing one - to - one matching strategy and binary representation for feature are more effective for palmprint identification . the analysis for the first argument is based on a statistical matching model and is supported by outperforming results on several publicly available 2d palmprpint databases . these two arguments are further evaluated for 3d palmprint matching and used to introduce a new method for encoding 3d palmprint feature . the proposed 3d feature is binary and more efficiently computed . it encodes the 3d shape of palmprint to either convex or concave . the experimental results on two publicly available , from contactless and contact - base 3d palmprint database of 177 and 200 subjects , respectively , outperform the state - of - the - art methods . this paper also provides our palmprint matching algorithm(s ) in public domain , unlike the previous work in this area , which will help to further advance research efforts in this area .
RANK = 2; score = 0.21978021978021978; correct = True; id = aba108259c0473eac6f16e6c741ce7e220a272c5
human identification using palm - vein images this paper presents two new approaches to improve the performance of palm - vein - based identification systems presented in the literature . the proposed approach attempts to more effectively accommodate the potential deformations , rotational and translational changes by encoding the orientation preserving features and utilizing a novel region - based matching scheme . we systematically compare the previously proposed palm - vein identification approaches with our proposed ones on two different databases that are acquired with the contactless and touch - based imaging setup . we evaluate the performance improvement in both verification and recognition scenarios and analyze the influence of enrollment size on the performance . in this context , the proposed approaches are also compared for its superiority using single image enrollment on two different databases . the rigorous experimental results presented in this paper , on the databases of 100 and 250 subjects , consistently conforms the superiority of the proposed approach in both the verification and recognition scenario .
RANK = 3; score = 0.20095693779904306; correct = False; id = f4b3844bcbcc67672907b0767247d8513a82855c
a unified framework for contactless hand verification two - dimensional ( 2-d ) hand - geometry features carry limited discriminatory information and therefore yield moderate performance when utilized for personal identification . this paper investigates a new approach to achieve performance improvement by simultaneously acquiring and combining three - dimensional ( 3-d ) and 2-d features from the human hand . the proposed approach utilizes a 3-d digitizer to simultaneously acquire intensity and range images of the presented hands of the users in a completely contact - free manner . two new representations that effectively characterize the local finger surface features are extracted from the acquired range images and are matched using the proposed matching metrics . in addition , the characterization of 3-d palm surface using surfacecode is proposed for matching a pair of 3-d palms . the proposed approach is evaluated on a database of 177 users acquired in two sessions . the experimental results suggest that the proposed 3-d hand - geometry features have significant discriminatory information to reliably authenticate individuals . our experimental results demonstrate that consolidating 3-d and 2-d hand - geometry features results in significantly improved performance that can not be achieved with the traditional 2-d hand - geometry features alone . furthermore , this paper also investigates the performance improvement that can be achieved by integrating five biometric features , i.e. , 2-d palmprint , 3-d palmprint , finger texture , along with 3-d and 2-d hand - geometry features , that are simultaneously extracted from the user 's hand presented for authentication .
RANK = 4; score = 0.20087336244541484; correct = False; id = 5df87caeb04ca3eb904cde366c157ccd7e589e15
importance of being unique from finger dorsal patterns : exploring minor finger knuckle patterns in verifying human identities automated biometrics identification using finger knuckle images has increasingly generated interest among researchers with emerging applications in human forensics and biometrics . prior efforts in the biometrics literature have only investigated the major finger knuckle patterns that are formed on the finger surface joining proximal phalanx and middle phalanx bones . this paper investigates the possible use of minor finger knuckle patterns , which are formed on the finger surface joining distal phalanx and middle phalanx bones . the minor finger knuckle patterns can either be used as independent biometric patterns or employed to improve the performance from the major finger knuckle patterns . a completely automated approach for the minor finger knuckle identification is developed with key steps for region of interest segmentation , image normalization , enhancement , and robust matching to accommodate image variations . this paper also introduces a new or first publicly available database for minor ( also major ) finger knuckle images from 503 different subjects . the efforts to develop an automated minor finger knuckle pattern matching scheme achieve promising results and illustrate its simultaneous use to significantly improve the performance over the conventional finger knuckle identification . several open questions on the stability and uniqueness of finger knuckle patterns should be addressed before knuckle pattern / image evidence can be admissible as supportive evidence in a court of law . therefore , this paper also presents a study on the stability of finger knuckle patterns from images acquired with an interval of 4 - 7 years . the experimental results and the images presented in this paper provide new insights on the finger knuckle pattern and identify the need for further work to exploit finger knuckle patterns in forensics and biometrics applications .
RANK = 5; score = 0.2; correct = False; id = 0ee661a1b6bbfadb5a482ec643573de53a9adf5e
on the use of discriminative cohort score normalization for unconstrained face recognition facial imaging has been largely addressed for automatic personal identification , in a variety of different environments . however , automatic face recognition becomes very challenging whenever the acquisition conditions are unconstrained . in this paper , a picture - specific cohort normalization approach , based on polynomial regression , is proposed to enhance the robustness of face matching under challenging conditions . a careful analysis is presented to better understand the actual discriminative power of a given cohort set . in particular , it is shown that the cohort polynomial regression alone conveys some discriminative information on the matching face pair , which is just marginally worse than the raw matching score . the influence of the cohort set size in the matching accuracy is also investigated . further , tests performed on the face recognition grand challenge ver 2 database and the labeled faces in the wild database allowed to determine the relation between the quality of the cohort samples and cohort normalization performance . experimental results obtained from the lfw data set demonstrate the effectiveness of the proposed approach to improve the recognition accuracy in unconstrained face acquisition scenarios .
RANK = 6; score = 0.19886363636363635; correct = False; id = 616dabec42d16fac59aaceec6d05a19a9f09e061
a new framework for adaptive multimodal biometrics management this paper presents a new evolutionary approach for adaptive combination of multiple biometrics to ensure the optimal performance for the desired level of security . the adaptive combination of multiple biometrics is employed to determine the optimal fusion strategy and the corresponding fusion parameters . the score - level fusion rules are adapted to ensure the desired system performance using a hybrid particle swarm optimization model . the rigorous experimental results presented in this paper illustrate that the proposed score - level approach can achieve significantly better and stable performance over the decision - level approach . there has been very little effort in the literature to investigate the performance of an adaptive multimodal fusion algorithm on real biometric data . this paper also presents the performance of the proposed approach from the real biometric samples which further validate the contributions from this paper .
RANK = 7; score = 0.19473684210526315; correct = False; id = 68b73d5d9a6e76d6611af0c6b71483768571716b
fingerprint verification using spectral minutiae representations most fingerprint recognition systems are based on the use of a minutiae set , which is an unordered collection of minutiae locations and orientations suffering from various deformations such as translation , rotation , and scaling . the spectral minutiae representation introduced in this paper is a novel method to represent a minutiae set as a fixed - length feature vector , which is invariant to translation , and in which rotation and scaling become translations , so that they can be easily compensated for . these characteristics enable the combination of fingerprint recognition systems with template protection schemes that require a fixed - length feature vector . this paper introduces the concept of algorithms for two representation methods : the location - based spectral minutiae representation and the orientation - based spectral minutiae representation . both algorithms are evaluated using two correlation - based spectral minutiae matching algorithms . we present the performance of our algorithms on three fingerprint databases . we also show how the performance can be improved by using a fusion scheme and singular points .
RANK = 8; score = 0.19444444444444445; correct = False; id = 3e4c7a8cde7ecbd0a3ef74ed819abe7924a440b6
personal identification using minor knuckle patterns from palm dorsal surface finger or palm dorsal surface is inherently revealed while presenting ( slap ) fingerprints during border crossings or during day - to - day activities , such as driving , holding arms , signing documents , or playing sports . finger knuckle patterns are believed to be correlated with the anatomy of fingers that involve complex interaction of finger bones , tissues , and skin , which can be uniquely identify the individuals . this paper investigates the possibility of using lowest finger knuckle patterns formed on joints between the metacarpal and proximal phalanx bones for the automated personal identification . we automatically segment such region of interest from the palm dorsal images and normalize / enhance them to accommodate illumination , scale , and pose variations resulting from the contactless imaging . the normalized knuckle images are investigated for the matching performance using several spatial and spectral domain approaches . we use database of 501 different subjects acquired from the contactless hand imaging to ascertain the performance . this paper also evaluates the possibility of using palm dorsal surface regions , in combination with minor knuckle patterns , and provides finger dorsal image database from 712 different subjects for the performance evaluation . the experimental results presented in this paper are very encouraging and demonstrates the potential of such unexplored minor finger knuckle patterns for the biometrics identification .
RANK = 9; score = 0.1935483870967742; correct = False; id = 36cf96fe11a2c1ea4d999a7f86ffef6eea7b5958
rgb - d face recognition with texture and attribute features face recognition algorithms generally utilize 2d images for feature extraction and matching . to achieve higher resilience toward covariates , such as expression , illumination , and pose , 3d face recognition algorithms are developed . while it is challenging to use specialized 3d sensors due to high cost , rgb - d images can be captured by low - cost sensors such as kinect . this research introduces a novel face recognition algorithm using rgb - d images . the proposed algorithm computes a descriptor based on the entropy of rgb - d faces along with the saliency feature obtained from a 2d face . geometric facial attributes are also extracted from the depth image and face recognition is performed by fusing both the descriptor and attribute match scores . the experimental results indicate that the proposed algorithm achieves high face recognition accuracy on rgb - d images obtained using kinect compared with existing 2d and 3d approaches .
RANK = 10; score = 0.18867924528301888; correct = False; id = b960dbad7dfac75661be61d9dd42593ae7f5c244
a delaunay quadrangle - based fingerprint authentication system with template protection using topology code for local registration and security enhancement although some nice properties of the delaunay triangle - based structure have been exploited in many fingerprint authentication systems and satisfactory outcomes have been reported , most of these systems operate without template protection . in addition , the feature sets and similarity measures utilized in these systems are not suitable for existing template protection techniques . moreover , local structural change caused by nonlinear distortion is often not considered adequately in these systems . in this paper , we propose a delaunay quadrangle - based fingerprint authentication system to deal with nonlinear distortion - induced local structural change that the delaunay triangle - based structure suffers . fixed - length and alignment - free feature vectors extracted from delaunay quadrangles are less sensitive to nonlinear distortion and more discriminative than those from delaunay triangles and can be applied to existing template protection directly . furthermore , we propose to construct a unique topology code from each delaunay quadrangle . not only can this unique topology code help to carry out accurate local registration under distortion , but it also enhances the security of template data . experimental results on public databases and security analysis show that the delaunay quadrangle - based system with topology code can achieve better performance and higher security level than the delaunay triangle - based system , the delaunay quadrangle - based system without topology code , and some other similar systems .
RANK = 11; score = 0.18831168831168832; correct = False; id = 347d58f011efd9d6dd375e1840341ec37e68f55a
a local tree alignment - based soft pattern matching approach for information extraction this paper presents a new soft pattern matching method which aims to improve the recall with minimized precision loss in information extraction tasks . our approach is based on a local tree alignment algorithm , and an effective strategy for controlling flexibility of the pattern matching will be presented . the experimental results show that the method can significantly improve the information extraction performance .
RANK = 12; score = 0.1853932584269663; correct = False; id = 0be7d828e66329ea004e1492b203b979ea4eee4b
biometric template protection using universal background models : an application to online signature data security and privacy are crucial issues to be addressed for assuring a successful deployment of biometrics - based recognition systems in real life applications . in this paper , a template protection scheme exploiting the properties of universal background models , eigen - user spaces , and the fuzzy commitment cryptographic protocol is presented . a detailed discussion on the security and information leakage of the proposed template protection system is given . the effectiveness of the proposed approach is investigated with application to online signature recognition . the given experimental results , evaluated on the public mcyt signature database , show that the proposed system can guarantee competitive recognition accuracy while providing protection to the employed biometric data .
RANK = 13; score = 0.18433179723502305; correct = False; id = 632156b68626ee1352c666c25310f8818e6932b6
face spoofing detection using colour texture analysis research on non - intrusive software - based face spoofing detection schemes has been mainly focused on the analysis of the luminance information of the face images , hence discarding the chroma component , which can be very useful for discriminating fake faces from genuine ones . this paper introduces a novel and appealing approach for detecting face spoofing using a colour texture analysis . we exploit the joint colour - texture information from the luminance and the chrominance channels by extracting complementary low - level feature descriptions from different colour spaces . more specifically , the feature histograms are computed over each image band separately . extensive experiments on the three most challenging benchmark data sets , namely , the casia face anti - spoofing database , the replay - attack database , and the msu mobile face spoof database , showed excellent results compared with the state of the art . more importantly , unlike most of the methods proposed in the literature , our proposed approach is able to achieve stable performance across all the three benchmark data sets . the promising results of our cross - database evaluation suggest that the facial colour texture representation is more stable in unknown conditions compared with its gray - scale counterparts .
RANK = 14; score = 0.18181818181818182; correct = False; id = 47594458d19edcc0cec630a673d3da460963c48a
domain adaptation for crf - based chinese word segmentation using free annotations supervised methods have been the dominant approach for chinese word segmentation . the performance can drop significantly when the test domain is different from the training domain . in this paper , we study the problem of obtaining partial annotation from freely available data to help chinese word segmentation on different domains . different sources of free annotations are transformed into a unified form of partial annotation and a variant crf model is used to leverage both fully and partially annotated data consistently . experimental results show that the chinese word segmentation model benefits from free partially annotated data . on the sighan bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .
RANK = 15; score = 0.1791907514450867; correct = False; id = 92b0a8d189abc6f2b2ea740a92c0fa2cede381c4
personal authentication using finger knuckle surface this paper investigates a new approach for personal authentication using fingerback surface imaging . the texture pattern produced by the finger knuckle bending is highly unique and makes the surface a distinctive biometric identifier . the finger geometry features can be simultaneously acquired from the same image at the same time and integrated to further improve the user - identification accuracy of such a system . the fingerback surface images from each user are normalized to minimize the scale , translation , and rotational variations in the knuckle images . this paper details the development of such an approach using peg - free imaging . the experimental results from the proposed approach are promising and confirm the usefulness of such an approach for personal authentication .
RANK = 16; score = 0.1787709497206704; correct = False; id = b6e5711c9fde7bec29baf19afbf9da41bb15cce0
improved sentence alignment on parallel web pages using a stochastic tree alignment model parallel web pages are important source of training data for statistical machine translation . in this paper , we present a new approach to sentence alignment on parallel web pages . parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . in our approach , the web page is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as “ hansard ” . with improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web .
RANK = 17; score = 0.17647058823529413; correct = False; id = 481f4625683db8ee254f15b955c0b568c8148523
multi - document summarisation using generic relation extraction experiments are reported that investigate the effect of various source document representations on the accuracy of the sentence extraction phase of a multidocument summarisation task . a novel representation is introduced based on generic relation extraction ( gre ) , which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters . results demonstrate performance that is significantly higher than a non - trivial baseline that uses tf*idf -weighted words and at least as good as a comparable but less general approach from the literature . analysis shows that the representations compared are complementary , suggesting that extraction performance could be further improved through system combination .
RANK = 18; score = 0.17592592592592593; correct = False; id = ed184fda0306079f2ee55a1ae60fbf675c8e11c6
on recognizing faces in videos using clustering - based re - ranking and fusion due to widespread applications , availability of large intra - personal variations in video and limited information content in still images , video - based face recognition has gained significant attention . unlike still face images , videos provide abundant information that can be leveraged to address variations in pose , illumination , and expression as well as enhance the face recognition performance . this paper presents a video - based face recognition algorithm that computes a discriminative video signature as an ordered list of still face images from a large dictionary . a three - stage approach is proposed for optimizing ranked lists across multiple video frames and fusing them into a single composite ordered list to compute the video signature . this signature embeds diverse intra - personal variations and facilitates in matching two videos with large variations . for matching two videos , a discounted cumulative gain measure is utilized , which uses the ranking of images in the video signature as well as the usefulness of images in characterizing the individual in the video . the efficacy of the proposed algorithm is evaluated under different video - based face recognition scenarios such as matching still face images with videos and matching videos with videos . the efficacy of the proposed algorithm is demonstrated on the youtube faces database and the mbgc v2 video challenge database that comprise different types of video - based face recognition challenges such as matching still face images with videos and matching videos with videos . performance comparison with the benchmark results on both the databases and a commercial face recognition system shows the efficiency of the proposed algorithm for video - based face recognition .
RANK = 19; score = 0.17372881355932204; correct = False; id = 08580d047002ba1f10bcbb812e92f2785a6239d9
where you are is who you are : user identification by matching statistics most users of online services have unique behavioral or usage patterns . these behavioral patterns can be exploited to identify and track users by using only the observed patterns in the behavior . we study the task of identifying users from statistics of their behavioral patterns . in particular , we focus on the setting in which we are given histograms of users ' data collected during two different experiments . we assume that , in the first data set , the users ' identities are anonymized or hidden and that , in the second data set , their identities are known . we study the task of identifying the users by matching the histograms of their data in the first data set with the histograms from the second data set . in recent works , the optimal algorithm for this user identification task is introduced . in this paper , we evaluate the effectiveness of this method on three different types of data sets with up to 50 000 users , and in multiple scenarios . using data sets such as call data records , web browsing histories , and gps trajectories , we demonstrate that a large fraction of users can be easily identified given only histograms of their data ; hence , these histograms can act as users ' fingerprints . we also verify that simultaneous identification of users achieves better performance compared with one - by - one user identification . furthermore , we show that using the optimal method for identification indeed gives higher identification accuracy than the heuristics - based approaches in the practical scenarios . the accuracy obtained under this optimal method can thus be used to quantify the maximum level of user identification that is possible in such settings . we show that the key factors affecting the accuracy of the optimal identification algorithm are the duration of the data collection , the number of users in the anonymized data set , and the resolution of the data set . we also analyze the effectiveness of k - anonymization in resisting user identification attacks on these data sets .
RANK = 20; score = 0.1735159817351598; correct = False; id = f6cea99f7830541ac640865a94c264972470a2e1
application of projective invariants in hand geometry biometrics our research focuses on finding mathematical representations of biometric features that are not only distinctive , but also invariant to projective transformations . we have chosen hand geometry technology to work with , because it has wide public awareness and acceptance and most important , large space for improvement . unlike the traditional hand geometry technologies , the hand descriptor in our hand geometry system is constructed using projective - invariant features . hand identification can be accomplished by a single view of a hand regardless of the viewing angles . the noise immunity and the discriminability possessed by a hand feature vector using different types of projective invariants are studied . we have found an appropriate symmetric polynomial representation of the hand features with which both noise immunity and discrimminability of a hand feature vector are considerably improved . experimental results show that the system achieves an equal error rate ( eer ) of 2.1 % by a 5-d feature vector on a database of 52 hand images . the eer reduces to 0.00 % when the feature vector dimension increases to 18 . in this paper , we extend the concept of hand geometry from a geometrical size - based technique that requires physical hand constraints to a projective invariant - based technique that allows free hand motion .

RANKING 1701
QUERY
label - sensitive deep metric learning for facial age estimation in this paper , we present a label - sensitive deep metric learning ( lsdml ) approach for facial age estimation . motivated by the fact that human age labels are chronologically correlated , our proposed lsdml aims to seek a series of hierarchical nonlinear transformations by deep residual network to project face samples to a latent common space , where the similarity of face pairs is equivalently isotonic to the age difference in a ranking - preserving manner . since traversal access to total negative samples catastrophically costs and leads to suboptimal , our model learns to mine hard meaningful samples in parallel to learning feature similarity , so that the local manifold of face samples is preserved in the transformed subspace . to better improve the performance on the data set that contains few labeled samples , we further extend our lsdml to a multi - source lsdml method , which aims at maximizing the cross - population correlation of different face aging data sets . extensive experimental results on four benchmarking data sets show the effectiveness of our proposed approach .
First cited at 4
TOP CITED PAPERS
RANK 4
joint feature learning for face recognition this paper presents a new joint feature learning ( jfl ) approach to automatically learn feature representation from raw pixels for face recognition . unlike many existing face recognition systems , where conventional feature descriptors , such as local binary patterns and gabor features , are used for face representation , we propose an unsupervised feature learning method to learn hierarchical feature representation . since different face regions have different physical characteristics , we propose to use different feature dictionaries to represent them , and to learn multiple yet related feature projection matrices for these regions simultaneously . hence position - specific discriminative information can be exploited for face representation . having learned these feature projections for different face regions , we perform spatial pooling for face patches within each region to enhance the representative power of the learned features . moreover , we stack our jfl model into a deep architecture to exploit hierarchical information for feature representation and further improve the recognition performance . experimental results on five widely used face data sets show the effectiveness of our proposed approach .
RANK 8
subspace learning for facial age estimation via pairwise age ranking age is one of the important biometric traits for reinforcing the identity authentication . the challenge of facial age estimation mainly comes from two difficulties : ( 1 ) the wide diversity of visual appearance existing even within the same age group and ( 2 ) the limited number of labeled face images in real cases . motivated by previous research on human cognition , human beings can confidently rank the relative ages of facial images , we postulate that the age rank plays a more important role in the age estimation than visual appearance attributes . in this paper , we assume that the age ranks can be characterized by a set of ranking features lying on a low - dimensional space . we propose a simple and flexible subspace learning method by solving a sequence of constrained optimization problems . with our formulation , both the aging manifold , which relies on exact age labels , and the implicit age ranks are jointly embedded in the proposed subspace . in addition to supervised age estimation , our method also extends to semi - supervised age estimation via automatically approximating the age ranks of unlabeled data . therefore , we can successfully include more available data to improve the feature discriminability . in the experiments , we adopt the support vector regression on the proposed ranking features to learn our age estimators . the results on the age estimation demonstrate that our method outperforms classic subspace learning approaches , and the semi - supervised learning successfully incorporates the age ranks from unlabeled data under different scales and sources of data set .
RANK 19
a discriminative model for age invariant face recognition aging variation poses a serious problem to automatic face recognition systems . most of the face recognition studies that have addressed the aging problem are focused on age estimation or aging simulation . designing an appropriate feature representation and an effective matching framework for age invariant face recognition remains an open problem . in this paper , we propose a discriminative model to address face matching in the presence of age variation . in this framework , we first represent each face by designing a densely sampled local feature description scheme , in which scale invariant feature transform ( sift ) and multi - scale local binary patterns ( mlbp ) serve as the local descriptors . by densely sampling the two kinds of local descriptors from the entire facial image , sufficient discriminatory information , including the distribution of the edge direction in the face image ( that is expected to be age invariant ) can be extracted for further analysis . since both sift - based local features and mlbp - based local features span a high - dimensional feature space , to avoid the overfitting problem , we develop an algorithm , called multi - feature discriminant analysis ( mfda ) to process these two local feature spaces in a unified framework . the mfda is an extension and improvement of the lda using multiple features combined with two different random sampling methods in feature and sample space . by random sampling the training set as well as the feature space , multiple lda - based classifiers are constructed and then combined to generate a robust decision via a fusion rule . experimental results show that our approach outperforms a state - of - the - art commercial face recognition engine on two public domain face aging data sets : morph and fg - net . we also compare the performance of the proposed discriminative model with a generative aging model . a fusion of discriminative and generative models further improves the face matching accuracy in the presence of aging .
TOP UNCITED PAPERS
RANK 1
muli - label text categorization with hidden components multi - label text categorization ( mtc ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously . the labels in the mtc are correlated and the correlation results in some hidden components , which represent the ” share ” variance of correlated labels . in this paper , we propose a method with hidden components for mtc . the proposed method employs pca to capture the hidden components , and incorporates them into a joint learning framework to improve the performance . experiments with real - world data sets and evaluation metrics validate the effectiveness of the proposed method .
RANK 2
reconstruction - based metric learning for unconstrained face verification in this paper , we propose a reconstruction - based metric learning method to learn a discriminative distance metric for unconstrained face verification . unlike conventional metric learning methods , which only consider the label information of training samples and ignore the reconstruction residual information in the learning procedure , we apply a reconstruction criterion to learn a discriminative distance metric . for each training example , the distance metric is learned by enforcing a margin between the interclass sparse reconstruction residual and interclass sparse reconstruction residual , so that the reconstruction residual of training samples can be effectively exploited to compute the between - class and within - class variations . to better use multiple features for distance metric learning , we propose a reconstruction - based multimetric learning method to collaboratively learn multiple distance metrics , one for each feature descriptor , to remove uncorrelated information for recognition . we evaluate our proposed methods on the labelled faces in the wild ( lfw ) and youtube face data sets and our experimental results clearly show the superiority of our methods over both previous metric learning methods and several state - of - the - art unconstrained face verification methods .
RANK 3
discriminative multimetric learning for kinship verification in this paper , we propose a new discriminative multimetric learning method for kinship verification via facial image analysis . given each face image , we first extract multiple features using different face descriptors to characterize face images from different aspects because different feature descriptors can provide complementary information . then , we jointly learn multiple distance metrics with these extracted multiple features under which the probability of a pair of face image with a kinship relation having a smaller distance than that of the pair without a kinship relation is maximized , and the correlation of different features of the same face sample is maximized , simultaneously , so that complementary and discriminative information is exploited for verification . experimental results on four face kinship data sets show the effectiveness of our proposed method over the existing single - metric and multimetric learning methods .
TOP 20
RANK = 1; score = 0.22297297297297297; correct = False; id = 5a33a061289207695e652201a4054e9163637da4
muli - label text categorization with hidden components multi - label text categorization ( mtc ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously . the labels in the mtc are correlated and the correlation results in some hidden components , which represent the ” share ” variance of correlated labels . in this paper , we propose a method with hidden components for mtc . the proposed method employs pca to capture the hidden components , and incorporates them into a joint learning framework to improve the performance . experiments with real - world data sets and evaluation metrics validate the effectiveness of the proposed method .
RANK = 2; score = 0.2138728323699422; correct = False; id = 1fcdc113a5df2f45a1f4b3249c041d942a3a730b
reconstruction - based metric learning for unconstrained face verification in this paper , we propose a reconstruction - based metric learning method to learn a discriminative distance metric for unconstrained face verification . unlike conventional metric learning methods , which only consider the label information of training samples and ignore the reconstruction residual information in the learning procedure , we apply a reconstruction criterion to learn a discriminative distance metric . for each training example , the distance metric is learned by enforcing a margin between the interclass sparse reconstruction residual and interclass sparse reconstruction residual , so that the reconstruction residual of training samples can be effectively exploited to compute the between - class and within - class variations . to better use multiple features for distance metric learning , we propose a reconstruction - based multimetric learning method to collaboratively learn multiple distance metrics , one for each feature descriptor , to remove uncorrelated information for recognition . we evaluate our proposed methods on the labelled faces in the wild ( lfw ) and youtube face data sets and our experimental results clearly show the superiority of our methods over both previous metric learning methods and several state - of - the - art unconstrained face verification methods .
RANK = 3; score = 0.21341463414634146; correct = False; id = b5050d74dd8f0384506bcd365b31044c80d476c0
discriminative multimetric learning for kinship verification in this paper , we propose a new discriminative multimetric learning method for kinship verification via facial image analysis . given each face image , we first extract multiple features using different face descriptors to characterize face images from different aspects because different feature descriptors can provide complementary information . then , we jointly learn multiple distance metrics with these extracted multiple features under which the probability of a pair of face image with a kinship relation having a smaller distance than that of the pair without a kinship relation is maximized , and the correlation of different features of the same face sample is maximized , simultaneously , so that complementary and discriminative information is exploited for verification . experimental results on four face kinship data sets show the effectiveness of our proposed method over the existing single - metric and multimetric learning methods .
RANK = 4; score = 0.21311475409836064; correct = True; id = 2bbe89f61a8d6d4d6e39fdcaf8c185f110a01c78
joint feature learning for face recognition this paper presents a new joint feature learning ( jfl ) approach to automatically learn feature representation from raw pixels for face recognition . unlike many existing face recognition systems , where conventional feature descriptors , such as local binary patterns and gabor features , are used for face representation , we propose an unsupervised feature learning method to learn hierarchical feature representation . since different face regions have different physical characteristics , we propose to use different feature dictionaries to represent them , and to learn multiple yet related feature projection matrices for these regions simultaneously . hence position - specific discriminative information can be exploited for face representation . having learned these feature projections for different face regions , we perform spatial pooling for face patches within each region to enhance the representative power of the learned features . moreover , we stack our jfl model into a deep architecture to exploit hierarchical information for feature representation and further improve the recognition performance . experimental results on five widely used face data sets show the effectiveness of our proposed approach .
RANK = 5; score = 0.21052631578947367; correct = False; id = bed96649ff6080fcb648e130a320e86effc24301
modeling semantic relevance for question - answer pairs in web social communities quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora . in this paper , a deep belief network is proposed to model the semantic relevance for question - answer pairs . observing the textual similarity between the community - driven questionanswering ( cqa ) dataset and the forum dataset , we present a novel learning strategy to promote the performance of our method on the social community datasets without hand - annotating work . the experimental results show that our method outperforms the traditional approaches on both the cqa and the forum corpora .
RANK = 6; score = 0.20930232558139536; correct = False; id = 42432b5f4b2a3e3e2b0399aa63b228a52534c534
non - linear learning for statistical machine translation modern statistical machine translation ( smt ) systems usually use a linear combination of features to model the quality of each translation hypothesis . the linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner , which might limit the expressive power of the model and lead to a under - fit model on the current data . in this paper , we propose a nonlinear modeling for the quality of translation hypotheses based on neural networks , which allows more complex interaction between features . a learning framework is presented for training the non - linear models . we also discuss possible heuristics in designing the network structure which may improve the non - linear learning performance . experimental results show that with the basic features of a hierarchical phrase - based machine translation system , our method produce translations that are better than a linear model .
RANK = 7; score = 0.20915032679738563; correct = False; id = 3b41bc34dde4284c174c27529c3707f95d8b35e6
language model weight adaptation based on cross - entropy for statistical machine translation in this paper , we investigate the language model ( lm ) adaptation issue for statistical machine translation ( smt ) . in order to overcome the weight bias on the lm obtained from the development data , a simple but effective method is proposed to adapt the lm for diverse test datasets by employing the cross entropy of translation hypotheses as a metric to measure the similarity between different datasets . experimental results show that the cross entropy of a test dataset is closely correlated with the bias in estimating the language models and our adaptation strategy significantly outperforms a strong baseline .
RANK = 8; score = 0.20754716981132076; correct = True; id = cfa40560fa74b2fb5c26bdd6ea7c610ba5130e2f
subspace learning for facial age estimation via pairwise age ranking age is one of the important biometric traits for reinforcing the identity authentication . the challenge of facial age estimation mainly comes from two difficulties : ( 1 ) the wide diversity of visual appearance existing even within the same age group and ( 2 ) the limited number of labeled face images in real cases . motivated by previous research on human cognition , human beings can confidently rank the relative ages of facial images , we postulate that the age rank plays a more important role in the age estimation than visual appearance attributes . in this paper , we assume that the age ranks can be characterized by a set of ranking features lying on a low - dimensional space . we propose a simple and flexible subspace learning method by solving a sequence of constrained optimization problems . with our formulation , both the aging manifold , which relies on exact age labels , and the implicit age ranks are jointly embedded in the proposed subspace . in addition to supervised age estimation , our method also extends to semi - supervised age estimation via automatically approximating the age ranks of unlabeled data . therefore , we can successfully include more available data to improve the feature discriminability . in the experiments , we adopt the support vector regression on the proposed ranking features to learn our age estimators . the results on the age estimation demonstrate that our method outperforms classic subspace learning approaches , and the semi - supervised learning successfully incorporates the age ranks from unlabeled data under different scales and sources of data set .
RANK = 9; score = 0.2; correct = False; id = 505ea4493e4b733352c921401a96d92b4e6d4448
coupled discriminative feature learning for heterogeneous face recognition this paper presents a coupled discriminative feature learning ( cdfl ) method for heterogeneous face recognition ( hfr ) . different from most existing hfr approaches which use hand - crafted feature descriptors for face representation , our cdfl directly learns discriminative features from raw pixels for face representation . in particular , a couple of image filters is learned in cdfl to simultaneously exploit discriminative information and to reduce the appearance difference of face images captured across different modalities . with the help of the learned filters , cdfl can maximize the interclass variations and minimize the intraclass variations of the learned feature vectors , and meanwhile maximize the correlation of face images of the same person from different modalities by solving a generalized eigenvalue problem . experimental results on three different heterogeneous face recognition applications show the effectiveness of our proposed approach .
RANK = 10; score = 0.2; correct = False; id = 4404a5ba8e9b6619f88bca58dc6f281e4a0dde49
ricoh at semeval-2016 task 1 : ir - based semantic textual similarity estimation this paper describes our ir ( information retrieval ) based method for semeval 2016 task 1 , semantic textual similarity ( sts ) . the main feature of our approach is to extend a conventional ir - based scheme by incorporating word alignment information . this enables us to develop a more fine - grained similarity measurement . in the evaluation results , we have seen that the proposed method improves upon a conventional ir - based method on average . in addition , one of our submissions achieved the best performance for the “ postediting ” data set .
RANK = 11; score = 0.19889502762430938; correct = False; id = c33c8bb663f651918ff8c4ce04d941e1d2e9be0e
cost - sensitive subspace analysis and extensions for face recognition conventional subspace - based face recognition methods seek low - dimensional feature subspaces to achieve high classification accuracy and assume the same loss from different types of misclassification . this assumption , however , may not hold in many practical face recognition systems as different types of misclassification could lead to different losses . motivated by this concern , this paper proposes a cost - sensitive subspace analysis approach for face recognition . our approach uses a cost matrix specifying different costs corresponding to different types of misclassifications , into two popular and widely used discriminative subspace analysis methods and devises the cost - sensitive linear discriminant analysis ( cslda ) and cost - sensitive marginal fisher analysis ( csmfa ) methods , to achieve a minimum overall recognition loss by performing recognition in these learned low - dimensional subspaces . to better exploit the complementary information from multiple features for improved face recognition , we further propose a multiview cost - sensitive subspace analysis approach by seeking a common feature subspace to fuse multiple face features to improve the recognition performance . extensive experimental results demonstrate the effectiveness of our proposed methods .
RANK = 12; score = 0.19653179190751446; correct = False; id = f6cb42fc00c4e8ce3cf38513a043e897a040f1be
co - regularizing character - based and word - based models for semi - supervised chinese word segmentation this paper presents a semi - supervised chinese word segmentation ( cws ) approach that co - regularizes character - based and word - based models . similarly to multi - view learning , the “ segmentation agreements ” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data . the proposed approach trains a character - based and word - based model on labeled data , respectively , as the initial models . then , the two models are constantly updated using unlabeled examples , where the learning objective is maximizing their segmentation agreements . the agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data . the segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models . the evaluation on the chinese tree bank reveals that our model results in better gains over the state - of - the - art semi - supervised models reported in the literature .
RANK = 13; score = 0.1951219512195122; correct = False; id = ddbdedf94222232f41cf1c0200d973f000568fe7
improving statistical machine translation accuracy using bilingual lexicon extractionwith paraphrases statistical machine translation ( smt ) suffers from the accuracy problem that the translation pairs and their feature scores in the translation model can be inaccurate . the accuracy problem is caused by the quality of the unsupervised methods used for translation model learning . previous studies propose estimating comparable features for the translation pairs in the translation model from comparable corpora , to improve the accuracy of the translation model . comparable feature estimation is based on bilingual lexicon extraction ( ble ) technology . however , ble suffers from the data sparseness problem , which makes the comparable features inaccurate . in this paper , we propose using paraphrases to address this problem . paraphrases are used to smooth the vectors used in comparable feature estimation with ble . in this way , we improve the quality of comparable features , which can improve the accuracy of the translation model thus improve smt performance . experiments conducted on chinese - english phrase - based smt ( pbsmt ) verify the effectiveness of our proposed method .
RANK = 14; score = 0.19411764705882353; correct = False; id = 9aafcf0cb5702c01fe7cbd591bd4e752ac5b8986
improving learning and inference in a large knowledge - base using latent syntactic cues automatically constructed knowledge bases ( kbs ) are often incomplete and there is a genuine need to improve their coverage . path ranking algorithm ( pra ) is a recently proposed method which aims to improve kb coverage by performing inference directly over the kb graph . for the first time , we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million web documents can significantly outperform previous prabased approaches on the kb inference task . we present extensive experimental results validating this finding . the resources presented in this paper are publicly available .
RANK = 15; score = 0.19375; correct = False; id = 83cf4b2f39bcc802b09fd59b69e23068447b26b7
multi - task learning for multiple language translation in this paper , we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages . our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem . we extend the neural machine translation to a multi - task learning framework which shares source language representation and separates the modeling of different target language translation . our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available . experiments show that our multi - task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available .
RANK = 16; score = 0.19318181818181818; correct = False; id = 02fc310d7032e56e242a8e560a99df869f172930
automated essay scoring by maximizing human - machine agreement previous approaches for automated essay scoring ( aes ) learn a rating model by minimizing either the classification , regression , or pairwise classification loss , depending on the learning algorithm used . in this paper , we argue that the current aes systems can be further improved by taking into account the agreement between human and machine raters . to this end , we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model , where the agreement between the human and machine raters is directly incorporated into the loss function . various linguistic and statistical features are utilized to facilitate the learning algorithms . experiments on the publicly available english essay dataset , automated student assessment prize ( asap ) , show that our proposed approach outperforms the state - of - the - art algorithms , and achieves performance comparable to professional human raters , which suggests the effectiveness of our proposed method for automated essay scoring .
RANK = 17; score = 0.192090395480226; correct = False; id = af7a442723b5c1a0becc1bd496b6382506c2026a
cross - lingual mixture model for sentiment classification the amount of labeled sentiment data in english is much larger than that in other languages . such a disproportion arouse interest in cross - lingual sentiment classification , which aims to conduct sentiment classification in the target language ( e.g. chinese ) using labeled data in the source language ( e.g. english ) . most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language . this approach suffers from the limited coverage of vocabulary in the machine translation results . in this paper , we propose a generative cross - lingual mixture model ( clmm ) to leverage unlabeled bilingual parallel data . by fitting parameters to maximize the likelihood of the bilingual parallel data , the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly . experiments on multiple data sets show that clmm is consistently effective in two settings : ( 1 ) labeled data in the target language are unavailable ; and ( 2 ) labeled data in the target language are also available .
RANK = 18; score = 0.19161676646706588; correct = False; id = 3adf295bdb7cc80148608922e8000c46e0d693d3
from bilingual dictionaries to interlingual document representations mapping documents into an interlingual representation can help bridge the language barrier of a cross - lingual corpus . previous approaches use aligned documents as training data to learn an interlingual representation , making them sensitive to the domain of the training data . in this paper , we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary . we first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation . since the candidate alignments are noisy , we develop a robust learning algorithm to learn the interlingual representation . we show that bilingual dictionaries generalize to different domains better : our approach gives better performance than either a word by word translation method or canonical correlation analysis ( cca ) trained on a different domain .
RANK = 19; score = 0.19130434782608696; correct = True; id = 4b519e2e88ccd45718b0fc65bfd82ebe103902f7
a discriminative model for age invariant face recognition aging variation poses a serious problem to automatic face recognition systems . most of the face recognition studies that have addressed the aging problem are focused on age estimation or aging simulation . designing an appropriate feature representation and an effective matching framework for age invariant face recognition remains an open problem . in this paper , we propose a discriminative model to address face matching in the presence of age variation . in this framework , we first represent each face by designing a densely sampled local feature description scheme , in which scale invariant feature transform ( sift ) and multi - scale local binary patterns ( mlbp ) serve as the local descriptors . by densely sampling the two kinds of local descriptors from the entire facial image , sufficient discriminatory information , including the distribution of the edge direction in the face image ( that is expected to be age invariant ) can be extracted for further analysis . since both sift - based local features and mlbp - based local features span a high - dimensional feature space , to avoid the overfitting problem , we develop an algorithm , called multi - feature discriminant analysis ( mfda ) to process these two local feature spaces in a unified framework . the mfda is an extension and improvement of the lda using multiple features combined with two different random sampling methods in feature and sample space . by random sampling the training set as well as the feature space , multiple lda - based classifiers are constructed and then combined to generate a robust decision via a fusion rule . experimental results show that our approach outperforms a state - of - the - art commercial face recognition engine on two public domain face aging data sets : morph and fg - net . we also compare the performance of the proposed discriminative model with a generative aging model . a fusion of discriminative and generative models further improves the face matching accuracy in the presence of aging .
RANK = 20; score = 0.1907514450867052; correct = False; id = e7a38f24b3b444b8aca5c15fe1687a6321d327b3
improving english - russian sentence alignment through pos tagging and damerau - levenshtein distance the present paper introduces approach to improve english - russian sentence alignment , based on pos - tagging of automatically aligned ( by hunalign ) source and target texts . the initial hypothesis is tested on a corpus of bitexts . sequences of pos tags for each sentence ( exactly , nouns , adjectives , verbs and pronouns ) are processed as “ words ” and dameraulevenshtein distance between them is computed . this distance is then normalized by the length of the target sentence and is used as a threshold between supposedly mis - aligned and “ good ” sentence pairs . the experimental results show precision 0.81 and recall 0.8 , which allows the method to be used as additional data source in parallel corpora alignment . at the same time , this leaves space for further improvement .

RANKING 2037
QUERY
multiwoz - a large - scale multi - domain wizard - of - oz dataset for task - oriented dialogue modelling even though machine learning has become the major scene in dialogue research community , the real breakthrough has been blocked by the scale of data available . to address this fundamental obstacle , we introduce the multidomain wizard - of - oz dataset ( multiwoz ) , a fully - labeled collection of human - human written conversations spanning over multiple domains and topics . at a size of 10k dialogues , it is at least one order of magnitude larger than all previous annotated task - oriented corpora . the contribution of this work apart from the open - sourced dataset labelled with dialogue belief states and dialogue actions is two - fold : firstly , a detailed description of the data collection procedure along with a summary of data structure and analysis is provided . the proposed data - collection pipeline is entirely based on crowd - sourcing without the need of hiring professional annotators ; secondly , a set of benchmark results of belief tracking , dialogue act and response generation is reported , which shows the usability of the data and sets a baseline for future studies .
First cited at 39
TOP CITED PAPERS
RANK 39
multi - domain neural network language generation for spoken dialogue systems moving from limited - domain natural language generation ( nlg ) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains . therefore , it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation . in this paper , we propose a procedure to train multi - domain , recurrent neural network - based ( rnn ) language generators via multiple adaptation steps . in this procedure , a model is first trained on counterfeited data synthesised from an out - of - domain dataset , and then fine tuned on a small set of in - domain utterances with a discriminative objective function . corpus - based evaluation results show that the proposed procedure can achieve competitive performance in terms of bleu score and slot error rate while significantly reducing the data needed to train generators in new , unseen domains . in subjective testing , human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain .
RANK 207
semantically conditioned lstm - based natural language generation for spoken dialogue systems natural language generation ( nlg ) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality . most nlg systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language . they are also not easily scaled to systems covering multiple domains and languages . this paper presents a statistical language generator based on a semantically controlled long short - term memory ( lstm ) structure . the lstm generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion , and language variation can be easily achieved by sampling from output candidates . with fewer heuristics , an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods . human judges scored the lstm system higher on informativeness and naturalness and overall preferred it to the other systems .
RANK 714
unsupervised modeling of twitter conversations we propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain . trained on a corpus of noisy twitter conversations , our method discovers dialogue acts by clustering raw utterances . because it accounts for the sequential behaviour of these acts , the learned model can provide insight into the shape of communication in a new medium . we address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task . this work is inspired by a corpus of 1.3 million twitter conversations , which will be made publicly available . this huge amount of data , available only because twitter blurs the line between chatting and publishing , highlights the need to be able to adapt quickly to a new medium .
TOP UNCITED PAPERS
RANK 1
a large annotated corpus for learning natural language inference understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations . however , machine learning research in this area has been dramatically limited by the lack of large - scale resources . to address this , we introduce the stanford natural language inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning . at 570 k pairs , it is two orders of magnitude larger than all other resources of its type . this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models , and it allows a neural network - based model to perform competitively on natural language inference benchmarks for the first time .
RANK 2
improving citation polarity classification with product reviews recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering . while this result confirms that citation classification is feasible , there are two drawbacks to this approach : ( i ) it requires a large annotated corpus for supervised classification , which in the case of scientific literature is quite expensive ; and ( ii ) feature engineering that is too specific to one area of scientific literature may not be portable to other domains , even within scientific literature . in this paper we address these two drawbacks . first , we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains . then , to avoid over - engineering specific citation features for a particular scientific domain , we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features . we achieve better citation classification results with this cross - domain approach than using in - domain classification .
RANK 3
a shared task involving multi - label classification of clinical free text this paper reports on a shared task involving the assignment of icd-9-cm codes to radiology reports . two features distinguished this task from previous shared tasks in the biomedical domain . one is that it resulted in the first freely distributable corpus of fully anonymized clinical text . this resource is permanently available and will ( we hope ) facilitate future research . the other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels . the number of participants was larger than in any previous biomedical challenge task . we describe the data production process and the evaluation measures , and give a preliminary analysis of the results . many systems performed at levels approaching the inter - coder agreement , suggesting that human - like performance on this task is within the reach of currently available technologies .
TOP 20
RANK = 1; score = 0.2154696132596685; correct = False; id = 0dab72129b4458d9e3dbf1f109848c2d6d7af8a8
a large annotated corpus for learning natural language inference understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations . however , machine learning research in this area has been dramatically limited by the lack of large - scale resources . to address this , we introduce the stanford natural language inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning . at 570 k pairs , it is two orders of magnitude larger than all other resources of its type . this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models , and it allows a neural network - based model to perform competitively on natural language inference benchmarks for the first time .
RANK = 2; score = 0.19487179487179487; correct = False; id = 1011dc13c78e7e805ae93470ccf9053ea247772b
improving citation polarity classification with product reviews recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering . while this result confirms that citation classification is feasible , there are two drawbacks to this approach : ( i ) it requires a large annotated corpus for supervised classification , which in the case of scientific literature is quite expensive ; and ( ii ) feature engineering that is too specific to one area of scientific literature may not be portable to other domains , even within scientific literature . in this paper we address these two drawbacks . first , we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains . then , to avoid over - engineering specific citation features for a particular scientific domain , we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features . we achieve better citation classification results with this cross - domain approach than using in - domain classification .
RANK = 3; score = 0.19270833333333334; correct = False; id = 0d01505985083d40f9cd8c6ae8a7d61b91aa624d
a shared task involving multi - label classification of clinical free text this paper reports on a shared task involving the assignment of icd-9-cm codes to radiology reports . two features distinguished this task from previous shared tasks in the biomedical domain . one is that it resulted in the first freely distributable corpus of fully anonymized clinical text . this resource is permanently available and will ( we hope ) facilitate future research . the other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels . the number of participants was larger than in any previous biomedical challenge task . we describe the data production process and the evaluation measures , and give a preliminary analysis of the results . many systems performed at levels approaching the inter - coder agreement , suggesting that human - like performance on this task is within the reach of currently available technologies .
RANK = 4; score = 0.1881720430107527; correct = False; id = 8a5f0fb91125bcb6dba9ba325ab69e710ca5a0a3
nru - hse at semeval-2016 task 4 : comparative analysis of two iterative methods using quantification library in many areas , such as social science , politics or market research , people need to track sentiment and their changes over time . for sentiment analysis in this field it is more important to correctly estimate proportions of each sentiment expressed in the set of documents ( quantification task ) than to accurately estimate sentiment of a particular document ( classification ) . basically , our study was aimed to analyze the effectiveness of two iterative quantification techniques and to compare their effectiveness with baseline methods . all the techniques are evaluated using a set of synthesized data and the semeval-2016 task4 dataset . we made the quantification methods from this paper available as a python open source library . the results of comparison and possible limitations of the quantification techniques are discussed .
RANK = 5; score = 0.18579234972677597; correct = False; id = 0528d266f13933f5ea09de200f4a401f6ab9020c
lcsts : a large scale chinese short text summarization dataset automatic text summarization is widely regarded as the highly difficult problem , partially because of the lack of large text summarization data set . due to the great challenge of constructing the large scale summaries for full text , in this paper , we introduce a large corpus of chinese short text summarization dataset constructed from the chinese microblogging website sina weibo , which is released to the public1 . this corpus consists of over 2 million real chinese short texts with short summaries given by the author of each text . we also manually tagged the relevance of 10,666 short summaries with their corresponding short texts . based on the corpus , we introduce recurrent neural network for the summary generation and achieve promising results , which not only shows the usefulness of the proposed corpus for short text summarization research , but also provides a baseline for further research on this topic .
RANK = 6; score = 0.18461538461538463; correct = False; id = 2ef9abf5245f8abb1dd05896a23b8885ce562b90
syntax - based word ordering incorporating a large - scale language model a fundamental problem in text generation is word ordering . word ordering is a computationally difficult problem , which can be constrained to some extent for particular applications , for example by using synchronous grammars for statistical machine translation . there have been some recent attempts at the unconstrained problem of generating a sentence from a multi - set of input words ( wan et al . , 2009 ; zhang and clark , 2011 ) . by using ccg and learning guided search , zhang and clark reported the highest scores on this task . one limitation of their system is the absence of an n - gram language model , which has been used by text generation systems to improve fluency . we take the zhang and clark system as the baseline , and incorporate an n - gram model by applying online large - margin training . our system significantly improved on the baseline by 3.7 bleu points .
RANK = 7; score = 0.18446601941747573; correct = False; id = 08f4d8e7626e55b7c4ffe1fd12eb034bc8022a43
aspect extraction with automated prior knowledge learning aspect extraction is an important task in sentiment analysis . topic modeling is a popular method for the task . however , unsupervised topic models often generate incoherent aspects . to address the issue , several knowledge - based models have been proposed to incorporate prior knowledge provided by the user to guide modeling . in this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the web . such knowledge can then be used by a topic model to discover more coherent aspects . there are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault - tolerant to handle possibly wrong knowledge . a novel approach is proposed to solve these problems . experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state - of - the - art baselines .
RANK = 8; score = 0.18333333333333332; correct = False; id = c16afe2799acb8efab1b21713a105227397dacf7
the nite xml toolkit : demonstration from five corpora the nite xml toolkit ( nxt ) is open source software for working with multimodal , spoken , or text language corpora . it is specifically designed to support the tasks of human annotators and analysts of heavily cross - annotated data sets , and has been used successfully on a range of projects with varying needs . in this text to accompany a demonstration , we describe nxt along with four uses on different corpora that together span its most novel features . the examples involve the ami and icsi meeting corpora ; a study of multimodal reference ; a syntactic analysis of genesis in classical hebrew ; and discourse annotation of switchboard dialogues .
RANK = 9; score = 0.1813186813186813; correct = False; id = 811e014002d1e4d1e185fc236cf9e3fafe2aade5
addressee and response selection for multi - party conversation to create conversational systems working in actual situations , it is crucial to assume that they interact with multiple agents . in this work , we tackle addressee and response selection for multi - party conversation , in which systems are expected to select whom they address as well as what they say . the key challenge of this task is to jointly model who is talking about what in a previous context . for the joint modeling , we propose two modeling frameworks : 1 ) static modeling and 2 ) dynamic modeling . to show benchmark results of our frameworks , we created a multi - party conversation corpus . our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents .
RANK = 10; score = 0.18032786885245902; correct = False; id = 01ad43b5665e8f56dae779d445ed5c67ea34790e
towards automatic generation of gene summary in this paper we present an extractive system that automatically generates gene summaries from the biomedical literature . the proposed text summarization system selects and ranks sentences from multiple medline abstracts by exploiting gene - specific information and similarity relationships between sentences . we evaluate our system on a large dataset of 7,294 human genes and 187,628 medline abstracts using recall - oriented understudy for gisting evaluation ( rouge ) , a widely used automatic evaluation metric in the text summarization community . two baseline methods are used for comparison . experimental results show that our system significantly outperforms the other two methods with regard to all rouge metrics . a demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp .
RANK = 11; score = 0.1797752808988764; correct = False; id = 418c5d3488698b81098b74c2b1fa275a3ece2769
exploiting latent information to predict diffusions of novel topics on social networks this paper brings a marriage of two seemly unrelated topics , natural language processing ( nlp ) and social network analysis ( sna ) . we propose a new task in sna which is to predict the diffusion of a new topic , and design a learning - based framework to solve this problem . we exploit the latent semantic information among users , topics , and social connections as features for prediction . our framework is evaluated on real data collected from public domain . the experiments show 16 % auc improvement over baseline methods . the source code and dataset are available at http://www.csie.ntu.edu.tw/~d97944007/dif fusion/
RANK = 12; score = 0.1792452830188679; correct = False; id = 4c3c1c5f475944a60d7873996b786ab30fccede0
question - answer driven semantic role labeling : using natural language to annotate natural language this paper introduces the task of questionanswer driven semantic role labeling ( qa - srl ) , where question - answer pairs are used to represent predicate - argument structure . for example , the verb “ introduce ” in the previous sentence would be labeled with the questions “ what is introduced ? ” , and “ what introduces something ? ” , each paired with the phrase from the sentence that gives the correct answer . posing the problem this way allows the questions themselves to define the set of possible roles , without the need for predefined frame or thematic role ontologies . it also allows for scalable data collection by annotators with very little training and no linguistic expertise . we gather data in two domains , newswire text and wikipedia articles , and introduce simple classifierbased models for predicting which questions to ask and what their answers should be . our results show that non - expert annotators can produce high quality qa - srl data , and also establish baseline performance levels for future work on this task .
RANK = 13; score = 0.17801047120418848; correct = False; id = af7a442723b5c1a0becc1bd496b6382506c2026a
cross - lingual mixture model for sentiment classification the amount of labeled sentiment data in english is much larger than that in other languages . such a disproportion arouse interest in cross - lingual sentiment classification , which aims to conduct sentiment classification in the target language ( e.g. chinese ) using labeled data in the source language ( e.g. english ) . most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language . this approach suffers from the limited coverage of vocabulary in the machine translation results . in this paper , we propose a generative cross - lingual mixture model ( clmm ) to leverage unlabeled bilingual parallel data . by fitting parameters to maximize the likelihood of the bilingual parallel data , the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly . experiments on multiple data sets show that clmm is consistently effective in two settings : ( 1 ) labeled data in the target language are unavailable ; and ( 2 ) labeled data in the target language are also available .
RANK = 14; score = 0.17714285714285713; correct = False; id = 4f32a4ff071b20dac72ad6c3b19ed4b98c09c66d
sentiment analysis on monolingual , multilingual and code - switching twitter corpora we address the problem of performing polarity classification on twitter over different languages , focusing on english and spanish , comparing three techniques : ( 1 ) a monolingual model which knows the language in which the opinion is written , ( 2 ) a monolingual model that acts based on the decision provided by a language identification tool and ( 3 ) a multilingual model trained on a multilingual dataset that does not need any language recognition step . results show that multilingual models are even able to outperform the monolingual models on some monolingual sets . we introduce the first code - switching corpus with sentiment labels , showing the robustness of a multilingual approach .
RANK = 15; score = 0.17682926829268292; correct = False; id = e62a527aa26625980d89c89301989c6f4d53add7
umduluth - cs8761 - 12 : a novel machine learning approach for aspect based sentiment analysis this paper provides a detailed description of the approach of our system for the aspectbased sentiment analysis task of semeval2015 . the task is to identify the aspect category ( entity and attribute pair ) , opinion target and sentiment of the reviews . for the in - domain subtask that is provided with the training data , the system is developed using a supervised technique support vector machine and for the out - of - domain subtask for which the training data is not provided , it is implemented based on the sentiment score of the vocabulary . for in - domain subtask , our system is developed specifically for restaurant data .
RANK = 16; score = 0.17676767676767677; correct = False; id = 09884db5aa2a9d8889c6d38535f31eea999de7d5
a clustering approach for nearly unsupervised recognition of nonliteral language in this paper we present trofi ( trope finder ) , a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word - sense disambiguation and clustering techniques . trofi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies . it also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning . we adapt a word - sense disambiguation algorithm to our task and augment it with multiple seed set learners , a voting schema , and additional features like supertags and extrasentential context . detailed experiments on hand - annotated data show that our enhanced algorithm outperforms the baseline by 24.4 % . using the trofi algorithm , we also build the trofi example base , an extensible resource of annotated literal / nonliteral examples which is freely available to the nlp research community .
RANK = 17; score = 0.17676767676767677; correct = False; id = 78211d37b9043175fb654c7e888bf58af793f685
neural joint learning for classifying wikipedia articles into fine - grained named entity types this paper addresses the task of assigning finegrained ne type labels to wikipedia articles . to address the data sparseness problem , which is salient particularly in fine - grained type classification , we introduce a multi - task learning framework where type classifiers are all jointly learned by a neural network with a hidden layer . in addition , we also propose to learn article vectors ( i.e. entity embeddings ) from wikipedia ’s hypertext structure using a skipgram model and incorporate them into the input feature set . to conduct large - scale practical experiments , we created a new dataset containing over 22,000 manually labeled instances . the dataset is available . the results of our experiments show that both ideas gained their own statistically significant improvement separately in classification accuracy .
RANK = 18; score = 0.17582417582417584; correct = False; id = 262c33371e1bafce1a667142468923e5a45d5c4f
am - fm : a semantic framework for translation quality assessment this work introduces am - fm , a semantic framework for machine translation evaluation . based upon this framework , a new evaluation metric , which is able to operate without the need for reference translations , is implemented and evaluated . the metric is based on the concepts of adequacy and fluency , which are independently assessed by using a cross - language latent semantic indexing approach and an n - gram based language model approach , respectively . comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks ( overall quality assessment and comparative ranking ) over a large collection of human evaluations involving five european languages . finally , the main pros and cons of the proposed framework are discussed along with future research directions .
RANK = 19; score = 0.17582417582417584; correct = False; id = 251fa07ce6e9af6681b50e972b2c72159aaa4daa
on the feasibility of open domain referring expression generation using large scale folksonomies generating referring expressions has received considerable attention in natural language generation . in recent years we start seeing deployments of referring expression generators moving away from limited domains with custom - made ontologies . in this work , we explore the feasibility of using large scale noisy ontologies ( folksonomies ) for open domain referring expression generation , an important task for summarization by re - generation . our experiments on a fully annotated anaphora resolution training set and a larger , volunteersubmitted news corpus show that existing algorithms are efficient enough to deal with large scale ontologies but need to be extended to deal with undefined values and some measure for information salience .
RANK = 20; score = 0.17582417582417584; correct = False; id = 4cc4898e86c4e9d2a87bbb26f18ec6ebb79e9ded
automatic labeling of topic models using text summaries labeling topics learned by topic models is a challenging problem . previous studies have used words , phrases and images to label topics . in this paper , we propose to use text summaries for topic labeling . several sentences are extracted from the most related documents to form the summary for each topic . in order to obtain summaries with both high relevance , coverage and discrimination for all the topics , we propose an algorithm based on submodular optimization . both automatic and manual analysis have been conducted on two real document collections , and we find 1 ) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods ; 2 ) the use of summaries as labels has obvious advantages over the use of words and phrases .

RANKING 1166
QUERY
cross - lingual cross - platform rumor verification pivoting on multimedia content with the increasing popularity of smart devices , rumors with multimedia content become more and more common on social networks . the multimedia information usually makes rumors look more convincing . therefore , finding an automatic approach to verify rumors with multimedia content is a pressing task . previous rumor verification research only utilizes multimedia as input features . we propose not to use the multimedia content but to find external information in other news platforms pivoting on it . we introduce a new features set , cross - lingual cross - platform features that leverage the semantic similarity between the rumors and the external information . when implemented , machine learning methods utilizing such features achieved the state - of - theart rumor verification results .
First cited at 2509
TOP CITED PAPERS
RANK 2509
rumor has it : identifying misinformation in microblogs a rumor is commonly defined as a statement whose true value is unverifiable . rumors may spread misinformation ( false information ) or disinformation ( deliberately false information ) on a network of people . identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority . in this paper , we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features : content - based , network - based , and microblog - specific memes for correctly identifying rumors . moreover , we show how these features are also effective in identifying disinformers , users who endorse a rumor and further help it to spread . we perform our experiments on more than 10,000 manually annotated tweets collected from twitter and show how our retrieval model achieves more than 0.95 in mean average precision ( map ) . finally , we believe that our dataset is the first large - scale dataset on rumor detection . it can open new dimensions in analyzing online misinformation and other aspects of microblog conversations .
RANK 6148
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
TOP UNCITED PAPERS
RANK 1
a machine learning approach to automatic term extraction using a rich feature set in this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms . in our preliminary experiments , we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction . we achieved state of the art results for unigram extraction in brazilian portuguese .
RANK 2
set expansion using sibling relations between semantic categories most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories . however , in the setting of set expansion with multiple semantic categories , we might leverage other types of prior knowledge about semantic categories . in this paper , we present a method of set expansion when ontological information related to target semantic categories is available . more specifically , the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge . we demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from wikipedia in a semi - automatic manner .
RANK 3
disease mention recognition with specific features despite an increasing amount of research on biomedical named entity recognition , there has been not enough work done on disease mention recognition . difficulty of obtaining adequate corpora is one of the key reasons which hindered this particular research . previous studies argue that correct identification of disease mentions is the key issue for further improvement of the disease - centric knowledge extraction tasks . in this paper , we present a machine learning based approach that uses a feature set tailored for disease mention recognition and outperforms the state - ofthe - art results . the paper also discusses why a feature set for the well studied gene / protein mention recognition task is not necessarily equally effective for other biomedical semantic types such as diseases .
TOP 20
RANK = 1; score = 0.2222222222222222; correct = False; id = aae01fd0efab4ec60d92eb7233d10bd573ef67e8
a machine learning approach to automatic term extraction using a rich feature set in this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms . in our preliminary experiments , we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction . we achieved state of the art results for unigram extraction in brazilian portuguese .
RANK = 2; score = 0.216; correct = False; id = 4b7b7d83a4d787397b3f98c7a4bda9a42e85d70b
set expansion using sibling relations between semantic categories most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories . however , in the setting of set expansion with multiple semantic categories , we might leverage other types of prior knowledge about semantic categories . in this paper , we present a method of set expansion when ontological information related to target semantic categories is available . more specifically , the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge . we demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from wikipedia in a semi - automatic manner .
RANK = 3; score = 0.2127659574468085; correct = False; id = b574ec199b2b1a7a18d856a37ae8862db644465f
disease mention recognition with specific features despite an increasing amount of research on biomedical named entity recognition , there has been not enough work done on disease mention recognition . difficulty of obtaining adequate corpora is one of the key reasons which hindered this particular research . previous studies argue that correct identification of disease mentions is the key issue for further improvement of the disease - centric knowledge extraction tasks . in this paper , we present a machine learning based approach that uses a feature set tailored for disease mention recognition and outperforms the state - ofthe - art results . the paper also discusses why a feature set for the well studied gene / protein mention recognition task is not necessarily equally effective for other biomedical semantic types such as diseases .
RANK = 4; score = 0.2109375; correct = False; id = 7a760caa4acd96f8e7a251239d8d0dfd09a779fa
casict - dcu participation in wmt2015 metrics task human - designed sub - structures are required by most of the syntax - based machine translation evaluation metrics . in this paper , we propose a novel evaluation metric based on dependency parsing model , which does not need this human involvement . experimental results show that the new single metric gets better correlation than meteor on system level and is comparable with it on sentence level . to introduce more information , we combine the new metric with many other metrics . the combined metric obtains state - of - theart performance on both system level evaluation and sentence level evaluation on wmt 2014 .
RANK = 5; score = 0.20833333333333334; correct = False; id = 1272fab500efd7c2585cdf5f59e4dbe704047daf
integrating knowledge for subjectivity sense labeling this paper introduces an integrative approach to automatic word sense subjectivity annotation . we use features that exploit the hierarchical structure and domain information in lexical resources such as wordnet , as well as other types of features that measure the similarity of glosses and the overlap among sets of semantically related words . integrated in a machine learning framework , the entire set of features is found to give better results than any individual type of feature .
RANK = 6; score = 0.2076923076923077; correct = False; id = 4101e922b788c4d60560b46d3cd9903f9559a81d
dependency - based empty category detection via phrase structure trees we describe a novel approach to detecting empty categories ( ec ) as represented in dependency trees as well as a new metric for measuring ec detection accuracy . the new metric takes into account not only the position and type of an ec , but also the head it is a dependent of in a dependency tree . we also introduce a variety of new features that are more suited for this approach . tested on a subset of the chinese treebank , our system improved significantly over the best previously reported results even when evaluated with this more stringent metric .
RANK = 7; score = 0.2074074074074074; correct = False; id = d03690b92408fbddcedfeeb6bdfcf9c7bcb70025
identification of coreference between names and faces to retrieve multimedia contents by their meaning , it is necessary to use not only the contents of distinct media , such as image or language , but also a certain semantic relation holding between them . for this purpose , in this paper , we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph . the method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces . our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media .
RANK = 8; score = 0.2074074074074074; correct = False; id = 9d83b17a61aa12968091ff2cf2a9a7823d9feaf4
automatic detection of cognates using orthographic alignment words undergo various changes when entering new languages . based on the assumption that these linguistic changes follow certain rules , we propose a method for automatically detecting pairs of cognates employing an orthographic alignment method which proved relevant for sequence alignment in computational biology . we use aligned subsequences as features for machine learning algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non - cognates . given a list of known cognates , our approach does not require any other linguistic information . however , it can be customized to integrate historical information regarding language evolution .
RANK = 9; score = 0.2037037037037037; correct = False; id = 1db4032ba2bca654de2793d20540df8e06a5aaeb
sentence dependency tagging in online question answering forums online forums are becoming a popular resource in the state of the art question answering ( qa ) systems . because of its nature as an online community , it contains more updated knowledge than other places . however , going through tedious and redundant posts to look for answers could be very time consuming . most prior work focused on extracting only question answering sentences from user conversations . in this paper , we introduce the task of sentence dependency tagging . finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations . we use linear - chain conditional random fields ( crf ) for sentence type tagging , and a 2d crf to label the dependency relation between sentences . our experimental results show that our proposed approach performs well for sentence dependency tagging . this dependency information can benefit other tasks such as thread ranking and answer summarization in online forums .
RANK = 10; score = 0.20353982300884957; correct = False; id = 387926e1812088a5558e8a1be59a93761ed4fe50
summarizing spoken and written conversations in this paper we describe research on summarizing conversations in the meetings and emails domains . we introduce a conversation summarization system that works in multiple domains utilizing general conversational features , and compare our results with domain - dependent systems for meeting and email data . we find that by treating meetings and emails as conversations with general conversational features in common , we can achieve competitive results with state - of - theart systems that rely on more domain - specific features .
RANK = 11; score = 0.203125; correct = False; id = 50c651e9f94f9d4927a726af0ef44818179d87da
semantic parsing as machine translation semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance . here we approach it as a straightforward machine translation task , and demonstrate that standard machine translation components can be adapted into a semantic parser . in experiments on the multilingual geoquery corpus we find that our parser is competitive with the state of the art , and in some cases achieves higher accuracy than recently proposed purpose - built systems . these results support the use of machine translation methods as an informative baseline in semantic parsing evaluations , and suggest that research in semantic parsing could benefit from advances in machine translation .
RANK = 12; score = 0.20300751879699247; correct = False; id = 8528325153c698cb0cac47b823eda099eda074ae
an automatic filter for non - parallel texts numerous cross - lingual applications , including state - of - the - art machine translation systems , require parallel texts aligned at the sentence level . however , collections of such texts are often polluted by pairs of texts that are comparable but not parallel . bitext maps can help to discriminate between parallel and comparable texts . bitext mapping algorithms use a larger set of document features than competing approaches to this task , resulting in higher accuracy . in addition , good bitext mapping algorithms are not limited to documents with structural mark - up such as web pages . the task of filtering non - parallel text pairs represents a new application of bitext mapping algorithms .
RANK = 13; score = 0.20300751879699247; correct = False; id = b02120f7656b9717c2c4aca60e0be212b9eb3a3a
automatic collocation suggestion in academic writing in recent years , collocation has been widely acknowledged as an essential characteristic to distinguish native speakers from non - native speakers . research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community . in our study , we propose a machine learning approach to implementing an online collocation writing assistant . we use a data - driven classifier to provide collocation suggestions to improve word choices , based on the result of classification . the system generates and ranks suggestions to assist learners’ collocation usages in their academic writing with satisfactory results . *
RANK = 14; score = 0.20161290322580644; correct = False; id = ff20f2c56d3f57c679c33174ab20c0260b3654a2
unsupervised extractive summarization via coverage maximization with syntactic and semantic concepts coverage maximization with bigram concepts is a state - of - the - art approach to unsupervised extractive summarization . it has been argued that such concepts are adequate and , in contrast to more linguistic concepts such as named entities or syntactic dependencies , more robust , since they do not rely on automatic processing . in this paper , we show that while this seems to be the case for a commonly used newswire dataset , use of syntactic and semantic concepts leads to significant improvements in performance in other domains .
RANK = 15; score = 0.20161290322580644; correct = False; id = b9b220d2939366847fe55d92a32db8c245ec2c92
an entity - focused approach to generating company descriptions finding quality descriptions on the web , such as those found in wikipedia articles , of newer companies can be difficult : search engines show many pages with varying relevance , while multi - document summarization algorithms find it difficult to distinguish between core facts and other information such as news stories . in this paper , we propose an entity - focused , hybrid generation approach to automatically produce descriptions of previously unseen companies , and show that it outperforms a strong summarization baseline .
RANK = 16; score = 0.2013888888888889; correct = False; id = a27e243d2ef62644e7a2a1fa51878fe7dbca4479
learning term embeddings for taxonomic relation identification using dynamic weighting neural network taxonomic relation identification aims to recognize the ‘ is - a’ relation between two terms . previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches , but the accuracy of these approaches is far from satisfactory . in this paper , we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings . for this purpose , we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms , but also the contextual information between them . we then apply such embeddings as features to identify taxonomic relations using a supervised method . the experimental results show that our proposed approach significantly outperforms other state - of - the - art methods by 9 % to 13 % in terms of accuracy for both general and specific domain datasets .
RANK = 17; score = 0.2; correct = False; id = 12ac2ba7fbcfeb0502aea2def38368e30c5b8dff
an etymological approach to cross - language orthographic similarity . application on romanian in this paper we propose a computational method for determining the orthographic similarity between romanian and related languages . we account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities . the method we propose is adaptable to any language , as far as resources are available .
RANK = 18; score = 0.19863013698630136; correct = False; id = c68ec24e2f97c6875cd006a9fa2f0fbe934e4ae0
bidirectional long short - term memory networks for relation classification relation classification is an important semantic processing , which has achieved great attention in recent years . the main challenge is the fact that important information can appear at any position in the sentence . therefore , we propose bidirectional long short - term memory networks ( blstm ) to model the sentence with complete , sequential information about all words . at the same time , we also use features derived from the lexical resources such as wordnet or nlp systems such as dependency parser and named entity recognizers ( ner ) . the experimental results on semeval-2010 show that blstmbased method only with word embeddings as input features is sufficient to achieve state - of - the - art performance , and importing more features could further improve the performance .
RANK = 19; score = 0.19852941176470587; correct = False; id = 8b302ae180547430d9201aae1cbde5e0852a623b
extracting definitions and hypernym relations relying on syntactic dependencies and support vector machines in this paper we present a technique to reveal definitions and hypernym relations from text . instead of using pattern matching methods that rely on lexico - syntactic patterns , we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser . the assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences . afterwards , we transform such syntactic contexts in abstract representations , that are then fed into a support vector machine classifier . the results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state - of - the - art techniques .
RANK = 20; score = 0.19708029197080293; correct = False; id = 3ccf1dad8c4422fdff56b695fd8b20fcbe0887a7
a convex and feature - rich discriminative approach to dependency grammar induction in this paper , we introduce a new method for the problem of unsupervised dependency parsing . most current approaches are based on generative models . learning the parameters of such models relies on solving a non - convex optimization problem , thus making them sensitive to initialization . we propose a new convex formulation to the task of dependency grammar induction . our approach is discriminative , allowing the use of different kinds of features . we describe an efficient optimization algorithm to learn the parameters of our model , based on the frank - wolfe algorithm . our method can easily be generalized to other unsupervised learning problems . we evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state - of - the - art methods .

