RANKING 2148
QUERY
alime chat : a sequence to sequence and rerank based chatbot engine we propose alime chat , an open - domain chatbot engine that integrates the joint results of information retrieval ( ir ) and sequence to sequence ( seq2seq ) based generation models . alime chat uses an attentive seq2seq based rerank model to optimize the joint results . extensive experiments show our engine outperforms both ir and generation based models . we launch alime chat for a real - world industrial application and observe better results than another public chatbot .
First cited at 27
TOP CITED PAPERS
RANK 27
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK 1601
data - driven response generation in social media we present a data - driven approach to generating responses to twitter status posts , based on phrase - based statistical machine translation . we find that mapping conversational stimuli onto responses is more difficult than translating between languages , due to the wider range of possible responses , the larger fraction of unaligned words / phrases , and the presence of large phrase pairs whose alignment can not be further decomposed . after addressing these challenges , we compare approaches based on smt and information retrieval in a human evaluation . we show that smt outperforms ir on this task , and its output is preferred over actual human responses in 15 % of cases . as far as we are aware , this is the first work to investigate the use of phrase - based smt to directly translate a linguistic stimulus into an appropriate response .
RANK 3165
a neural network approach to context - sensitive generation of conversational responses we present a novel response generation system that can be trained end to end on large quantities of unstructured twitter conversations . a neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models , allowing the system to take into account previous dialog utterances . our dynamic - context generative models show consistent gains over both context - sensitive and non - context - sensitive machine translation and information retrieval baselines .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 3
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
TOP 20
RANK = 1; score = 0.9926968216896057; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9918296933174133; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 3; score = 0.9910869002342224; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 4; score = 0.9905099272727966; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 5; score = 0.989809513092041; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9880164265632629; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 7; score = 0.9855544567108154; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 8; score = 0.9850191473960876; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 9; score = 0.9849649667739868; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 10; score = 0.9849328398704529; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 11; score = 0.983614981174469; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 12; score = 0.9832800626754761; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 13; score = 0.9820037484169006; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 14; score = 0.9805077910423279; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 15; score = 0.9803277850151062; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 16; score = 0.9796431660652161; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 17; score = 0.9792550206184387; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 18; score = 0.9771270155906677; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 19; score = 0.9763888716697693; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 20; score = 0.9753394722938538; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .

RANKING 1161
QUERY
learning to negate adjectives with bilinear models we learn a mapping that negates adjectives by predicting an adjective ’s antonym in an arbitrary word embedding model . we show that both linear models and neural networks improve on this task when they have access to a vector representing the semantic domain of the input word , e.g. a centroid of temperature words when predicting the antonym of ‘ cold’ . we introduce a continuous class - conditional bilinear neural network which is able to negate adjectives with high precision .
First cited at 425
TOP CITED PAPERS
RANK 425
multi - relational latent semantic analysis we present multi - relational latent semantic analysis ( mrlsa ) which generalizes latent semantic analysis ( lsa ) . mrlsa provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor . similar to lsa , a lowrank approximation of the tensor is derived using a tensor decomposition . each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix . the degree of two words having a specific relation can then be measured through simple linear algebraic operations . we demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources , mrlsa achieves stateof - the - art performance on existing benchmark datasets for two relations , antonymy and is - a .
RANK 2442
integrating distributional lexical contrast into word embeddings for antonym - synonym distinction we propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity . the improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66–0.76 across word classes ( adjectives , nouns , verbs ) . moreover , we integrate the lexical contrast vectors into the objective function of a skip - gram model . the novel embedding outperforms state - of - the - art models on predicting word similarities in simlex999 , and on distinguishing antonyms from synonyms .
RANK 2871
a multitask objective to inject lexical contrast into distributional semantics distributional semantic models have trouble distinguishing strongly contrasting words ( such as antonyms ) from highly compatible ones ( such as synonyms ) , because both kinds tend to occur in similar contexts in corpora . we introduce the multitask lexical contrast model ( mlcm ) , an extension of the effective skip - gram method that optimizes semantic vectors on the joint tasks of predicting corpus contexts and making the representations of wordnet synonyms closer than that of matching wordnet antonyms . mlcm outperforms skip - gram both on general semantic tasks and on synonym / antonym discrimination , even when no direct lexical contrast information about the test words is provided during training . mlcm also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
TOP 20
RANK = 1; score = 0.9979602098464966; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.997499406337738; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.9970126152038574; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 4; score = 0.9969762563705444; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 5; score = 0.9967761635780334; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 6; score = 0.9966505169868469; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 7; score = 0.9964410662651062; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 8; score = 0.9963791966438293; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 9; score = 0.9962542057037354; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 10; score = 0.9961555600166321; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 11; score = 0.9959736466407776; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 12; score = 0.995947539806366; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 13; score = 0.9958359003067017; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 14; score = 0.9956749081611633; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 15; score = 0.9954531192779541; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 16; score = 0.9954493641853333; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 17; score = 0.9953979849815369; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 18; score = 0.99524986743927; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 19; score = 0.9951955676078796; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 20; score = 0.9950323104858398; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112

RANKING 982
QUERY
myanmar number normalization for text - to - speech --text normalization is an essential module for text - to - speech ( tts ) system as tts systems need to work on real text . this paper describes myanmar number normalization designed for myanmar text - to - speech system . semiotic classes for myanmar language are identified by the study of myanmar text corpus and weighted finite state transducers ( wfst ) based myanmar number normalization is implemented . number suffixes and prefixes are also applied for token classification and finally , postprocessing has been done for tokens that can not be classified . this approach achieves average tag accuracy of 93.5 % for classification phase and average word error rate ( wer ) 0.95 % for overall performance which is 5.65 % lower than rule - based system . the results show that this approach can be used in myanmar tts system and to our knowledge , this is the first published work of myanmar number normalization system designed for myanmar tts system . keywords - myanmar number normalization ; text normalization ; weighted finite state transducer ; myanmar text - to - speech ; myanmar ;
First cited at 561
TOP CITED PAPERS
RANK 561
the opengrm open - source finite - state grammar software libraries in this paper , we present a new collection of open - source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context - sensitive rewrite rules into finite - state transducers , and for n - gram language modeling . the opengrm libraries use the openfst library to provide an efficient encoding of grammars and general algorithms for building , modifying and applying models .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
TOP 20
RANK = 1; score = 0.9918671250343323; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9871693849563599; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.98121577501297; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 4; score = 0.980556845664978; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 5; score = 0.9757887721061707; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9734947085380554; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 7; score = 0.9720230102539062; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 8; score = 0.9719488620758057; correct = False; id = c9214ebe91454e6369720136ab7dd990d52a07d4
improved statistical alignment models 
RANK = 9; score = 0.9695674180984497; correct = False; id = 11aedb8f95a007363017dae311fc525f67bd7876
minimum error rate training in statistical machine translation often , the training procedure for statistical machine translation models is based on maximum likelihood or related criteria . a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text . in this paper , we analyze various training criteria which directly optimize translation quality . these training criteria make use of recently proposed automatic evaluation metrics . we describe a new algorithm for efficient training an unsmoothed error count . we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure .
RANK = 10; score = 0.9666515588760376; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 11; score = 0.9642797708511353; correct = False; id = 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
moses : open source toolkit for statistical machine translation we describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models . in addition to the smt decoder , the toolkit also includes a wide variety of tools for training , tuning and applying the system to many translation tasks .
RANK = 12; score = 0.961250364780426; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 13; score = 0.953082263469696; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 14; score = 0.9514397978782654; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 15; score = 0.9479326605796814; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 16; score = 0.9453967213630676; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 17; score = 0.9425742030143738; correct = False; id = 60f4f98ff57be60a786803a88f5e7e970b35c79e
re - evaluating the role of bleu in machine translation research we argue that the machine translation community is overly reliant on the bleu machine translation evaluation metric . we show that an improved bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to bleu ’s correlation with human judgments of quality . this offers new potential for research which was previously deemed unpromising by an inability to improve upon bleu scores .
RANK = 18; score = 0.941476047039032; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 19; score = 0.9410512447357178; correct = False; id = 5b31e43f8b0490779d8013e0705ae4df8f0488c9
shallow parsing with conditional random fields conditional random fields for sequence labeling offer advantages over both generative models like hmms and classifiers applied at each sequence position . among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods . we show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the conll task , and better than any reported single model . improved training methods based on modern optimization algorithms were critical in achieving these results . we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models .
RANK = 20; score = 0.935809850692749; correct = False; id = 225f7d72eacdd136b0ceb0a522e3a3930c5af9b8
automatic identification of word translations from unrelated english and german corpora algorithms for the alignment of words in translated texts are w ell established . however , only recently , new approaches have been proposed to identify word translations f rom non - parallel or even unrelated texts . this task is more difficult , because most statistical clues useful in the processing of parallel texts can not be applied for non - parallel tex ts . for this reason , whereas for parallel texts in some studies up to 99 % of the word alignments have been shown to be correct , the accuracy for non - parallel texts has been around 30 % up to now . the current study , which is based on the assumption that there is a correlation between the patterns of word cooccurrences in corpora of different languages , makes a significant improvement to about 72 % of word translations identified correctly .

RANKING 2525
QUERY
fact : a framework for authentication in cloud - based ip traceback ip traceback plays an important role in cyber investigation processes , where the sources and the traversed paths of packets need to be identified . it has a wide range of applications , including network forensics , security auditing , network fault diagnosis , and performance testing . despite a plethora of research on ip traceback , the internet is yet to see a large - scale practical deployment of traceback . some of the major challenges that still impede an internet - scale traceback solution are , concern of disclosing internet service provider ( isp ’s ) internal network topologies ( in other words , concern of privacy leak ) , poor incremental deployment , and lack of incentives for isps to provide traceback services . in this paper , we argue that cloud services offer better options for the practical deployment of an ip traceback system . we first present a novel cloud - based traceback architecture , which possesses several favorable properties encouraging isps to deploy traceback services on their networks . while this makes the traceback service more accessible , regulating access to traceback service in a cloud - based architecture becomes an important issue . consequently , we address the access control problem in cloud - based traceback . our design objective is to prevent illegitimate users from requesting traceback information for malicious intentions ( such as isps topology discovery ) . to this end , we propose a temporal token - based authentication framework , called fact , for authenticating traceback service queries . fact embeds temporal access tokens in traffic flows , and then delivers them to end - hosts in an efficient manner . the proposed solution ensures that the entity requesting for traceback service is an actual recipient of the packets to be traced . finally , we analyze and validate the proposed design using real - world internet data sets .
First cited at 74
TOP CITED PAPERS
RANK 74
large - scale ip traceback in high - speed internet : practical techniques and theoretical foundation tracing attack packets to their sources , known as ip traceback , is an important step to counter distributed denial - of - service ( ddos ) attacks . in this paper , we propose a novel packet logging based ( i.e. , hash - based ) traceback scheme that requires an order of magnitude smaller processing and storage cost than the hash - based scheme proposed by snoeren , et al . ( 2001 ) , thereby being able to scalable to much higher link speed ( e.g. , oc-768 ) . the baseline idea of our approach is to sample and log a small percentage ( e.g. , 3.3 % ) of packets . the challenge of this low sampling rate is that much more sophisticated techniques need to be used for traceback . our solution is to construct the attack tree using the correlation between the attack packets sampled by neighboring routers . the scheme using naive independent random sampling does not perform well due to the low correlation between the packets sampled by neighboring routers . we invent a sampling scheme that improves this correlation and the overall efficiency significantly . another major contribution of this work is that we introduce a novel information - theoretic framework for our traceback scheme to answer important questions on system parameter tuning and the fundamental trade - off between the resource used for traceback and the traceback accuracy . simulation results based on real - world network topologies ( e.g. skitter ) match very well with results from the information - theoretic analysis . the simulation results also demonstrate that our traceback scheme can achieve high accuracy , and scale very well to a large number of attackers ( e.g. , 5000 + ) .
RANK 636
passive ip traceback : disclosing the locations of ip spoofers from path backscatter it is long known attackers may use forged source ip address to conceal their real locations . to capture the spoofers , a number of ip traceback mechanisms have been proposed . however , due to the challenges of deployment , there has been not a widely adopted ip traceback solution , at least at the internet level . as a result , the mist on the locations of spoofers has never been dissipated till now . this paper proposes passive ip traceback ( pit ) that bypasses the deployment difficulties of ip traceback techniques . pit investigates internet control message protocol error messages ( named path backscatter ) triggered by spoofing traffic , and tracks the spoofers based on public available information ( e.g. , topology ) . in this way , pit can find the spoofers without any deployment requirement . this paper illustrates the causes , collection , and the statistical results on path backscatter , demonstrates the processes and effectiveness of pit , and shows the captured locations of spoofers through applying pit on the path backscatter data set . these results can help further reveal ip spoofing , which has been studied for long but never well understood . though pit can not work in all the spoofing attacks , it may be the most useful mechanism to trace spoofers before an internet - level traceback system has been deployed in real .
RANK 676
pi : a path identification mechanism to defend against ddos attack distributed denial of service ( ddos ) attacks continue to plague the internet . defense against these attacks is complicated by spoofed source ip addresses , which make it difficult to determine a packet ’s true origin . we propose pi ( short for path identifier ) , a new packet marking approach in which a path fingerprint is embedded in each packet , enabling a victim to identify packets traversing the same path s through the internet on a per packet basis , regardless of source ip address spoofing . pi features many unique properties . it is a per - packet deterministic mechanism : each packet traveling along the same path carries the same identifier . this allows the victim to take a proactive role in defending against a ddos attack by using the pi mark to filter out packets matching the attackers’ identifiers on a per packet basis . the pi scheme performs well under large - scale ddos attacks consisting of thousands of attackers , and is effective even when only half the routers in the internet participate in packet marking . p i marking and filtering are both extremely light - weight and require negligible state . we use traceroute maps of real internet topologies ( e.g. , caida ’s skitter [ 6 ] and burch and cheswick ’s internet map [ 4 , 14 ] ) to simulate ddos attacks and validate our design .
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
native client : a sandbox for portable , untrusted x86 native code 
RANK 3
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
TOP 20
RANK = 1; score = 0.9614806175231934; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.9603321552276611; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 3; score = 0.9569208025932312; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 4; score = 0.956864595413208; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 5; score = 0.9567793607711792; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 6; score = 0.9567638635635376; correct = False; id = 2f95e2ca11610cb334d8d777d7b0f0d5561e67bc
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK = 7; score = 0.9565989971160889; correct = False; id = 660ad810c69affa189f567e76ff83af682228703
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
RANK = 8; score = 0.9564465284347534; correct = False; id = 0c5de0e5cb46e862b933c6bd543cc15695506034
automatic patch - based exploit generation is possible : techniques and implications the automatic patch - based exploit generation problem is : given a program p and a patched version of the program p ' , automatically generate an exploit for the potentially unknown vulnerability present in p but fixed in p ' . in this paper , we propose techniques for automatic patch - based exploit generation , and show that our techniques can automatically generate exploits for 5 microsoft programs based upon patches provided via windows update . although our techniques may not work in all cases , a fundamental tenant of security is to conservatively estimate the capabilities of attackers . thus , our results indicate that automatic patch - based exploit generation should be considered practical . one important security implication of our results is that current patch distribution schemes which stagger patch distribution over long time periods , such as windows update , may allow attackers who receive the patch first to compromise the significant fraction of vulnerable hosts who have not yet received the patch .
RANK = 9; score = 0.9561012983322144; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 10; score = 0.9560192227363586; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 11; score = 0.9557913541793823; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 12; score = 0.9556566476821899; correct = False; id = 67f961f98d34fea3ab15f473429a5156b62b5c65
vigilare : toward snoop - based kernel integrity monitor in this paper , we present vigilare system , a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware . this snoop - based monitoring enabled by the vigilare system , overcomes the limitations of the snapshot - based monitoring employed in previous kernel integrity monitoring solutions . being based on inspecting snapshots collected over a certain interval , the previous hardware - based monitoring solutions can not detect transient attacks that can occur in between snapshots . we implemented a prototype of the vigilare system on gaisler 's grlib - based system - on - a - chip ( soc ) by adding snooper hardware connections module to the host system for bus snooping . to evaluate the benefit of snoop - based monitoring , we also implemented similar soc with a snapshot - based monitor to be compared with . the vigilare system detected all the transient attacks without performance degradation while the snapshot - based monitor could not detect all the attacks and induced considerable performance degradation as much as 10 % in our tuned stream benchmark test .
RANK = 13; score = 0.9543132185935974; correct = False; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 14; score = 0.9541480541229248; correct = False; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 15; score = 0.9538689851760864; correct = False; id = 4681a0116597fd0804b07e8176b8761e4f569743
password cracking using probabilistic context - free grammars choosing the most effective word - mangling rules to use when performing a dictionary - based password cracking attack can be a difficult task . in this paper we discuss a new method that generates password structures in highest probability order . we first automatically create a probabilistic context - free grammar based upon a training set of previously disclosed passwords . this grammar then allows us to generate word - mangling rules , and from them , password guesses to be used in password cracking . we will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets . in one series of experiments , training on a set of disclosed passwords , our approach was able to crack 28 % to 129 % more passwords than john the ripper , a publicly available standard password cracking program .
RANK = 16; score = 0.9537506699562073; correct = False; id = 0dfe28b14b7c35fc1b876954a09ff6b7e47a8ee9
openconflict : preventing real time map hacks in online games we present a generic tool , kartograph , that lifts the fog of war in online real - time strategy games by snooping on the memory used by the game . kartograph is passive and can not be detected remotely . motivated by these passive attacks , we present secure protocols for distributing game state among players so that each client only has data it is allowed to see . our system , open conflict , runs real - time games with distributed state . to support our claim that open conflict is sufficiently fast for real - time strategy games , we show the results of an extensive study of 1000 replays of star craft ii games between expert players . at the peak of a typical game , open conflict needs only 22 milliseconds on one cpu core each time state is synchronized .
RANK = 17; score = 0.9536125063896179; correct = False; id = 0fd2467de521b52805eea902edc9587c87818276
discoverer : automatic protocol reverse engineering from network traces application - level protocol specifications are useful for many security applications , including intrusion prevention and detection that performs deep packet inspection and traffic normalization , and penetration testing that generates network inputs to an application to uncover potential vulnerabilities . however , current practice in deriving protocol specifications is mostly manual . in this paper , we present discoverer , a tool for automatically reverse engineering the protocol message formats of an application from its network trace . a key property of discoverer is that it operates in a protocol - independent fashion by inferring protocol idioms commonly seen in message formats of many application - level protocols . we evaluated the efficacy of discoverer over one text protocol ( http ) and two binary protocols ( rpc and cifs / smb ) by comparing our inferred formats with true formats obtained from ethereal [ 5 ] . for all three protocols , more than 90 % of our inferred formats correspond to exactly one true format ; one true format is reflected in five inferred formats on average ; our inferred formats cover over 95 % of messages , which belong to 30 - 40 % of true formats observed in the trace .
RANK = 18; score = 0.9535154104232788; correct = False; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
RANK = 19; score = 0.953281044960022; correct = False; id = 2f7b92282d42d645ee2d4bc34aa2bf132275e82b
libfte : a toolkit for constructing practical , format - abiding encryption schemes encryption schemes where the ciphertext must abide by a specified format have diverse applications , ranging from in - place encryption in databases to per - message encryption of network traffic for censorship circumvention . despite this , a unifying framework for deploying such encryption schemes has not been developed . one consequence of this is that current schemes are ad - hoc ; another is a requirement for expert knowledge that can disuade one from using encryption at all . we present a general - purpose library ( called libfte ) that aids engineers in the development and deployment of format - preserving encryption ( fpe ) and formattransforming encryption ( fte ) schemes . it incorporates a new algorithmic approach for performing fpe / fte using the nondeterministic finite - state automata ( nfa ) representation of a regular expression when specifying formats . this approach was previously considered unworkable , and our approach closes this open problem . we evaluate libfte and show that , compared to other encryption solutions , it introduces negligible latency overhead , and can decrease diskspace usage by as much as 62.5 % when used for simultaneous encryption and compression in a postgresql database ( both relative to conventional encryption mechanisms ) . in the censorship circumvention setting we show that , using regularexpression formats lifted from the snort ids , libfte can reduce client / server memory requirements by as much as 30 % .
RANK = 20; score = 0.9532734155654907; correct = False; id = 0a5eacf219767f05c35f509b3e37c1ae75ae107b
using programmer - written compiler extensions to catch security holes this paper shows how system speci c static analysis can nd security errors that violate rules such as in tegers from untrusted sources must be sanitized before use and do not dereference user supplied pointers in our approach programmers write system speci c extensions that are linked into the compiler and check their code for errors we demonstrate the approach s e ectiveness by using it to nd over security er rors in linux and openbsd over of which have led to kernel patches an unusual feature of our ap proach is the use of methods to automatically detect when we miss code actions that should be checked

RANKING 275
QUERY
these are not the stereotypes you are looking for : bias and fairness in authorial gender attribution stylometric and text categorization results show that author gender can be discerned in texts with relatively high accuracy . however , it is difficult to explain what gives rise to these results and there are many possible confounding factors , such as the domain , genre , and target audience of a text . more fundamentally , such classification efforts risk invoking stereotyping and essentialism . we explore this issue in two datasets of dutch literary novels , using commonly used descriptive ( liwc , topic modeling ) and predictive ( machine learning ) methods . our results show the importance of controlling for variables in the corpus and we argue for taking care not to overgeneralize from the results .
First cited at 443
TOP CITED PAPERS
RANK 443
from once upon a time to happily ever after : tracking emotions in novels and fairy tales today we have access to unprecedented amounts of literary texts . however , search still relies heavily on key words . in this paper , we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in both individual books and across very large collections . we introduce the concept of emotion word density , and using the brothers grimm fairy tales as example , we show how collections of text can be organized for better search . using the google books corpus we show how to determine an entity ’s emotion associations from cooccurring words . finally , we compare emotion words in fairy tales and novels , to show that fairy tales have a much wider range of emotion word densities than novels .
RANK 3673
analyzing biases in human perception of user age and gender from text • how accurate are people in judging " traits of other users ? • are there systematic biases humans " are subject to ? • what are the implications of using " human perception as a proxy for truth ? • which textual cues lead to a false " perception of the truth ? • which textual cues make people " more or less confident in their ratings ? • gender 2,607 authors , age – 826 authors • we use 100 tweets per author , 9 mturk votes per author • urls and mentions anonymized , english only filtered , duplicates eliminated , same 6 month time interval gender perception
TOP UNCITED PAPERS
RANK 1
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 2
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK 3
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
TOP 20
RANK = 1; score = 0.994485080242157; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 2; score = 0.9935547709465027; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 3; score = 0.9932577610015869; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 4; score = 0.9927654266357422; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 5; score = 0.9925158619880676; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 6; score = 0.9914968609809875; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 7; score = 0.9914755821228027; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 8; score = 0.9913522005081177; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 9; score = 0.9912614822387695; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 10; score = 0.9910061955451965; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 11; score = 0.9898486733436584; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 12; score = 0.9889624714851379; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 13; score = 0.9885826110839844; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 14; score = 0.9877222180366516; correct = False; id = 303b0b6e6812c60944a4ac9914222ac28b0813a2
recognizing contextual polarity in phrase - level sentiment analysis this paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions . with this approach , the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions , achieving results that are significantly better than baseline .
RANK = 15; score = 0.9872549772262573; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 16; score = 0.9872243404388428; correct = False; id = 3cad59ed9faf5194c499ae8a3d57caece152360f
hadoopperceptron : a toolkit for distributed perceptron training and prediction with mapreduce we propose a set of open - source software modules to perform structured perceptron training , prediction and evaluation within the hadoop framework . apache hadoop is a freely available environment for running distributed applications on a computer cluster . the software is designed within the map - reduce paradigm . thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data - sets . the distributed perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial perceptron . the presented modules can be executed as stand - alone software or easily extended or integrated in complex systems . the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs .
RANK = 17; score = 0.9871713519096375; correct = False; id = 94db635f54d25bdb95edb42185aca93ba53b051b
biographies , bollywood , boom - boxes and blenders : domain adaptation for sentiment classification automatic sentiment classification has been extensively studied and applied in recent years . however , sentiment is expressed differently in different domains , and annotating corpora for every possible domain of interest is impractical . we investigate domain adaptation for sentiment classifiers , focusing on online reviews for different types of products . first , we extend to sentiment classification the recently - proposed structural correspondence learning ( scl ) algorithm , reducing the relative error due to adaptation between domains by an average of 30 % over the original scl algorithm and 46 % over a supervised baseline . second , we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another . this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains .
RANK = 18; score = 0.986842691898346; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 19; score = 0.9862992167472839; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 20; score = 0.9857478737831116; correct = False; id = 2ae6014a451801671d41b6171f86e657d8b1fbaf
wordnet : : similarity - measuring the relatedness of concepts wordnet::similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts ( or word senses ) . it provides six measures of similarity , and three measures of relatedness , all of which are based on the lexical database wordnet . these measures are implemented as perl modules which take as input two concepts , and return a numeric value that represents the degree to which they are similar or related .

RANKING 412
QUERY
gw_qa at semeval-2017 task 3 : question answer re - ranking on arabic fora this paper describes our submission to semeval-2017 task 3 subtask d , ” question answer ranking in arabic community question answering ” . in this work , we applied a supervised machine learning approach to automatically re - rank a set of qa pairs according to their relevance to a given question . we employ features based on latent semantic models , namely wtmf , as well as a set of lexical features based on string length and surface level matching . the proposed system ranked first out of 3 submissions , with a map score of 61.16 % .
First cited at 600
TOP CITED PAPERS
RANK 600
modeling sentences in the latent space sentence similarity is the process of computing a similarity score between two sentences . previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences . in this paper , we show that by carefully handling words that are not in the sentences ( missing words ) , we can train a reliable latent variable model on sentences . in the process , we propose a new evaluation framework for sentence similarity : concept definition retrieval . the new framework allows for large scale tuning and testing of sentence similarity models . experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models . our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity .
RANK 8321
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 8503
semeval-2016 task 3 : community question answering this paper describes the semeval–2016 task 3 on community question answering , which we offered in english and arabic . for english , we had three subtasks : question – comment similarity ( subtask a ) , question – question similarity ( b ) , and question – external comment similarity ( c ) . for arabic , we had another subtask : rerank the correct answers for a new question ( d ) . eighteen teams participated in the task , submitting a total of 95 runs ( 38 primary and 57 contrastive ) for the four subtasks . a variety of approaches and features were used by the participating systems to address the different subtasks , which are summarized in this paper . the best systems achieved an official score ( map ) of 79.19 , 76.70 , 55.41 , and 45.83 in subtasks a , b , c , and d , respectively . these scores are significantly better than those for the baselines that we provided . for subtask a , the best system improved over the 2015 winner by 3 points absolute in terms of accuracy .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
TOP 20
RANK = 1; score = 0.9976959824562073; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9972463846206665; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.996185839176178; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 4; score = 0.996126115322113; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 5; score = 0.9955183267593384; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9951737523078918; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 7; score = 0.9951326251029968; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 8; score = 0.9950120449066162; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 9; score = 0.994924783706665; correct = False; id = 11aedb8f95a007363017dae311fc525f67bd7876
minimum error rate training in statistical machine translation often , the training procedure for statistical machine translation models is based on maximum likelihood or related criteria . a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text . in this paper , we analyze various training criteria which directly optimize translation quality . these training criteria make use of recently proposed automatic evaluation metrics . we describe a new algorithm for efficient training an unsmoothed error count . we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure .
RANK = 10; score = 0.9948710203170776; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 11; score = 0.9943608641624451; correct = False; id = 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
moses : open source toolkit for statistical machine translation we describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models . in addition to the smt decoder , the toolkit also includes a wide variety of tools for training , tuning and applying the system to many translation tasks .
RANK = 12; score = 0.9943007230758667; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 13; score = 0.9942342638969421; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 14; score = 0.9940940737724304; correct = False; id = c9214ebe91454e6369720136ab7dd990d52a07d4
improved statistical alignment models 
RANK = 15; score = 0.9936967492103577; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 16; score = 0.9933653473854065; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 17; score = 0.9932017922401428; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 18; score = 0.993062436580658; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 19; score = 0.9926750063896179; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 20; score = 0.9925079941749573; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .

RANKING 515
QUERY
bootstomp : on the security of bootloaders in mobile devices modern mobile bootloaders play an important role in both the function and the security of the device . they help ensure the chain of trust ( cot ) , where each stage of the boot process verifies the integrity and origin of the following stage before executing it . this process , in theory , should be immune even to attackers gaining full control over the operating system , and should prevent persistent compromise of a device ’s cot. however , not only do these bootloaders necessarily need to take untrusted input from an attacker in control of the os in the process of performing their function , but also many of their verification steps can be disabled ( “ unlocked ” ) to allow for development and user customization . applying traditional analyses on bootloaders is problematic , as hardware dependencies hinder dynamic analysis , and the size , complexity , and opacity of the code involved preclude the usage of many previous techniques . in this paper , we explore vulnerabilities in both the design and implementation of mobile bootloaders . we examine bootloaders from four popular manufacturers , and discuss the standards and design principles that they strive to achieve . we then propose bootstomp , a multi - tag taint analysis resulting from a novel combination of static analyses and dynamic symbolic execution , designed to locate problematic areas where input from an attacker in control of the os can compromise the bootloader ’s execution , or its security features . using our tool , we find six previously - unknown vulnerabilities ( of which five have been confirmed by the respective vendors ) , as well as rediscover one that had been previouslyreported . some of these vulnerabilities would allow an attacker to execute arbitrary code as part of the bootloader ( thus compromising the entire chain of trust ) , or to perform permanent denial - of - service attacks . our tool also identified two bootloader vulnerabilities that can be leveraged by an attacker with root privileges on the os to unlock the device and break the cot. we conclude by proposing simple mitigation steps that can be implemented by manufacturers to safeguard the bootloader and os from all of the discovered attacks , using alreadydeployed hardware features .
First cited at 13
TOP CITED PAPERS
RANK 13
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK 159
dowsing for overflows : a guided fuzzer to find buffer boundary violations dowser is a ‘ guided’ fuzzer that combines taint tracking , program analysis and symbolic execution to find buffer overflow and underflow vulnerabilities buried deep in a program ’s logic . the key idea is that analysis of a program lets us pinpoint the right areas in the program code to probe and the appropriate inputs to do so . intuitively , for typical buffer overflows , we need consider only the code that accesses an array in a loop , rather than all possible instructions in the program . after finding all such candidate sets of instructions , we rank them according to an estimation of how likely they are to contain interesting vulnerabilities . we then subject the most promising sets to further testing . specifically , we first use taint analysis to determine which input bytes influence the array index and then execute the program symbolically , making only this set of inputs symbolic . by constantly steering the symbolic execution along branch outcomes most likely to lead to overflows , we were able to detect deep bugs in real programs ( like the nginx webserver , the inspircd irc server , and the ffmpeg videoplayer ) . two of the bugs we found were previously undocumented buffer overflows in ffmpeg and the poppler pdf rendering library .
RANK 328
unleashing mayhem on binary code in this paper we present mayhem , a new system for automatically finding exploitable bugs in binary ( i.e. , executable ) programs . every bug reported by mayhem is accompanied by a working shell - spawning exploit . the working exploits ensure soundness and that each bug report is security - critical and actionable . mayhem works on raw binary code without debugging information . to make exploit generation possible at the binary - level , mayhem addresses two major technical challenges : actively managing execution paths without exhausting memory , and reasoning about symbolic memory indices , where a load or a store address depends on user input . to this end , we propose two novel techniques : 1 ) hybrid symbolic execution for combining online and offline ( concolic ) execution to maximize the benefits of both techniques , and 2 ) index - based memory modeling , a technique that allows mayhem to efficiently reason about symbolic memory at the binary level . we used mayhem to find and demonstrate 29 exploitable vulnerabilities in both linux and windows programs , 2 of which were previously undocumented .
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK 3
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
TOP 20
RANK = 1; score = 0.9631795287132263; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.9609325528144836; correct = False; id = 2f95e2ca11610cb334d8d777d7b0f0d5561e67bc
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK = 3; score = 0.9608722925186157; correct = False; id = 660ad810c69affa189f567e76ff83af682228703
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
RANK = 4; score = 0.9607083797454834; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 5; score = 0.9606742858886719; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 6; score = 0.9606534242630005; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 7; score = 0.9605547189712524; correct = False; id = 0c5de0e5cb46e862b933c6bd543cc15695506034
automatic patch - based exploit generation is possible : techniques and implications the automatic patch - based exploit generation problem is : given a program p and a patched version of the program p ' , automatically generate an exploit for the potentially unknown vulnerability present in p but fixed in p ' . in this paper , we propose techniques for automatic patch - based exploit generation , and show that our techniques can automatically generate exploits for 5 microsoft programs based upon patches provided via windows update . although our techniques may not work in all cases , a fundamental tenant of security is to conservatively estimate the capabilities of attackers . thus , our results indicate that automatic patch - based exploit generation should be considered practical . one important security implication of our results is that current patch distribution schemes which stagger patch distribution over long time periods , such as windows update , may allow attackers who receive the patch first to compromise the significant fraction of vulnerable hosts who have not yet received the patch .
RANK = 8; score = 0.9601119160652161; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 9; score = 0.9601042866706848; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 10; score = 0.9600206017494202; correct = False; id = 67f961f98d34fea3ab15f473429a5156b62b5c65
vigilare : toward snoop - based kernel integrity monitor in this paper , we present vigilare system , a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware . this snoop - based monitoring enabled by the vigilare system , overcomes the limitations of the snapshot - based monitoring employed in previous kernel integrity monitoring solutions . being based on inspecting snapshots collected over a certain interval , the previous hardware - based monitoring solutions can not detect transient attacks that can occur in between snapshots . we implemented a prototype of the vigilare system on gaisler 's grlib - based system - on - a - chip ( soc ) by adding snooper hardware connections module to the host system for bus snooping . to evaluate the benefit of snoop - based monitoring , we also implemented similar soc with a snapshot - based monitor to be compared with . the vigilare system detected all the transient attacks without performance degradation while the snapshot - based monitor could not detect all the attacks and induced considerable performance degradation as much as 10 % in our tuned stream benchmark test .
RANK = 11; score = 0.959505021572113; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 12; score = 0.95877605676651; correct = False; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 13; score = 0.9584273099899292; correct = True; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 14; score = 0.9582068920135498; correct = False; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
RANK = 15; score = 0.9581626057624817; correct = False; id = 2f7b92282d42d645ee2d4bc34aa2bf132275e82b
libfte : a toolkit for constructing practical , format - abiding encryption schemes encryption schemes where the ciphertext must abide by a specified format have diverse applications , ranging from in - place encryption in databases to per - message encryption of network traffic for censorship circumvention . despite this , a unifying framework for deploying such encryption schemes has not been developed . one consequence of this is that current schemes are ad - hoc ; another is a requirement for expert knowledge that can disuade one from using encryption at all . we present a general - purpose library ( called libfte ) that aids engineers in the development and deployment of format - preserving encryption ( fpe ) and formattransforming encryption ( fte ) schemes . it incorporates a new algorithmic approach for performing fpe / fte using the nondeterministic finite - state automata ( nfa ) representation of a regular expression when specifying formats . this approach was previously considered unworkable , and our approach closes this open problem . we evaluate libfte and show that , compared to other encryption solutions , it introduces negligible latency overhead , and can decrease diskspace usage by as much as 62.5 % when used for simultaneous encryption and compression in a postgresql database ( both relative to conventional encryption mechanisms ) . in the censorship circumvention setting we show that , using regularexpression formats lifted from the snort ids , libfte can reduce client / server memory requirements by as much as 30 % .
RANK = 16; score = 0.9581118822097778; correct = False; id = 0dfe28b14b7c35fc1b876954a09ff6b7e47a8ee9
openconflict : preventing real time map hacks in online games we present a generic tool , kartograph , that lifts the fog of war in online real - time strategy games by snooping on the memory used by the game . kartograph is passive and can not be detected remotely . motivated by these passive attacks , we present secure protocols for distributing game state among players so that each client only has data it is allowed to see . our system , open conflict , runs real - time games with distributed state . to support our claim that open conflict is sufficiently fast for real - time strategy games , we show the results of an extensive study of 1000 replays of star craft ii games between expert players . at the peak of a typical game , open conflict needs only 22 milliseconds on one cpu core each time state is synchronized .
RANK = 17; score = 0.9579377174377441; correct = False; id = 0fd2467de521b52805eea902edc9587c87818276
discoverer : automatic protocol reverse engineering from network traces application - level protocol specifications are useful for many security applications , including intrusion prevention and detection that performs deep packet inspection and traffic normalization , and penetration testing that generates network inputs to an application to uncover potential vulnerabilities . however , current practice in deriving protocol specifications is mostly manual . in this paper , we present discoverer , a tool for automatically reverse engineering the protocol message formats of an application from its network trace . a key property of discoverer is that it operates in a protocol - independent fashion by inferring protocol idioms commonly seen in message formats of many application - level protocols . we evaluated the efficacy of discoverer over one text protocol ( http ) and two binary protocols ( rpc and cifs / smb ) by comparing our inferred formats with true formats obtained from ethereal [ 5 ] . for all three protocols , more than 90 % of our inferred formats correspond to exactly one true format ; one true format is reflected in five inferred formats on average ; our inferred formats cover over 95 % of messages , which belong to 30 - 40 % of true formats observed in the trace .
RANK = 18; score = 0.9579339027404785; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 19; score = 0.9578502178192139; correct = False; id = 4681a0116597fd0804b07e8176b8761e4f569743
password cracking using probabilistic context - free grammars choosing the most effective word - mangling rules to use when performing a dictionary - based password cracking attack can be a difficult task . in this paper we discuss a new method that generates password structures in highest probability order . we first automatically create a probabilistic context - free grammar based upon a training set of previously disclosed passwords . this grammar then allows us to generate word - mangling rules , and from them , password guesses to be used in password cracking . we will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets . in one series of experiments , training on a set of disclosed passwords , our approach was able to crack 28 % to 129 % more passwords than john the ripper , a publicly available standard password cracking program .
RANK = 20; score = 0.9578246474266052; correct = False; id = 64205427d0f900997ec0a22fdd4946a3ba16f1b9
hey , you have a problem : on the feasibility of large - scale web vulnerability notification large - scale discovery of thousands of vulnerable web sites has become a frequent event , thanks to recent advances in security research and the rise in maturity of internet - wide scanning tools . the issues related to disclosing the vulnerability information to the affected parties , however , have only been treated as a side note in prior research . in this paper , we systematically examine the feasibility and efficacy of large - scale notification campaigns . for this , we comprehensively survey existing communication channels and evaluate their usability in an automated notification process . using a data set of over 44,000 vulnerable web sites , we measure success rates , both with respect to the total number of fixed vulnerabilities and to reaching responsible parties , with the following highlevel results : although our campaign had a statistically significant impact compared to a control group , the increase in the fix rate of notified domains is marginal . if a notification report is read by the owner of the vulnerable application , the likelihood of a subsequent resolution of the issues is sufficiently high : about 40 % . but , out of 35,832 transmitted vulnerability reports , only 2,064 ( 5.8 % ) were actually received successfully , resulting in an unsatisfactory overall fix rate , leaving 74.5 % of web applications exploitable after our month - long experiment . thus , we conclude that currently no reliable notification channels exist , which significantly inhibits the success and impact of large - scale notification .

RANKING 322
QUERY
robust dictionary lookup in multiple noisy orthographies we present the multiscript phonetic search algorithm to address the problem of language learners looking up unfamiliar words that they heard . we apply it to arabic dictionary lookup with noisy queries done using both the arabic and roman scripts . our algorithm is based on a computational phonetic distance metric that can be optionally machine learned . to benchmark our performance , we created the arabscribe dataset , containing 10,000 noisy transcriptions of random arabic dictionary words . our algorithm outperforms google translate ’s “ did you mean " feature , as well as the yamli smart arabic keyboard .
First cited at 302
TOP CITED PAPERS
RANK 302
machine transliteration of names in arabic texts 
RANK 889
pronunciation modeling for improved spelling correction this paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction . the proposed method builds an explicit error model for word pronunciations . by modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction .
RANK 1127
the first qalb shared task on automatic text correction for arabic we present a summary of the first shared task on automatic text correction for arabic text . the shared task received 18 systems submissions from nine teams in six countries and represented a diversity of approaches . our report includes an overview of the qalb corpus which was the source of the datasets used for training and evaluation , an overview of participating systems , results of the competition and an analysis of the results and systems .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
improved statistical alignment models 
TOP 20
RANK = 1; score = 0.997722327709198; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9973313808441162; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.9964095950126648; correct = False; id = c9214ebe91454e6369720136ab7dd990d52a07d4
improved statistical alignment models 
RANK = 4; score = 0.9958691000938416; correct = False; id = 11aedb8f95a007363017dae311fc525f67bd7876
minimum error rate training in statistical machine translation often , the training procedure for statistical machine translation models is based on maximum likelihood or related criteria . a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text . in this paper , we analyze various training criteria which directly optimize translation quality . these training criteria make use of recently proposed automatic evaluation metrics . we describe a new algorithm for efficient training an unsmoothed error count . we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure .
RANK = 5; score = 0.9956926703453064; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 6; score = 0.9956310987472534; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 7; score = 0.9949799180030823; correct = False; id = 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
moses : open source toolkit for statistical machine translation we describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models . in addition to the smt decoder , the toolkit also includes a wide variety of tools for training , tuning and applying the system to many translation tasks .
RANK = 8; score = 0.9947652816772461; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 9; score = 0.9945861101150513; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 10; score = 0.9945532083511353; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 11; score = 0.9942491054534912; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 12; score = 0.9934477210044861; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 13; score = 0.9925583600997925; correct = False; id = 60f4f98ff57be60a786803a88f5e7e970b35c79e
re - evaluating the role of bleu in machine translation research we argue that the machine translation community is overly reliant on the bleu machine translation evaluation metric . we show that an improved bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to bleu ’s correlation with human judgments of quality . this offers new potential for research which was previously deemed unpromising by an inability to improve upon bleu scores .
RANK = 14; score = 0.9924576282501221; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 15; score = 0.9910734295845032; correct = False; id = 7533d30329cfdbf04ee8ee82bfef792d08015ee5
meteor : an automatic metric for mt evaluation with improved correlation with human judgments we describe meteor , an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human - produced reference translations . unigrams can be matched based on their surface forms , stemmed forms , and meanings ; furthermore , meteor can be easily extended to include more advanced matching strategies . once all generalized unigram matches between the two strings have been found , meteor computes a score for this matching using a combination of unigram - precision , unigram - recall , and a measure of fragmentation that is designed to directly capture how well - ordered the matched words in the machine translation are in relation to the reference . we evaluate meteor by measuring the correlation between the metric scores and human judgments of translation quality . we compute the pearson r correlation value between its scores and human quality assessments of the ldc tides 2003 arabic - to - english and chinese - to - english datasets . we perform segment - bysegment correlation , and show that meteor gets an r correlation value of 0.347 on the arabic data and 0.331 on the chinese data . this is shown to be an improvement on using simply unigramprecision , unigram - recall and their harmonic f1 combination . we also perform experiments to show the relative contributions of the various mapping modules .
RANK = 16; score = 0.9910141825675964; correct = False; id = 5b31e43f8b0490779d8013e0705ae4df8f0488c9
shallow parsing with conditional random fields conditional random fields for sequence labeling offer advantages over both generative models like hmms and classifiers applied at each sequence position . among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods . we show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the conll task , and better than any reported single model . improved training methods based on modern optimization algorithms were critical in achieving these results . we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models .
RANK = 17; score = 0.9907921552658081; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 18; score = 0.9907853603363037; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 19; score = 0.9907065033912659; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 20; score = 0.9906238913536072; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .

RANKING 2534
QUERY
pocket knowledge base population existing knowledge base population methods extract relations from a closed relational schema with limited coverage , leading to sparse kbs . we propose pocket knowledge base population ( pkbp ) , the task of dynamically constructing a kb of entities related to a query and finding the best characterization of relationships between entities . we describe novel open information extraction methods which leverage the pkb to find informative trigger words . we evaluate using existing kbp shared - task data as well as new annotations collected for this work . our methods produce high quality kbs from just text with many more entities and relationships than existing kbp systems .
First cited at 24
TOP CITED PAPERS
RANK 24
identifying relations for open information extraction open information extraction ( ie ) is the task of extracting assertions from massive corpora without requiring a pre - specified vocabulary . this paper shows that the output of state - ofthe - art open ie systems is rife with uninformative and incoherent extractions . to overcome these problems , we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs . we implemented the constraints in the reverb open ie system , which more than doubles the area under the precision - recall curve relative to previous extractors such as textrunner and woe . more than 30 % of reverb ’s extractions are at precision 0.8 or higher— compared to virtually none for earlier systems . the paper concludes with a detailed analysis of reverb ’s errors , suggesting directions for future work.1
RANK 231
open language learning for information extraction open information extraction ( ie ) systems extract relational tuples from text , without requiring a pre - specified vocabulary , by identifying relation phrases and associated arguments in arbitrary sentences . however , stateof - the - art open ie systems such as reverb and woe share two important weaknesses – ( 1 ) they extract only relations that are mediated by verbs , and ( 2 ) they ignore context , thus extracting tuples that are not asserted as factual . this paper presents ollie , a substantially improved open ie system that addresses both these limitations . first , ollie achieves high yield by extracting relations mediated by nouns , adjectives , and more . second , a context - analysis step increases precision by including contextual information from the sentence in the extractions . ollie obtains 2.7 times the area under precision - yield curve ( auc ) compared to reverb and 1.9 times the auc of woe .
RANK 638
distant supervision for relation extraction without labeled data modern models of relation extraction for tasks like ace are based on supervised learning of relations from small hand - labeled corpora . we investigate an alternative paradigm that does not require labeled corpora , avoiding the domain dependence of acestyle algorithms , and allowing the use of corpora of any size . our experiments use freebase , a large semantic database of several thousand relations , to provide distant supervision . for each pair of entities that appears in some freebase relation , we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier . our algorithm combines the advantages of supervised ie ( combining 400,000 noisy pattern features in a probabilistic classifier ) and unsupervised ie ( extracting large numbers of relations from large corpora of any domain ) . our model is able to extract 10,000 instances of 102 relations at a precision of 67.6 % . we also analyze feature performance , showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression .
TOP UNCITED PAPERS
RANK 1
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 2
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK 3
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
TOP 20
RANK = 1; score = 0.994886577129364; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 2; score = 0.9940491914749146; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 3; score = 0.9937753677368164; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 4; score = 0.9933605790138245; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 5; score = 0.9931098222732544; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 6; score = 0.9922332763671875; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 7; score = 0.9921244978904724; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 8; score = 0.9921034574508667; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 9; score = 0.9920755624771118; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 10; score = 0.9918196201324463; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 11; score = 0.9908366203308105; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 12; score = 0.9900746941566467; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 13; score = 0.9897414445877075; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 14; score = 0.9889636039733887; correct = False; id = 303b0b6e6812c60944a4ac9914222ac28b0813a2
recognizing contextual polarity in phrase - level sentiment analysis this paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions . with this approach , the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions , achieving results that are significantly better than baseline .
RANK = 15; score = 0.9886570572853088; correct = False; id = 3cad59ed9faf5194c499ae8a3d57caece152360f
hadoopperceptron : a toolkit for distributed perceptron training and prediction with mapreduce we propose a set of open - source software modules to perform structured perceptron training , prediction and evaluation within the hadoop framework . apache hadoop is a freely available environment for running distributed applications on a computer cluster . the software is designed within the map - reduce paradigm . thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data - sets . the distributed perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial perceptron . the presented modules can be executed as stand - alone software or easily extended or integrated in complex systems . the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs .
RANK = 16; score = 0.9885798096656799; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 17; score = 0.9884361028671265; correct = False; id = 94db635f54d25bdb95edb42185aca93ba53b051b
biographies , bollywood , boom - boxes and blenders : domain adaptation for sentiment classification automatic sentiment classification has been extensively studied and applied in recent years . however , sentiment is expressed differently in different domains , and annotating corpora for every possible domain of interest is impractical . we investigate domain adaptation for sentiment classifiers , focusing on online reviews for different types of products . first , we extend to sentiment classification the recently - proposed structural correspondence learning ( scl ) algorithm , reducing the relative error due to adaptation between domains by an average of 30 % over the original scl algorithm and 46 % over a supervised baseline . second , we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another . this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains .
RANK = 18; score = 0.9881318807601929; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 19; score = 0.9877408146858215; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 20; score = 0.9872745275497437; correct = False; id = 2ae6014a451801671d41b6171f86e657d8b1fbaf
wordnet : : similarity - measuring the relatedness of concepts wordnet::similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts ( or word senses ) . it provides six measures of similarity , and three measures of relatedness , all of which are based on the lexical database wordnet . these measures are implemented as perl modules which take as input two concepts , and return a numeric value that represents the degree to which they are similar or related .

RANKING 88
QUERY
neural network for heterogeneous annotations multiple treebanks annotated under heterogeneous standards give rise to the research question of best utilizing multiple resources for improving statistical models . prior research has focused on discrete models , leveraging stacking and multi - view learning to address the problem . in this paper , we empirically investigate heterogeneous annotations using neural network models , building a neural network counterpart to discrete stacking and multiview learning , respectively , finding that neural models have their unique advantages thanks to the freedom from manual feature engineering . neural model achieves not only better accuracy improvements , but also an order of magnitude faster speed compared to its discrete baseline , adding little time cost compared to a neural model trained on a single treebank .
First cited at 46
TOP CITED PAPERS
RANK 46
frustratingly easy domain adaptation we describe an approach to domain adaptation that is appropriate exactly in the case when one has enough “ target ” data to do slightly better than just using only “ source ” data . our approach is incredibly simple , easy to implement as a preprocessing step ( 10 lines of perl ! ) and outperforms stateof - the - art approaches on a range of datasets . moreover , it is trivially extended to a multidomain adaptation problem , where one has data from a variety of different domains .
RANK 48
clause restructuring for statistical machine translation we describe a method for incorporating syntactic information in statistical machine translation systems . the first step of the method is to parse the source language string that is being translated . the second step is to apply a series of transformations to the parse tree , effectively reordering the surface string on the source language side of the translation system . the goal of this step is to recover an underlying word order that is closer to the target language word - order than the original string . the reordering approach is applied as a pre - processing step in both the training and decoding phases of a phrase - based statistical mt system . we describe experiments on translation from german to english , showing an improvement from 25.2 % bleu score for a baseline system to 26.8 % bleu score for the system with reordering , a statistically significant improvement .
RANK 103
a tale of two parsers : investigating and combining graph - based and transition - based dependency parsing graph - based and transition - based approaches to dependency parsing adopt very different views of the problem , each view having its own strengths and limitations . we study both approaches under the framework of beamsearch . by developing a graph - based and a transition - based dependency parser , we show that a beam - search decoder is a competitive choice for both methods . more importantly , we propose a beam - search - based parser that combines both graph - based and transitionbased parsing into a single system for training and decoding , showing that it outperforms both the pure graph - based and the pure transition - based parsers . testing on the english and chinese penn treebank data , the combined system gave state - of - the - art accuracies of92.1 % and86.2 % , respectively .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
TOP 20
RANK = 1; score = 0.9978047013282776; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9974817633628845; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.9964510202407837; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 4; score = 0.9963579773902893; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 5; score = 0.9961174726486206; correct = False; id = c9214ebe91454e6369720136ab7dd990d52a07d4
improved statistical alignment models 
RANK = 6; score = 0.9959357976913452; correct = False; id = 11aedb8f95a007363017dae311fc525f67bd7876
minimum error rate training in statistical machine translation often , the training procedure for statistical machine translation models is based on maximum likelihood or related criteria . a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text . in this paper , we analyze various training criteria which directly optimize translation quality . these training criteria make use of recently proposed automatic evaluation metrics . we describe a new algorithm for efficient training an unsmoothed error count . we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure .
RANK = 7; score = 0.9958280920982361; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 8; score = 0.9955266118049622; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 9; score = 0.995395839214325; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 10; score = 0.9953120946884155; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 11; score = 0.9952671527862549; correct = False; id = 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
moses : open source toolkit for statistical machine translation we describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models . in addition to the smt decoder , the toolkit also includes a wide variety of tools for training , tuning and applying the system to many translation tasks .
RANK = 12; score = 0.9952263832092285; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 13; score = 0.9947409629821777; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 14; score = 0.9945576190948486; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 15; score = 0.9940687417984009; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 16; score = 0.9938639402389526; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 17; score = 0.9936428070068359; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 18; score = 0.9933826923370361; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 19; score = 0.9932669401168823; correct = False; id = 60f4f98ff57be60a786803a88f5e7e970b35c79e
re - evaluating the role of bleu in machine translation research we argue that the machine translation community is overly reliant on the bleu machine translation evaluation metric . we show that an improved bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to bleu ’s correlation with human judgments of quality . this offers new potential for research which was previously deemed unpromising by an inability to improve upon bleu scores .
RANK = 20; score = 0.9930691719055176; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .

