RANKING 742
QUERY
crime scene reconstruction : online gold farming network analysis many online games have their own ecosystems , where players can purchase in - game assets using game money . players can obtain game money through active participation or “ real money trading ” through official channels : converting real money into game money . the unofficial market for real money trading gave rise to gold farming groups ( gfgs ) , a phenomenon with serious impact in the cyber and real worlds . gfgs in massively multiplayer online role - playing games ( mmorpgs ) are some of the most interesting underground cyber economies because of the massive nature of the game . to detect gfgs , there have been various studies using behavioral traits . however , they can only detect gold farmers , not entire gfgs with internal hierarchies . even worse , gfgs continuously develop techniques to hide , such as forming front organizations , concealing cyber - money , and changing trade patterns when online game service providers ban gfgs . in this paper , we analyze the characteristics of the ecosystem of a large - scale mmorpg , and devise a method for detecting gfgs . we build a graph that characterizes virtual economy transactions , and trace abnormal trades and activities . we derive features from the trading graph and physical networks used by gfgs to identify them in their entirety . using their structure , we provide recommendations to defend effectively against gfgs while not affecting the existing virtual ecosystem .
First cited at 497
TOP CITED PAPERS
RANK 497
battle of botcraft : fighting bots in online games with human observational proofs the abuse of online games by automated programs , known as game bots , for gaining unfair advantages has plagued millions of participating players with escalating severity in recent years . the current methods for distinguishing bots and humans are based on human interactive proofs ( hips ) , such as captchas . however , hip - based approaches have inherent drawbacks . in particular , they are too obtrusive to be tolerated by human players in a gaming context . in this paper , we propose a non - interactive approach based on human observational proofs ( hops ) for continuous game bot detection . hops differentiate bots from human players by passively monitoring input actions that are difficult for current bots to perform in a human - like manner . we collect a series of user - input traces in one of the most popular online games , world of warcraft . based on the traces , we characterize the game playing behaviors of bots and humans . then , we develop a hop - based game bot defense system that analyzes user - input actions with a cascade - correlation neural network to distinguish bots from humans . the hop system is effective in capturing current game bots , which raises the bar against game exploits and forces a determined adversary to build more complicated game bots for detection evasion in the future .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK 3
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
TOP 20
RANK = 1; score = 0.773492693901062; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.7714366912841797; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 3; score = 0.7691935300827026; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 4; score = 0.7676709890365601; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 5; score = 0.7675597667694092; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 6; score = 0.7613028883934021; correct = False; id = 15ae0badc584a287fc51e5de46d1ef51495a2398
finding deceptive opinion spam by any stretch of the imagination consumers increasingly rate , review and research products online ( jansen , 2010 ; litvin et al . , 2008 ) . consequently , websites containing consumer reviews are becoming targets of opinion spam . while recent work has focused primarily on manually identifiable instances of opinion spam , in this work we study deceptive opinion spam — fictitious opinions that have been deliberately written to sound authentic . integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold - standard opinion spam dataset . based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing .
RANK = 7; score = 0.7609255909919739; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 8; score = 0.7609177827835083; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 9; score = 0.760222315788269; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 10; score = 0.7569562196731567; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 11; score = 0.7568458318710327; correct = False; id = 5c2bba059f15e8d3cda8b091ec02e180b81a6d3c
acing the ioc game : toward automatic discovery and analysis of open - source cyber threat intelligence to adapt to the rapidly evolving landscape of cyber threats , security professionals are actively exchanging indicators of compromise ( ioc ) ( e.g. , malware signatures , botnet ips ) through public sources ( e.g. blogs , forums , tweets , etc . ) . such information , often presented in articles , posts , white papers etc . , can be converted into a machine - readable openioc format for automatic analysis and quick deployment to various security mechanisms like an intrusion detection system . with hundreds of thousands of sources in the wild , the ioc data are produced at a high volume and velocity today , which becomes increasingly hard to manage by humans . efforts to automatically gather such information from unstructured text , however , is impeded by the limitations of today 's natural language processing ( nlp ) techniques , which can not meet the high standard ( in terms of accuracy and coverage ) expected from the iocs that could serve as direct input to a defense system . in this paper , we present iace , an innovation solution for fully automated ioc extraction . our approach is based upon the observation that the iocs in technical articles are often described in a predictable way : being connected to a set of context terms ( e.g. , " download " ) through stable grammatical relations . leveraging this observation , iace is designed to automatically locate a putative ioc token ( e.g. , a zip file ) and its context ( e.g. , " malware " , " download " ) within the sentences in a technical article , and further analyze their relations through a novel application of graph mining techniques . once the grammatical connection between the tokens is found to be in line with the way that the ioc is commonly presented , these tokens are extracted to generate an openioc item that describes not only the indicator ( e.g. , a malicious zip file ) but also its context ( e.g. , download from an external source ) . running on 71,000 articles collected from 45 leading technical blogs , this new approach demonstrates a remarkable performance : it generated 900 k openioc items with a precision of 95 % and a coverage over 90 % , which is way beyond what the state - of - the - art nlp technique and industry ioc tool can achieve , at a speed of thousands of articles per hour . further , by correlating the iocs mined from the articles published over a 13-year span , our study sheds new light on the links across hundreds of seemingly unrelated attack instances , particularly their shared infrastructure resources , as well as the impacts of such open - source threat intelligence on security protection and evolution of attack strategies .
RANK = 12; score = 0.7562277317047119; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 13; score = 0.7549387812614441; correct = False; id = 68930f06c3444d00731fd01b6eb5f5c0b9cc6f1f
on the feasibility of internet - scale author identification we study techniques for identifying an anonymous author via linguistic stylometry , i.e. , comparing the writing style against a corpus of texts of known authorship . we experimentally demonstrate the effectiveness of our techniques with as many as 100,000 candidate authors . given the increasing availability of writing samples online , our result has serious implications for anonymity and free speech - an anonymous blogger or whistleblower may be unmasked unless they take steps to obfuscate their writing style . while there is a huge body of literature on authorship recognition based on writing style , almost none of it has studied corpora of more than a few hundred authors . the problem becomes qualitatively different at a large scale , as we show , and techniques from prior work fail to scale , both in terms of accuracy and performance . we study a variety of classifiers , both " lazy " and " eager , " and show how to handle the huge number of classes . we also develop novel techniques for confidence estimation of classifier outputs . finally , we demonstrate stylometric authorship recognition on texts written in different contexts . in over 20 % of cases , our classifiers can correctly identify an anonymous author given a corpus of texts from 100,000 authors ; in about 35 % of cases the correct author is one of the top 20 guesses . if we allow the classifier the option of not making a guess , via confidence estimation we are able to increase the precision of the top guess from 20 % to over 80 % with only a halving of recall .
RANK = 14; score = 0.7448104023933411; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 15; score = 0.7445324659347534; correct = False; id = 28044e5f574b56fc869576d841b749b60a55f9db
keyboard acoustic emanations revisited 
RANK = 16; score = 0.7433878779411316; correct = False; id = 36918139c42c201557cef3dc69e26a06460155d8
impar : a deterministic algorithm for implicit semantic role labelling this paper presents a novel deterministic algorithm for implicit semantic role labeling . the system exploits a very simple but relevant discursive property , the argument coherence over different instances of a predicate . the algorithm solves the implicit arguments sequentially , exploiting not only explicit but also the implicit arguments previously solved . in addition , we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data .
RANK = 17; score = 0.7433415055274963; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 18; score = 0.7432460784912109; correct = False; id = 6470930ff36cde541c837bedcf17c20490fedbbc
@spam : the underground on 140 characters or less in this work we present a characterization of spam on twitter . we find that 8 % of 25 million urls posted to the site point to phishing , malware , and scams listed on popular blacklists . we analyze the accounts that send spam and find evidence that it originates from previously legitimate accounts that have been compromised and are now being puppeteered by spammers . using clickthrough data , we analyze spammers ' use of features unique to twitter and the degree that they affect the success of spam . we find that twitter is a highly successful platform for coercing users to visit spam pages , with a clickthrough rate of 0.13 % , compared to much lower rates previously reported for email spam . we group spam urls into campaigns and identify trends that uniquely distinguish phishing , malware , and spam , to gain an insight into the underlying techniques used to attract users . given the absence of spam filtering on twitter , we examine whether the use of url blacklists would help to significantly stem the spread of twitter spam . our results indicate that blacklists are too slow at identifying new threats , allowing more than 90 % of visitors to view a page before it becomes blacklisted . we also find that even if blacklist delays were reduced , the use by spammers of url shortening services for obfuscation negates the potential gains unless tools that use blacklists develop more sophisticated spam filtering .
RANK = 19; score = 0.738270103931427; correct = False; id = 648c0a6d5023374c0c93fafb571b782da1dfbeed
the enemy in your own camp : how well can we detect statistically - generated fake reviews - an adversarial study . online reviews are a growing market , but it is struggling with fake reviews . they undermine both the value of reviews to the user , and their trust in the review sites . however , fake positive reviews can boost a business , and so a small industry producing fake reviews has developed . the two sides are facing an arms race that involves more and more natural language processing ( nlp ) . so far , nlp has been used mostly for detection , and works well on human - generated reviews . but what happens if nlp techniques are used to generate fake reviews as well ? we investigate the question in an adversarial setup , by assessing the detectability of different fake - review generation strategies . we use generative models to produce reviews based on meta - information , and evaluate their effectiveness against deceptiondetection models and human judges . we find that meta - information helps detection , but that nlp - generated reviews conditioned on such information are also much harder to detect than conventional ones .
RANK = 20; score = 0.738020658493042; correct = False; id = 0eeba9d2cb597f4128f18f1a1354a56c1b9207c5
polarity sensitivity and evaluation order in type - logical grammar we present a novel , type - logical analysis of polarity sensitivity : how negative polarity items ( like any and ever ) or positive ones ( like some ) are licensed or prohibited . it takes not just scopal relations but also linear order into account , using the programming - language notions of delimited continuations and evaluation order , respectively . it thus achieves greater empirical coverage than previous proposals .

RANKING 1079
QUERY
ethical research protocols for social media health research social media have transformed datadriven research in political science , the social sciences , health , and medicine . since health research often touches on sensitive topics that relate to ethics of treatment and patient privacy , similar ethical considerations should be acknowledged when using social media data in health research . while much has been said regarding the ethical considerations of social media research , health research leads to an additional set of concerns . we provide practical suggestions in the form of guidelines for researchers working with social media data in health research . these guidelines can inform an irb proposal for researchers new to social media health research .
First cited at 9
TOP CITED PAPERS
RANK 9
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK 32
discriminating gender on twitter accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing , personalization , and legal investigation . this paper describes the construction of a large , multilingual dataset labeled with gender , and investigates statistical models for determining the gender of uncharacterized twitter users . we explore several different classifier types on this dataset . we show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models . we also perform a large - scale human assessment using amazon mechanical turk . our methods significantly out - perform both baseline models and almost all humans on the same task .
RANK 656
the role of personality , age , and gender in tweeting about mental illness mental illnesses , such as depression and post traumatic stress disorder ( ptsd ) , are highly underdiagnosed globally . populations sharing similar demographics and personality traits are known to be more at risk than others . in this study , we characterise the language use of users disclosing their mental illness on twitter . language - derived personality and demographic estimates show surprisingly strong performance in distinguishing users that tweet a diagnosis of depression or ptsd from random controls , reaching an area under the receiveroperating characteristic curve – auc – of around .8 in all our binary classification tasks . in fact , when distinguishing users disclosing depression from those disclosing ptsd , the single feature of estimated age shows nearly as strong performance ( auc = .806 ) as using thousands of topics ( auc = .819 ) or tens of thousands of n - grams ( auc = .812 ) . we also find that differential language analyses , controlled for demographics , recover many symptoms associated with the mental illnesses in the clinical literature .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.7729198336601257; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.7701489329338074; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.7692480683326721; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.7406586408615112; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.7398489117622375; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 6; score = 0.7263357043266296; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.7252238392829895; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.7224947810173035; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 9; score = 0.7213751673698425; correct = True; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 10; score = 0.7179428339004517; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.7154936194419861; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 12; score = 0.7099078297615051; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.7080444097518921; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 14; score = 0.7074488401412964; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 15; score = 0.705101728439331; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 16; score = 0.698948860168457; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 17; score = 0.6985229253768921; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 18; score = 0.6984789967536926; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 19; score = 0.6946600675582886; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 20; score = 0.6918648481369019; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .

RANKING 569
QUERY
stacked sentence - document classifier approach for improving native language identification in this paper , we describe the approach of the italianlp lab team to native language identification and discuss the results we submitted as participants to the essay track of nli shared task 2017 . we introduce for the first time a 2-stacked sentencedocument architecture for native language identification that is able to exploit both local sentence information and a wide set of general – purpose features qualifying the lexical and grammatical structure of the whole document . when evaluated on the official test set , our sentence - document stacked architecture obtained the best result among all the participants of the essay track with an f1 score of 0.8818 .
First cited at 146
TOP CITED PAPERS
RANK 146
a report on the first native language identification shared task native language identification , or nli , is the task of automatically classifying the l1 of a writer based solely on his or her essay written in another language . this problem area has seen a spike in interest in recent years as it can have an impact on educational applications tailored towards non - native speakers of a language , as well as authorship profiling . while there has been a growing body of work in nli , it has been difficult to compare methodologies because of the different approaches to pre - processing the data , different sets of languages identified , and different splits of the data used . in this shared task , the first ever for native language identification , we sought to address the above issues by providing a large corpus designed specifically for nli , in addition to providing an environment for systems to be directly compared . in this paper , we report the results of the shared task . a total of 29 teams from around the world competed across three different sub - tasks .
RANK 992
can characters reveal your native language ? a language - independent approach to native language identification a common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part - of - speech tags , stems , or some other high - level linguistic features . in this work , an approach that uses character n - grams as features is proposed for the task of native language identification . instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning . kernel ridge regression and kernel discriminant analysis are independently used in the learning stage . the empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 nli shared task . furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral . in the cross - corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .
RANK 2659
linguistic profiling based on general - purpose features and native language identification in this paper , we describe our approach to native language identification and discuss the results we submitted as participants to the first nli shared task . by resorting to a wide set of general – purpose features qualifying the lexical and grammatical structure of a text , rather than to ad hoc features specifically selected for the nli task , we achieved encouraging results , which show that the proposed approach is general – purpose and portable across different tasks , domains and languages .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8499199748039246; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.846952497959137; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8463244438171387; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8271694779396057; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.8205481171607971; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 6; score = 0.8191584944725037; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 7; score = 0.814815878868103; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 8; score = 0.8129016160964966; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 9; score = 0.8107214570045471; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 10; score = 0.8046417236328125; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 11; score = 0.8041094541549683; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 12; score = 0.8033486008644104; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 13; score = 0.803297221660614; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 14; score = 0.8003200888633728; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 15; score = 0.7977212071418762; correct = False; id = 06d22d068996a25df4e463410c2f36deded36512
semeval-2015 task 4 : timeline : cross - document event ordering this paper describes the outcomes of the timeline task ( cross - document event ordering ) , that was organised within the time and space track of semeval-2015 . given a set of documents and a set of target entities , the task consisted of building a timeline for each entity , by detecting , anchoring in time and ordering the events involving that entity . the timeline task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents . four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs , the best of which obtained an f1-score of 7.85 in the main track ( timeline creation from raw text ) .
RANK = 16; score = 0.7962440848350525; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 17; score = 0.7956486344337463; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 18; score = 0.7905118465423584; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 19; score = 0.7898620963096619; correct = False; id = 436a1de1c3fafa0e5500ebbaf70ca79df232b87c
semeval-2015 task 14 : analysis of clinical text we describe two tasks — named entity recognition ( task 1 ) and template slot filling ( task 2)—for clinical texts . the tasks leverage annotations from the share corpus , which consists of clinical notes with annotated mentions disorders , along with their normalization to a medical terminology and eight additional attributes . the purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling . task 2 consisted of two subtasks : template slot filling given gold - standard disorder spans ( task 2a ) and end - to - end disorder span identification together with template slot filling ( task 2b ) . for task 1 ( disorder span detection and normalization ) , 16 teams participated . the best system yielded a strict f1-score of 75.7 , with a precision of 78.3 and recall of 73.2 . for task 2a ( template slot filling given goldstandard disorder spans ) , six teams participated . the best system yielded a combined overall weighted accuracy for slot filling of 88.6 . for task 2b ( disorder recognition and template slot filling ) , nine teams participated . the best system yielded a combined relaxed f ( for span detection ) and overall weighted accuracy of 80.8 .
RANK = 20; score = 0.7888322472572327; correct = False; id = 4773dd8af1413146383f53894a888184b7c68938
semeval-2015 task 1 : paraphrase and semantic similarity in twitter ( pit ) in this shared task , we present evaluations on two related tasks paraphrase identification ( pi ) and semantic textual similarity ( ss ) systems for the twitter data . given a pair of sentences , participants are asked to produce a binary yes / no judgement or a graded score to measure their semantic equivalence . the task features a newly constructed twitter paraphrase corpus that contains 18,762 sentence pairs . a total of 19 teams participated , submitting 36 runs to the pi task and 26 runs to the ss task . the evaluation shows encouraging results and open challenges for future research . the best systems scored a f1-measure of 0.674 for the pi task and a pearson correlation of 0.619 for the ss task respectively , comparing to a strong baseline using logistic regression model of 0.589 f1 and 0.511 pearson ; while the best ss systems can often reach > 0.80 pearson on well - formed text . this shared task also provides insights into the relation between the pi and ss tasks and suggests the importance to bringing these two research areas together . we make all the data , baseline systems and evaluation scripts publicly available.1

RANKING 495
QUERY
predicting word association strengths this paper looks at the task of predicting word association strengths across three datasets ; wordnet evocation ( boydgraber et al . , 2006 ) , university of southern florida free association norms ( nelson et al . , 2004 ) , and edinburgh associative thesaurus ( kiss et al . , 1973 ) . we achieve results of r = 0.357 and ρ = 0.379 , r = 0.344 and ρ = 0.300 , an ρ = 0.292 and ρ = 0.363 , respectively . we find word2vec ( mikolov et al . , 2013 ) and glove ( pennington et al . , 2014 ) cosine similarities , as well as vector offsets , to be the highest performing features . furthermore , we examine the usefulness of gaussian embeddings ( vilnis and mccallum , 2014 ) for predicting word association strength , the first work to do so .
First cited at 1
TOP CITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2381
verb semantics and lexical selection this paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation ( mt ) . two groups of english and chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments . a novel representation scheme is suggested , and is compared to representations with selection restrictions used in transfer - based mt . we see our approach as closely aligned with knowledge - based mt approaches ( kbmt ) , and as a separate component that could be incorporated into existing systems . examples and experimental results will show that , using this scheme , inexact matches can achieve correct lexical selection .
RANK 3643
autoextend : extending word embeddings to embeddings for synsets and lexemes we present autoextend , a system to learn embeddings for synsets and lexemes . it is flexible in that it can take any word embeddings as input and does not need an additional training corpus . the synset / lexeme embeddings obtained live in the same vector space as the word embeddings . a sparse tensor formalization guarantees efficiency and parallelizability . we use wordnet as a lexical resource , but autoextend can be easily applied to other resources like freebase . autoextend achieves state - of - the - art performance on word similarity and word sense disambiguation tasks .
TOP UNCITED PAPERS
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK 4
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
TOP 20
RANK = 1; score = 0.858902096748352; correct = True; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8556461930274963; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8547157645225525; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8361987471580505; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.8329840898513794; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 6; score = 0.8272472620010376; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 7; score = 0.8239462971687317; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 8; score = 0.8230451345443726; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 9; score = 0.8209362030029297; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 10; score = 0.8186348676681519; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 11; score = 0.8161987662315369; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 12; score = 0.8150514364242554; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 13; score = 0.8143974542617798; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 14; score = 0.8091114163398743; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 15; score = 0.8078566193580627; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 16; score = 0.8064708113670349; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 17; score = 0.8042285442352295; correct = False; id = 06d22d068996a25df4e463410c2f36deded36512
semeval-2015 task 4 : timeline : cross - document event ordering this paper describes the outcomes of the timeline task ( cross - document event ordering ) , that was organised within the time and space track of semeval-2015 . given a set of documents and a set of target entities , the task consisted of building a timeline for each entity , by detecting , anchoring in time and ordering the events involving that entity . the timeline task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents . four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs , the best of which obtained an f1-score of 7.85 in the main track ( timeline creation from raw text ) .
RANK = 18; score = 0.8012840151786804; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 19; score = 0.8001852631568909; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 20; score = 0.7983177304267883; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .

RANKING 2354
QUERY
niletmrg at semeval-2017 task 8 : determining rumour and veracity support for rumours on twitter this paper presents the results and conclusions of our participation in semeval-2017 task 8 : determining rumour veracity and support for rumours . we have participated in 2 subtasks : sdqc ( subtask a ) which deals with tracking how tweets orient to the accuracy of a rumourous story , and veracity prediction ( subtask b ) which deals with the goal of predicting the veracity of a given rumour . our participation was in the closed task variant , in which the prediction is made solely from the tweet itself . for subtask a , linear support vector classification was applied to a model of bag of words , and the help of a naïve bayes classifier was used for semantic feature extraction . for subtask b , a similar approach was used . many features were used during the experimentation process but only a few proved to be useful with the data set provided . our system achieved 71 % accuracy and ranked 5th among 8 systems for subtask a and achieved 53 % accuracy with the lowest rmse value of 0.672 ranking at the first place among 5 systems for subtask b.
First cited at 196
TOP CITED PAPERS
RANK 196
rumor has it : identifying misinformation in microblogs a rumor is commonly defined as a statement whose true value is unverifiable . rumors may spread misinformation ( false information ) or disinformation ( deliberately false information ) on a network of people . identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority . in this paper , we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features : content - based , network - based , and microblog - specific memes for correctly identifying rumors . moreover , we show how these features are also effective in identifying disinformers , users who endorse a rumor and further help it to spread . we perform our experiments on more than 10,000 manually annotated tweets collected from twitter and show how our retrieval model achieves more than 0.95 in mean average precision ( map ) . finally , we believe that our dataset is the first large - scale dataset on rumor detection . it can open new dimensions in analyzing online misinformation and other aspects of microblog conversations .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8568775057792664; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8551341891288757; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8546186089515686; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8396657109260559; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.835678219795227; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 6; score = 0.8309844136238098; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 7; score = 0.8302519917488098; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 8; score = 0.8300952315330505; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 9; score = 0.8217565417289734; correct = False; id = 06d22d068996a25df4e463410c2f36deded36512
semeval-2015 task 4 : timeline : cross - document event ordering this paper describes the outcomes of the timeline task ( cross - document event ordering ) , that was organised within the time and space track of semeval-2015 . given a set of documents and a set of target entities , the task consisted of building a timeline for each entity , by detecting , anchoring in time and ordering the events involving that entity . the timeline task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents . four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs , the best of which obtained an f1-score of 7.85 in the main track ( timeline creation from raw text ) .
RANK = 10; score = 0.8201384544372559; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.8200511336326599; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 12; score = 0.8165585994720459; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 13; score = 0.8158175349235535; correct = False; id = 436a1de1c3fafa0e5500ebbaf70ca79df232b87c
semeval-2015 task 14 : analysis of clinical text we describe two tasks — named entity recognition ( task 1 ) and template slot filling ( task 2)—for clinical texts . the tasks leverage annotations from the share corpus , which consists of clinical notes with annotated mentions disorders , along with their normalization to a medical terminology and eight additional attributes . the purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling . task 2 consisted of two subtasks : template slot filling given gold - standard disorder spans ( task 2a ) and end - to - end disorder span identification together with template slot filling ( task 2b ) . for task 1 ( disorder span detection and normalization ) , 16 teams participated . the best system yielded a strict f1-score of 75.7 , with a precision of 78.3 and recall of 73.2 . for task 2a ( template slot filling given goldstandard disorder spans ) , six teams participated . the best system yielded a combined overall weighted accuracy for slot filling of 88.6 . for task 2b ( disorder recognition and template slot filling ) , nine teams participated . the best system yielded a combined relaxed f ( for span detection ) and overall weighted accuracy of 80.8 .
RANK = 14; score = 0.8151208758354187; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 15; score = 0.8143711090087891; correct = False; id = 3209e1d72cdb630a43c50bd86742b9f7c7c01470
semeval-2015 task 17 : taxonomy extraction evaluation ( texeval ) this paper describes the first shared task on taxonomy extraction evaluation organised as part of semeval-2015 . participants were asked to find hypernym - hyponym relations between given terms . for each of the four selected target domains the participants were provided with two lists of domainspecific terms : a wordnet collection of terms and a well - known terminology extracted from an online publicly available taxonomy . a total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures , the structural similarity with a gold standard taxonomy , and through manual quality assessment of sampled novel relations .
RANK = 16; score = 0.8138932585716248; correct = False; id = 4773dd8af1413146383f53894a888184b7c68938
semeval-2015 task 1 : paraphrase and semantic similarity in twitter ( pit ) in this shared task , we present evaluations on two related tasks paraphrase identification ( pi ) and semantic textual similarity ( ss ) systems for the twitter data . given a pair of sentences , participants are asked to produce a binary yes / no judgement or a graded score to measure their semantic equivalence . the task features a newly constructed twitter paraphrase corpus that contains 18,762 sentence pairs . a total of 19 teams participated , submitting 36 runs to the pi task and 26 runs to the ss task . the evaluation shows encouraging results and open challenges for future research . the best systems scored a f1-measure of 0.674 for the pi task and a pearson correlation of 0.619 for the ss task respectively , comparing to a strong baseline using logistic regression model of 0.589 f1 and 0.511 pearson ; while the best ss systems can often reach > 0.80 pearson on well - formed text . this shared task also provides insights into the relation between the pi and ss tasks and suggests the importance to bringing these two research areas together . we make all the data , baseline systems and evaluation scripts publicly available.1
RANK = 17; score = 0.8135989308357239; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 18; score = 0.8113506436347961; correct = False; id = 3ff04518e8d6a2720c009be2fc481fbb7cc5040b
semeval-2015 task 2 : semantic textual similarity , english , spanish and pilot on interpretability in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence between two text snippets . this year , the participants were challenged with new datasets in english and spanish . for the english subtask , we exposed the systems to a diversity of testing scenarios , by preparing additional pairs from headlines and image descriptions , as well as introducing new genres , including answer pairs from a tutorial dialogue system , answer pairs from q&a websites , and pairs from a committed belief dataset . for the spanish subtask , additional pairs from news and wikipedia articles were selected . the annotations for both subtasks leveraged crowdsourcing . the english subtask attracted 29 teams with 74 system runs , and the spanish subtask engaged 7 teams participating with 16 system runs . in addition , this year we ran a pilot task on interpretable sts , where the systems needed to add an explanatory layer , that is , they had to align the chunks in the sentence pair , explicitly annotating the kind of relation and the score for the chunk pair . the train and test data were manually annotated by an expert , and included headline and image sentence pairs from previous years . 7 teams participated with 29 runs .
RANK = 19; score = 0.8072200417518616; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 20; score = 0.8068685531616211; correct = False; id = 8208d5644984a8ef5d10c9bc2f898f539d189263
semeval-2015 task 10 : sentiment analysis in twitter in this paper , we describe the 2015 iteration of the semeval shared task on sentiment analysis in twitter . this was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years . this year ’s shared task competition consisted of five sentiment prediction subtasks . two were reruns from previous years : ( a ) sentiment expressed by a phrase in the context of a tweet , and ( b ) overall sentiment of a tweet . we further included three new subtasks asking to predict ( c ) the sentiment towards a topic in a single tweet , ( d ) the overall sentiment towards a topic in a set of tweets , and ( e ) the degree of prior polarity of a phrase .

RANKING 2329
QUERY
( un)wisdom of crowds : accurately spotting malicious ip clusters using not - so - accurate ip blacklists most complex tasks on the internet — both malicious and benign — are collectively carried out by clusters of ip addresses . we demonstrate that it is often possible to discover such clusters by processing data sets and logs collected at various vantage points in the network . obviously , not all clusters discovered in this way are malicious . nevertheless , we show that malicious clusters can accurately be distinguished from benign ones by simply using an ip blacklist and without requiring any complex analysis to verify malicious behavior . in this paper , we first propose a novel clustering framework which can be applied on data sets of network interactions to identify ip clusters carrying out a specific task collectively . then , given such a list of identified clusters of ip addresses , we present a simple procedure to spot the malicious ones using an ip blacklist . we show that by choosing the parameter of the proposed clustering process optimally using a blacklist , hence making it blacklist - aware , we significantly improve our overall ability to detect malicious clusters . furthermore , we mathematically show that even a blacklist with poor accuracy can be used to detect malicious clusters with high precision and recall . finally , we demonstrate the efficacy of the proposed scheme using real - world login events captured at the login servers of a large webmail provider with hundreds of millions of active users .
First cited at 610
TOP CITED PAPERS
RANK 610
your botnet is my botnet : analysis of a botnet takeover botnets , networks of malware - infected machines that are controlled by an adversary , are the root cause of a large number of security problems on the internet . a particularly sophisticated and insidious type of bot is torpig , a malware program that is designed to harvest sensitive information ( such as bank account and credit card data ) from its victims . in this paper , we report on our efforts to take control of the torpig botnet and study its operations for a period of ten days . during this time , we observed more than 180 thousand infections and recorded almost 70 gb of data that the bots collected . while botnets have been " hijacked " and studied previously , the torpig botnet exhibits certain properties that make the analysis of the data particularly interesting . first , it is possible ( with reasonable accuracy ) to identify unique bot infections and relate that number to the more than 1.2 million ip addresses that contacted our command and control server . second , the torpig botnet is large , targets a variety of applications , and gathers a rich and diverse set of data from the infected victims . this data provides a new understanding of the type and amount of personal information that is stolen by botnets .
RANK 650
botminer : clustering analysis of network traffic for protocol- and structure - independent botnet detection botnets are now the key platform for many internet attacks , such as spam , distributed denial - of - service ( ddos ) , identity theft , and phishing . most of the current botnet detection approaches work only on specific botnet command and control ( c&c ) protocols ( e.g. , irc ) and structures ( e.g. , centralized ) , and can become ineffective as botnets change their c&c techniques . in this paper , we present a general detection framework that is independent of botnet c&c protocol and structure , and requires no a priori knowledge of botnets ( such as captured bot binaries and hence the botnet signatures , and c&c server names / addresses ) . we start from the definition and essential properties of botnets . we define a botnet as a coordinated group of malware instances that are controlled via c&c communication channels . the essential properties of a botnet are that the bots communicate with some c&c servers / peers , perform malicious activities , and do so in a similar or correlated way . accordingly , our detection framework clusters similar communication traffic and similar malicious traffic , and performs cross cluster correlation to identify the hosts that share both similar communication patterns and similar malicious activity patterns . these hosts are thus bots in the monitored network . we have implemented our botminer prototype system and evaluated it using many real network traces . the results show that it can detect real - world botnets ( irc - based , http - based , and p2p botnets including nugache and storm worm ) , and has a very low false positive rate .
RANK 872
botgrep : finding p2p bots with structured graph analysis a key feature that distinguishes modern botnets from earlier counterparts is their increasing use of structured overlay topologies . this lets them carry out sophisticated coordinated activities while being resilient to churn , but it can also be used as a point of detection . in this work , we devise techniques to localize botnet members based on the unique communication patterns arising from their overlay topologies used for command and control . experimental results on synthetic topologies embedded within internet traffic traces from an isp ’s backbone network indicate that our techniques ( i ) can localize the majority of bots with low false positive rate , and ( ii ) are resilient to incomplete visibility arising from partial deployment of monitoring systems and measurement inaccuracies from dynamics of background traffic .
TOP UNCITED PAPERS
RANK 1
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK 2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK 3
acing the ioc game : toward automatic discovery and analysis of open - source cyber threat intelligence to adapt to the rapidly evolving landscape of cyber threats , security professionals are actively exchanging indicators of compromise ( ioc ) ( e.g. , malware signatures , botnet ips ) through public sources ( e.g. blogs , forums , tweets , etc . ) . such information , often presented in articles , posts , white papers etc . , can be converted into a machine - readable openioc format for automatic analysis and quick deployment to various security mechanisms like an intrusion detection system . with hundreds of thousands of sources in the wild , the ioc data are produced at a high volume and velocity today , which becomes increasingly hard to manage by humans . efforts to automatically gather such information from unstructured text , however , is impeded by the limitations of today 's natural language processing ( nlp ) techniques , which can not meet the high standard ( in terms of accuracy and coverage ) expected from the iocs that could serve as direct input to a defense system . in this paper , we present iace , an innovation solution for fully automated ioc extraction . our approach is based upon the observation that the iocs in technical articles are often described in a predictable way : being connected to a set of context terms ( e.g. , " download " ) through stable grammatical relations . leveraging this observation , iace is designed to automatically locate a putative ioc token ( e.g. , a zip file ) and its context ( e.g. , " malware " , " download " ) within the sentences in a technical article , and further analyze their relations through a novel application of graph mining techniques . once the grammatical connection between the tokens is found to be in line with the way that the ioc is commonly presented , these tokens are extracted to generate an openioc item that describes not only the indicator ( e.g. , a malicious zip file ) but also its context ( e.g. , download from an external source ) . running on 71,000 articles collected from 45 leading technical blogs , this new approach demonstrates a remarkable performance : it generated 900 k openioc items with a precision of 95 % and a coverage over 90 % , which is way beyond what the state - of - the - art nlp technique and industry ioc tool can achieve , at a speed of thousands of articles per hour . further , by correlating the iocs mined from the articles published over a 13-year span , our study sheds new light on the links across hundreds of seemingly unrelated attack instances , particularly their shared infrastructure resources , as well as the impacts of such open - source threat intelligence on security protection and evolution of attack strategies .
TOP 20
RANK = 1; score = 0.7731137275695801; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 2; score = 0.7605497241020203; correct = False; id = 13dcd0c2c035417b7ab9c493b05f5294b27b6de2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK = 3; score = 0.7438293695449829; correct = False; id = 5c2bba059f15e8d3cda8b091ec02e180b81a6d3c
acing the ioc game : toward automatic discovery and analysis of open - source cyber threat intelligence to adapt to the rapidly evolving landscape of cyber threats , security professionals are actively exchanging indicators of compromise ( ioc ) ( e.g. , malware signatures , botnet ips ) through public sources ( e.g. blogs , forums , tweets , etc . ) . such information , often presented in articles , posts , white papers etc . , can be converted into a machine - readable openioc format for automatic analysis and quick deployment to various security mechanisms like an intrusion detection system . with hundreds of thousands of sources in the wild , the ioc data are produced at a high volume and velocity today , which becomes increasingly hard to manage by humans . efforts to automatically gather such information from unstructured text , however , is impeded by the limitations of today 's natural language processing ( nlp ) techniques , which can not meet the high standard ( in terms of accuracy and coverage ) expected from the iocs that could serve as direct input to a defense system . in this paper , we present iace , an innovation solution for fully automated ioc extraction . our approach is based upon the observation that the iocs in technical articles are often described in a predictable way : being connected to a set of context terms ( e.g. , " download " ) through stable grammatical relations . leveraging this observation , iace is designed to automatically locate a putative ioc token ( e.g. , a zip file ) and its context ( e.g. , " malware " , " download " ) within the sentences in a technical article , and further analyze their relations through a novel application of graph mining techniques . once the grammatical connection between the tokens is found to be in line with the way that the ioc is commonly presented , these tokens are extracted to generate an openioc item that describes not only the indicator ( e.g. , a malicious zip file ) but also its context ( e.g. , download from an external source ) . running on 71,000 articles collected from 45 leading technical blogs , this new approach demonstrates a remarkable performance : it generated 900 k openioc items with a precision of 95 % and a coverage over 90 % , which is way beyond what the state - of - the - art nlp technique and industry ioc tool can achieve , at a speed of thousands of articles per hour . further , by correlating the iocs mined from the articles published over a 13-year span , our study sheds new light on the links across hundreds of seemingly unrelated attack instances , particularly their shared infrastructure resources , as well as the impacts of such open - source threat intelligence on security protection and evolution of attack strategies .
RANK = 4; score = 0.7412751913070679; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 5; score = 0.7406142950057983; correct = False; id = 142947116c7baec662984feb82b18c3812bc91f9
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
RANK = 6; score = 0.7398874163627625; correct = False; id = 66a6b8b5086454d2f511089ed3c157075239eb7d
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK = 7; score = 0.7376624941825867; correct = False; id = 8db2f1e1986c1185c674f34b74df126c6c6bb4dc
the seaview security model 
RANK = 8; score = 0.732876718044281; correct = False; id = 6470930ff36cde541c837bedcf17c20490fedbbc
@spam : the underground on 140 characters or less in this work we present a characterization of spam on twitter . we find that 8 % of 25 million urls posted to the site point to phishing , malware , and scams listed on popular blacklists . we analyze the accounts that send spam and find evidence that it originates from previously legitimate accounts that have been compromised and are now being puppeteered by spammers . using clickthrough data , we analyze spammers ' use of features unique to twitter and the degree that they affect the success of spam . we find that twitter is a highly successful platform for coercing users to visit spam pages , with a clickthrough rate of 0.13 % , compared to much lower rates previously reported for email spam . we group spam urls into campaigns and identify trends that uniquely distinguish phishing , malware , and spam , to gain an insight into the underlying techniques used to attract users . given the absence of spam filtering on twitter , we examine whether the use of url blacklists would help to significantly stem the spread of twitter spam . our results indicate that blacklists are too slow at identifying new threats , allowing more than 90 % of visitors to view a page before it becomes blacklisted . we also find that even if blacklist delays were reduced , the use by spammers of url shortening services for obfuscation negates the potential gains unless tools that use blacklists develop more sophisticated spam filtering .
RANK = 9; score = 0.7319532036781311; correct = False; id = 56ecb04f003d76317ff2f9a2b614dd9fba317317
an analysis of covert timing channels 
RANK = 10; score = 0.7317447662353516; correct = False; id = 0e873a513c525fe556dd5660630bd330bf1d0be8
eon : modeling and analyzing dynamic access control systems with logic programs we present eon , a logic - programming language and tool that can be used to model and analyze dynamic access control systems . our language extends datalog with some carefully designed constructs that allow the introduction and transformation of new relations . for example , these constructs can model the creation of processes and objects , and the modification of their security labels at runtime . the information - flow properties of such systems can be analyzed by asking queries in this language . we show that query evaluation in eon can be reduced to decidable query satisfiability in a fragment of datalog , and further , under some restrictions , to efficient query evaluation in datalog . we implement these reductions in our tool , and demonstrate its scope through several case studies . in particular , we study in detail the dynamic access control models of the windows vista and asbestos operating systems . we also automatically prove the security of a webserver running on asbestos .
RANK = 11; score = 0.7317123413085938; correct = False; id = 8c56a28b6d287c93dc401470dadb6f74f240e29c
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
RANK = 12; score = 0.7290095090866089; correct = False; id = 4b7c753bb235b9aafd234e00300c942665e8e481
enf extraction from digital recordings using adaptive techniques and frequency tracking a novel forensic tool used for assessing the authenticity of digital audio recordings is known as the electric network frequency ( enf ) criterion . it involves extracting the embedded power line ( utility ) frequency from said recordings and matching it to a known database to verify the time the recording was made , and its authenticity . in this paper , a nonparametric , adaptive , and high resolution technique , known as the time - recursive iterative adaptive approach , is presented as a tool for the extraction of the enf from digital audio recordings . a comparison is made between this data dependent ( adaptive ) filter and the conventional short - time fourier transform ( stft ) . results show that the adaptive algorithm improves the enf estimation accuracy in the presence of interference from other signals . to further enhance the enf estimation accuracy , a frequency tracking method based on dynamic programming will be proposed . the algorithm uses the knowledge that the enf is varying slowly with time to estimate with high accuracy the frequency present in the recording .
RANK = 13; score = 0.72764652967453; correct = False; id = 3b5c6faa99a454499e33d87cfaef9dfcb0d7b796
tapdance : end - to - middle anticensorship without flow blocking in response to increasingly sophisticated state - sponsored internet censorship , recent work has proposed a new approach to censorship resistance : end - to - middle proxying . this concept , developed in systems such as telex , decoy routing , and cirripede , moves anticensorship technology into the core of the network , at large isps outside the censoring country . in this paper , we focus on two technical obstacles to the deployment of certain end - to - middle schemes : the need to selectively block flows and the need to observe both directions of a connection . we propose a new construction , tapdance , that removes these requirements . tapdance employs a novel tcp - level technique that allows the anticensorship station at an isp to function as a passive network tap , without an inline blocking component . we also apply a novel steganographic encoding to embed control messages in tls ciphertext , allowing us to operate on https connections even under asymmetric routing . we implement and evaluate a tapdance prototype that demonstrates how the system could function with minimal impact on an isp ’s network operations .
RANK = 14; score = 0.7276378870010376; correct = False; id = 3f770cc7662340485f8fb328b3f2c95403a08e8d
alice in warningland : a large - scale field study of browser security warning effectiveness we empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature . we used mozilla firefox and google chrome ’s in - browser telemetry to observe over 25 million warning impressions in situ . during our field study , users continued through a tenth of mozilla firefox ’s malware and phishing warnings , a quarter of google chrome ’s malware and phishing warnings , and a third of mozilla firefox ’s ssl warnings . this demonstrates that security warnings can be effective in practice ; security experts and system architects should not dismiss the goal of communicating security information to end users . we also find that user behavior varies across warnings . in contrast to the other warnings , users continued through 70.2 % of google chrome ’s ssl warnings . this indicates that the user experience of a warning can have a significant impact on user behavior . based on our findings , we make recommendations for warning designers and researchers .
RANK = 15; score = 0.7270225286483765; correct = False; id = 28044e5f574b56fc869576d841b749b60a55f9db
keyboard acoustic emanations revisited 
RANK = 16; score = 0.7268409729003906; correct = False; id = 539265193da35286d4f46497755dc9cc6d51387c
digital single lens reflex camera identification from traces of sensor dust digital single lens reflex cameras suffer from a well - known sensor dust problem due to interchangeable lenses that they deploy . the dust particles that settle in front of the imaging sensor create a persistent pattern in all captured images . in this paper , we propose a novel source camera identification method based on detection and matching of these dust - spot characteristics . dust spots in the image are detected based on a ( gaussian ) intensity loss model and shape properties . to prevent false detections , lens parameter - dependent characteristics of dust spots are also taken into consideration . experimental results show that the proposed detection scheme can be used in identification of the source digital single lens reflex camera at low false positive rates , even under heavy compression and downsampling .
RANK = 17; score = 0.7265437841415405; correct = False; id = 970d38a721503f2fb0cdd7c48bd0713f6f157702
the cyber arms race when internet became commonplace in the mid-1990s , the decision makers ignored it . they did not see it as important or in any way relevant to them . as a direct result , global freedom flourished in the unrestricted online world . suddenly people all over the world had in their reach something truly and really global . and suddenly , people were not just consuming content ; they were creating content for others . but eventually politicians and leaders realized just how important the internet is . and they realized how useful the internet was for other purposes — especially for the purposes of doing surveillance on citizens . the two arguably most important inventions of our generation — the internet and the mobile phones — changed the world . however , they both turned out to be perfect tools for the surveillance state . and in a surveillance state , everybody is assumed guilty . internet surveillance became front page material when edward snowden started leaking information on prism , xkeyscore and other nsa programs in the summer of 2013 . but do not get me wrong . i do understand the need for doing both monitoring and surveillance . if somebody is suspected for running a drug ring , planning a school shooting , or participating in a terror organization , he should be monitored with a relevant court order . however , that is not what prism is about . prism is not about monitoring suspicious people . prism is about monitoring everyone . it is about monitoring people that are known to be innocent . and it is about building dossiers on everyone , eventually going back decades . such dossiers based on our internet activity will build a thorough picture of us . and if the powers - that - be ever need to find a way to twist your hand , they would certainly find something suspicious or embarrassing on everyone , if they have enough of their internet history recorded . united states intelligence agencies have a full legal right to monitor foreigners . which does not sound too bad — until you realize that most of us are foreigners to the ameripermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . copyrights for third - party components of this work must be honored . for all other uses , contact the owner / author . ccs’13 , november 4–8 , 2013 , berlin , germany . copyright is held by the owner / author(s ) . acm 978 - 1 - 4503 - 2477 - 9/13/11 ... $ 15.00 . http://dx.doi.org/10.1145/2508859.2516756 . cans . in fact , 96 % of the people on the planet turn out to be such “ foreigners . ” and when these people use us - based services , they are legally under surveillance . when the prism leaks started , us intelligence tried to calm the rest of the world by explaining that there is no need to worry and that these programs were just about fighting terrorists . but then further leaks proved that the united states was using their tools to monitor the european commission and the united nations as well . it is difficult for them to argue that they would be trying to find terrorists in the european union headquarters . another argument we have heard from the us intelligence apparatus is that everyone else is doing internet surveillance too . and indeed , most countries do have intelligence agencies , and most of them do monitor what other countries are doing . however , united states has an unfair advantage . almost all of the common internet services , search engines , webmails , web browsers and mobile operating systems come from the usa . put in another way : how many spanish politicians and decision makers use american services ? answer : all of them . and how many american politicians and decision makers use spanish services ? answer : none of them . all this should make it obvious that we foreigners should not use us - based services . they have proven to us that they are not trustworthy . why would we voluntarily hand our data to a foreign intelligence agency ? but in practice , it is very hard to avoid using services like google , facebook , linkedin , dropbox , amazon , skydrive , icloud , android , windows , ios and so on . this is a clear example on the failure of europe , asia and africa to compete with the usa on internet services . and when the rest of the world does produce a global hit , like skype or nokia , it typically ends up acquired by an american company , bringing it under us control . but if you are not doing anything wrong , why worry about this ? or , if you are worrying about this , what do you have to hide ? my answer to his question is that i have nothing to hide but i have nothing in particular that i would want to share with an intelligence agency either . in particular , i have nothing to share with a foreign intelligence agency . if we really need a big brother , i would rather have a domestic big brother than a foreign big brother . people have asked me if they really should worry about prism . i have told them that they should not be worried — they should be outraged instead . we should not just accept such blanket and wholesale surveillance from one country onto the rest of the world .
RANK = 18; score = 0.726504921913147; correct = False; id = d0a3852f95e51a6b730df75e153d8446d6e8a90a
easeandroid : automatic policy analysis and refinement for security enhanced android via large - scale semi - supervised learning mandatory protection systems such as selinux and seandroid harden operating system integrity . unfortunately , policy development is error prone and requires lengthy refinement using audit logs from deployed systems . while prior work has studied selinux policy in detail , seandroid is relatively new and has received little attention . seandroid policy engineering differs significantly from selinux : android fundamentally differs from traditional linux ; the same policy is used on millions of devices for which new audit logs are continually available ; and audit logs contain a mix of benign and malicious accesses . in this paper , we propose easeandroid , the first seandroid analytic platform for automatic policy analysis and refinement . our key insight is that the policy refinement process can be modeled and automated using semi - supervised learning . given an existing policy and a small set of known access patterns , easeandroid continually expands the knowledge base as new audit logs become available , producing suggestions for policy refinement . we evaluate easeandroid on 1.3 million audit logs from real - world devices . easeandroid successfully learns 2,518 new access patterns and generates 331 new policy rules . during this process , easeandroid discovers eight categories of attack access patterns in real devices , two of which are new attacks directly against the seandroid mac mechanism .
RANK = 19; score = 0.726012647151947; correct = False; id = 31e4845a40cfa6a953aef78387b34ea3284cdff9
all your biases belong to us : breaking rc4 in wpa - tkip and tls we present new biases in rc4 , break the wi - fi protected access temporal key integrity protocol ( wpa - tkip ) , and design a practical plaintext recovery attack against the transport layer security ( tls ) protocol . to empirically find new biases in the rc4 keystream we use statistical hypothesis tests . this reveals many new biases in the initial keystream bytes , as well as several new longterm biases . our fixed - plaintext recovery algorithms are capable of using multiple types of biases , and return a list of plaintext candidates in decreasing likelihood . to break wpa - tkip we introduce a method to generate a large number of identical packets . this packet is decrypted by generating its plaintext candidate list , and using redundant packet structure to prune bad candidates . from the decrypted packet we derive the tkip mic key , which can be used to inject and decrypt packets . in practice the attack can be executed within an hour . we also attack tls as used by https , where we show how to decrypt a secure cookie with a success rate of 94 % using 9 · 227 ciphertexts . this is done by injecting known data around the cookie , abusing this using mantin ’s absab bias , and brute - forcing the cookie by traversing the plaintext candidates . using our traffic generation technique , we are able to execute the attack in merely 75 hours .
RANK = 20; score = 0.7257970571517944; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .

RANKING 2148
QUERY
alime chat : a sequence to sequence and rerank based chatbot engine we propose alime chat , an open - domain chatbot engine that integrates the joint results of information retrieval ( ir ) and sequence to sequence ( seq2seq ) based generation models . alime chat uses an attentive seq2seq based rerank model to optimize the joint results . extensive experiments show our engine outperforms both ir and generation based models . we launch alime chat for a real - world industrial application and observe better results than another public chatbot .
First cited at 66
TOP CITED PAPERS
RANK 66
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK 252
a neural network approach to context - sensitive generation of conversational responses we present a novel response generation system that can be trained end to end on large quantities of unstructured twitter conversations . a neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models , allowing the system to take into account previous dialog utterances . our dynamic - context generative models show consistent gains over both context - sensitive and non - context - sensitive machine translation and information retrieval baselines .
RANK 497
neural responding machine for short - text conversation we propose neural responding machine ( nrm ) , a neural networ k - based response generator for short - text conversation . nrm takes the general encoder - de coder framework : it formalizes the generation of response as a decoding process based on the lat ent representation of the input text , while both encoding and decoding are realized with recurren t n ural networks ( rnn ) . the nrm is trained with a large amount of one - round conversation dat a collected from a microblogging service . empirical study shows that nrm can generate gramma tically correct and content - wise appropriate responses to over 75 % of the input text , outperf orming state - of - the - arts in the same setting , including retrieval - based and smt - based models .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8253560662269592; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8223693370819092; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8215439915657043; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.7971101403236389; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.794525682926178; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 6; score = 0.7812313437461853; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 7; score = 0.7794455289840698; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 8; score = 0.7749602198600769; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 9; score = 0.7741352915763855; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 10; score = 0.772596538066864; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 11; score = 0.7714359760284424; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 12; score = 0.7694084048271179; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 13; score = 0.7652696967124939; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 14; score = 0.7621046900749207; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 15; score = 0.7601926326751709; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 16; score = 0.7578826546669006; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 17; score = 0.7572426795959473; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 18; score = 0.7568821310997009; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 19; score = 0.7502853870391846; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 20; score = 0.7494156360626221; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .

RANKING 1095
QUERY
learning structured natural language representations for semantic parsing we introduce a neural semantic parser which is interpretable and scalable . our model converts natural language utterances to intermediate , domain - general natural language representations in the form of predicate - argument structures , which are induced with a transition system and subsequently mapped to target domains . the semantic parser is trained end - to - end using annotated logical forms or their denotations . we achieve the state of the art on spades and graphquestions and obtain competitive results on geoquery and webquestions . the induced predicate - argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.1
First cited at 1
TOP CITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 7
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK 265
semantic parsing via staged query graph generation : question answering with knowledge base given an input question , the query graph generation process can be formally defined as a search problem by a tuple 〈 s , a,π , t , γ 〉 . s = ⋃ { { φ},se , sp , sc } is the set of states , where each state is an empty graph ( φ ) , a single - node graph with the topic entity ( se ) , a core inferential chain ( sp ) , or a more complex query graph with additional constraints ( sc ) . a = ⋃ { ae , ap , ac , aa } is the set of actions . ae picks an entity node ; ap determines the core inferential chain ; ac and aa add constraints and aggregation nodes , respectively . not all the actions are valid to a given state . we use π : s → 2a to denote the legitimate set of actions . specifically , π(φ ) = ae ; π(se ) = ap , ∀se ∈ se ; π(s ) = ac ∪ aa , ∀s ∈ sp ∪ sc . t ( s , a ) → s′ is the transition function that maps a state s and an action a to the next state s′. γ : s → r is the reward function that depends on the input question and is defined over the state space . staring from s0 = φ , valid actions are iteratively applied to the current best state in the priority queue . the procedure stops when there is no more state to examine . algorithm 1 shows the pseudo code . the search procedure essentially keeps up to n candidate states in the priority queue ( n = 1000 in this work ) , and explores as many legitimate states as possible to find the best query graph , according to the reward function γ. obviously , whether the approach can work in practice depends on the quality of the reward function , as well as whether the search space can be effectively controlled .
TOP UNCITED PAPERS
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK 4
textrank : bringing order into text 
TOP 20
RANK = 1; score = 0.8577555418014526; correct = True; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8540941476821899; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8524088263511658; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8299093842506409; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.8287044763565063; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 6; score = 0.827399730682373; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.8141353726387024; correct = True; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.8123469352722168; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 9; score = 0.8119788765907288; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 10; score = 0.8071032762527466; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.8057273030281067; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 12; score = 0.7994914650917053; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.7972164154052734; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 14; score = 0.7952083945274353; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 15; score = 0.7929555177688599; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 16; score = 0.7926681637763977; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 17; score = 0.7914833426475525; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 18; score = 0.7890573143959045; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 19; score = 0.7856593728065491; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 20; score = 0.7839909791946411; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .

RANKING 2392
QUERY
a key distribution scheme for mobile wireless sensor networks : $ q$ - $ s$ -composite the majority of security systems for wireless sensor networks are based on symmetric encryption . the main open issue for these approaches concerns the establishment of symmetric keys . a promising key distribution technique is the random predistribution of secret keys . despite its effectiveness , this approach presents considerable memory overheads , in contrast with the limited resources of wireless sensor networks . in this paper , an in - depth analytical study is conducted on the state - of - the - art key distribution schemes based on random predistribution . a new protocol , called q - s - composite , is proposed in order to exploit the best features of random predistribution and to improve it with lower requirements . the main novelties of q - s - composite are represented by the organization of the secret material that allows a storing reduction , by the proposed technique for pairwise key generation , and by the limited number of predistributed keys used in the generation of a pairwise key . a comparative analysis demonstrates that the proposed approach provides a higher level of security than the state - of - the - art schemes .
First cited at 1693
TOP CITED PAPERS
RANK 1693
random key predistribution schemes for sensor networks key establishment in sensor networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor nodes , and also because the nodes could be physically compromised by an adversary . we present three new mechanisms for key establishment using the framework of pre - distributing a random set of keys to each node . first , in the q - composite keys scheme , we trade off the unlikeliness of a large - scale network attack in order to significantly strengthen random key predistribution ’s strength against smaller - scale att acks . second , in the multipath - reinforcement scheme , we show how to strengthen the security between any two nodes by leveraging the security of other links . finally , we present the random - pairwise keys scheme , which perfectly preserves the secrecy of the rest of the network when any node is captured , and also enables node - to - node authentication and quorum - based revocation .
RANK 2075
a key - management scheme for distributed sensor networks distributed sensor networks ( dsns ) are ad - hoc mobile networks that include sensor nodes with limited computation and communication capabilities . dsns are dynamic in the sense that they allow addition and deletion of sensor nodes after deployment to grow the network or replace failing and unreliable nodes . dsns may be deployed in hostile areas where communication is monitored and nodes are subject to capture and surreptitious use by an adversary . hence dsns require cryptographic protection of communications , sensor - capture detection , key revocation and sensor disabling . in this paper , we present a key - management scheme designed to satisfy both operational and security requirements of dsns . the scheme includes selective distribution and revocation of keys to sensor nodes as well as node re - keying without substantial computation and communication capabilities . it relies on probabilistic key sharing among the nodes of a random graph and uses simple protocols for shared - key discovery and path - key establishment , and for key revocation , re - keying , and incremental addition of nodes . the security and network connectivity characteristics supported by the key - management scheme are discussed and simulation experiments presented .
RANK 3052
a game - theoretic framework for robust optimal intrusion detection in wireless sensor networks a robust optimization model is considered for nonzero - sum discounted stochastic games with incomplete information in order to formally formulate and analyze the intrusion detection problem in wireless sensor networks ( wsns ) . security requirements of wsns are taken into account to characterize the game parameters and model the player objectives . to generalize the problem , the game data are assumed not to be fully known to the players , who take a robust optimization approach to address this data uncertainty . for assessing the validity and effectiveness of the framework , illustrative instances of the developed game model are generated . equilibrium analysis reveals how the conflicting objectives of the intruder and intrusion detection system compel them to adopt different conservative stances toward data uncertainty . it is also shown , by numerical results , that the robust approach in the presence of uncertainty reduces the sensitivity of the solution with respect to data perturbations , and thus improves design stability .
TOP UNCITED PAPERS
RANK 1
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK 2
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK 3
an empirical study of textual key - fingerprint representations no peter gutman , 2011
TOP 20
RANK = 1; score = 0.7622809410095215; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 2; score = 0.7247133851051331; correct = False; id = 66a6b8b5086454d2f511089ed3c157075239eb7d
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK = 3; score = 0.7239692807197571; correct = False; id = 13dcd0c2c035417b7ab9c493b05f5294b27b6de2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK = 4; score = 0.7203057408332825; correct = False; id = 142947116c7baec662984feb82b18c3812bc91f9
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
RANK = 5; score = 0.7181869745254517; correct = False; id = 8c56a28b6d287c93dc401470dadb6f74f240e29c
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
RANK = 6; score = 0.7159267067909241; correct = False; id = 8db2f1e1986c1185c674f34b74df126c6c6bb4dc
the seaview security model 
RANK = 7; score = 0.715747058391571; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 8; score = 0.7146995663642883; correct = False; id = 4b7c753bb235b9aafd234e00300c942665e8e481
enf extraction from digital recordings using adaptive techniques and frequency tracking a novel forensic tool used for assessing the authenticity of digital audio recordings is known as the electric network frequency ( enf ) criterion . it involves extracting the embedded power line ( utility ) frequency from said recordings and matching it to a known database to verify the time the recording was made , and its authenticity . in this paper , a nonparametric , adaptive , and high resolution technique , known as the time - recursive iterative adaptive approach , is presented as a tool for the extraction of the enf from digital audio recordings . a comparison is made between this data dependent ( adaptive ) filter and the conventional short - time fourier transform ( stft ) . results show that the adaptive algorithm improves the enf estimation accuracy in the presence of interference from other signals . to further enhance the enf estimation accuracy , a frequency tracking method based on dynamic programming will be proposed . the algorithm uses the knowledge that the enf is varying slowly with time to estimate with high accuracy the frequency present in the recording .
RANK = 9; score = 0.7137488722801208; correct = False; id = 0e873a513c525fe556dd5660630bd330bf1d0be8
eon : modeling and analyzing dynamic access control systems with logic programs we present eon , a logic - programming language and tool that can be used to model and analyze dynamic access control systems . our language extends datalog with some carefully designed constructs that allow the introduction and transformation of new relations . for example , these constructs can model the creation of processes and objects , and the modification of their security labels at runtime . the information - flow properties of such systems can be analyzed by asking queries in this language . we show that query evaluation in eon can be reduced to decidable query satisfiability in a fragment of datalog , and further , under some restrictions , to efficient query evaluation in datalog . we implement these reductions in our tool , and demonstrate its scope through several case studies . in particular , we study in detail the dynamic access control models of the windows vista and asbestos operating systems . we also automatically prove the security of a webserver running on asbestos .
RANK = 10; score = 0.7123408913612366; correct = False; id = 56ecb04f003d76317ff2f9a2b614dd9fba317317
an analysis of covert timing channels 
RANK = 11; score = 0.7122146487236023; correct = False; id = 539265193da35286d4f46497755dc9cc6d51387c
digital single lens reflex camera identification from traces of sensor dust digital single lens reflex cameras suffer from a well - known sensor dust problem due to interchangeable lenses that they deploy . the dust particles that settle in front of the imaging sensor create a persistent pattern in all captured images . in this paper , we propose a novel source camera identification method based on detection and matching of these dust - spot characteristics . dust spots in the image are detected based on a ( gaussian ) intensity loss model and shape properties . to prevent false detections , lens parameter - dependent characteristics of dust spots are also taken into consideration . experimental results show that the proposed detection scheme can be used in identification of the source digital single lens reflex camera at low false positive rates , even under heavy compression and downsampling .
RANK = 12; score = 0.7117806077003479; correct = False; id = 31e4845a40cfa6a953aef78387b34ea3284cdff9
all your biases belong to us : breaking rc4 in wpa - tkip and tls we present new biases in rc4 , break the wi - fi protected access temporal key integrity protocol ( wpa - tkip ) , and design a practical plaintext recovery attack against the transport layer security ( tls ) protocol . to empirically find new biases in the rc4 keystream we use statistical hypothesis tests . this reveals many new biases in the initial keystream bytes , as well as several new longterm biases . our fixed - plaintext recovery algorithms are capable of using multiple types of biases , and return a list of plaintext candidates in decreasing likelihood . to break wpa - tkip we introduce a method to generate a large number of identical packets . this packet is decrypted by generating its plaintext candidate list , and using redundant packet structure to prune bad candidates . from the decrypted packet we derive the tkip mic key , which can be used to inject and decrypt packets . in practice the attack can be executed within an hour . we also attack tls as used by https , where we show how to decrypt a secure cookie with a success rate of 94 % using 9 · 227 ciphertexts . this is done by injecting known data around the cookie , abusing this using mantin ’s absab bias , and brute - forcing the cookie by traversing the plaintext candidates . using our traffic generation technique , we are able to execute the attack in merely 75 hours .
RANK = 13; score = 0.7115387320518494; correct = False; id = d0a3852f95e51a6b730df75e153d8446d6e8a90a
easeandroid : automatic policy analysis and refinement for security enhanced android via large - scale semi - supervised learning mandatory protection systems such as selinux and seandroid harden operating system integrity . unfortunately , policy development is error prone and requires lengthy refinement using audit logs from deployed systems . while prior work has studied selinux policy in detail , seandroid is relatively new and has received little attention . seandroid policy engineering differs significantly from selinux : android fundamentally differs from traditional linux ; the same policy is used on millions of devices for which new audit logs are continually available ; and audit logs contain a mix of benign and malicious accesses . in this paper , we propose easeandroid , the first seandroid analytic platform for automatic policy analysis and refinement . our key insight is that the policy refinement process can be modeled and automated using semi - supervised learning . given an existing policy and a small set of known access patterns , easeandroid continually expands the knowledge base as new audit logs become available , producing suggestions for policy refinement . we evaluate easeandroid on 1.3 million audit logs from real - world devices . easeandroid successfully learns 2,518 new access patterns and generates 331 new policy rules . during this process , easeandroid discovers eight categories of attack access patterns in real devices , two of which are new attacks directly against the seandroid mac mechanism .
RANK = 14; score = 0.7109965682029724; correct = False; id = 3f770cc7662340485f8fb328b3f2c95403a08e8d
alice in warningland : a large - scale field study of browser security warning effectiveness we empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature . we used mozilla firefox and google chrome ’s in - browser telemetry to observe over 25 million warning impressions in situ . during our field study , users continued through a tenth of mozilla firefox ’s malware and phishing warnings , a quarter of google chrome ’s malware and phishing warnings , and a third of mozilla firefox ’s ssl warnings . this demonstrates that security warnings can be effective in practice ; security experts and system architects should not dismiss the goal of communicating security information to end users . we also find that user behavior varies across warnings . in contrast to the other warnings , users continued through 70.2 % of google chrome ’s ssl warnings . this indicates that the user experience of a warning can have a significant impact on user behavior . based on our findings , we make recommendations for warning designers and researchers .
RANK = 15; score = 0.710607647895813; correct = False; id = 06f16d9430d5f6213cf5399b167a3d989c3ff798
practical mitigations for timing - based side - channel attacks on modern x86 processors this paper studies and evaluates the extent to which automated compiler techniques can defend against timing - based side - channel attacks on modern x86 processors . we study how modern x86 processors can leak timing information through side - channels that relate to control flow and data flow . to eliminate key - dependent control flow and key - dependent timing behavior related to control flow , we propose the use of if - conversion in a compiler backend , and evaluate a proof - of - concept prototype implementation . furthermore , we demonstrate two ways in which programs that lack key - dependent control flow and key- dependent cache behavior can still leak timing information on modern x86 implementations such as the intel core 2 duo , and propose defense mechanisms against them .
RANK = 16; score = 0.7105568647384644; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 17; score = 0.7101472616195679; correct = False; id = 3b5c6faa99a454499e33d87cfaef9dfcb0d7b796
tapdance : end - to - middle anticensorship without flow blocking in response to increasingly sophisticated state - sponsored internet censorship , recent work has proposed a new approach to censorship resistance : end - to - middle proxying . this concept , developed in systems such as telex , decoy routing , and cirripede , moves anticensorship technology into the core of the network , at large isps outside the censoring country . in this paper , we focus on two technical obstacles to the deployment of certain end - to - middle schemes : the need to selectively block flows and the need to observe both directions of a connection . we propose a new construction , tapdance , that removes these requirements . tapdance employs a novel tcp - level technique that allows the anticensorship station at an isp to function as a passive network tap , without an inline blocking component . we also apply a novel steganographic encoding to embed control messages in tls ciphertext , allowing us to operate on https connections even under asymmetric routing . we implement and evaluate a tapdance prototype that demonstrates how the system could function with minimal impact on an isp ’s network operations .
RANK = 18; score = 0.7097371816635132; correct = False; id = 5c2bba059f15e8d3cda8b091ec02e180b81a6d3c
acing the ioc game : toward automatic discovery and analysis of open - source cyber threat intelligence to adapt to the rapidly evolving landscape of cyber threats , security professionals are actively exchanging indicators of compromise ( ioc ) ( e.g. , malware signatures , botnet ips ) through public sources ( e.g. blogs , forums , tweets , etc . ) . such information , often presented in articles , posts , white papers etc . , can be converted into a machine - readable openioc format for automatic analysis and quick deployment to various security mechanisms like an intrusion detection system . with hundreds of thousands of sources in the wild , the ioc data are produced at a high volume and velocity today , which becomes increasingly hard to manage by humans . efforts to automatically gather such information from unstructured text , however , is impeded by the limitations of today 's natural language processing ( nlp ) techniques , which can not meet the high standard ( in terms of accuracy and coverage ) expected from the iocs that could serve as direct input to a defense system . in this paper , we present iace , an innovation solution for fully automated ioc extraction . our approach is based upon the observation that the iocs in technical articles are often described in a predictable way : being connected to a set of context terms ( e.g. , " download " ) through stable grammatical relations . leveraging this observation , iace is designed to automatically locate a putative ioc token ( e.g. , a zip file ) and its context ( e.g. , " malware " , " download " ) within the sentences in a technical article , and further analyze their relations through a novel application of graph mining techniques . once the grammatical connection between the tokens is found to be in line with the way that the ioc is commonly presented , these tokens are extracted to generate an openioc item that describes not only the indicator ( e.g. , a malicious zip file ) but also its context ( e.g. , download from an external source ) . running on 71,000 articles collected from 45 leading technical blogs , this new approach demonstrates a remarkable performance : it generated 900 k openioc items with a precision of 95 % and a coverage over 90 % , which is way beyond what the state - of - the - art nlp technique and industry ioc tool can achieve , at a speed of thousands of articles per hour . further , by correlating the iocs mined from the articles published over a 13-year span , our study sheds new light on the links across hundreds of seemingly unrelated attack instances , particularly their shared infrastructure resources , as well as the impacts of such open - source threat intelligence on security protection and evolution of attack strategies .
RANK = 19; score = 0.7094961404800415; correct = False; id = 605ed83a6d1f4eaf995e85830f373923b11d6c13
cover your acks : pitfalls of covert channel censorship circumvention in response to increasingly sophisticated methods of blocking access to censorship circumvention schemes such as tor , recently proposed systems such as skypemorph , freewave , and censorspoofer have used voice and video conferencing protocols as " cover channels " to hide proxy connections . we demonstrate that even with perfect emulation of the cover channel , these systems can be vulnerable to attacks that detect or disrupt the covert communications while having no effect on legitimate cover traffic . our attacks stem from differences in the channel requirements for the cover protocols , which are peer - to - peer and loss tolerant , and the covert traffic , which is client - proxy and loss intolerant . these differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumvention schemes .
RANK = 20; score = 0.7075821161270142; correct = False; id = 22652399c7fb219a093344b9b47028b5d0069711
the cracked cookie jar : http cookie hijacking and the exposure of private information the widespread demand for online privacy , also fueled by widely - publicized demonstrations of session hijacking attacks against popular websites , has spearheaded the increasing deployment of https . however , many websites still avoid ubiquitous encryption due to performance or compatibility issues . the prevailing approach in these cases is to force critical functionality and sensitive data access over encrypted connections , while allowing more innocuous functionality to be accessed over http . in practice , this approach is prone to flaws that can expose sensitive information or functionality to third parties . in this paper , we conduct an in - depth assessment of a diverse set of major websites and explore what functionality and information is exposed to attackers that have hijacked a user 's http cookies . we identify a recurring pattern across websites with partially deployed https , service personalization inadvertently results in the exposure of private information . the separation of functionality across multiple cookies with different scopes and inter - dependencies further complicates matters , as imprecise access control renders restricted account functionality accessible to non - session cookies . our cookie hijacking study reveals a number of severe flaws , attackers can obtain the user 's home and work address and visited websites from google , bing and baidu expose the user 's complete search history , and yahoo allows attackers to extract the contact list and send emails from the user 's account . furthermore , e - commerce vendors such as amazon and ebay expose the user 's purchase history ( partial and full respectively ) , and almost every website exposes the user 's name and email address . ad networks like doubleclick can also reveal pages the user has visited . to fully evaluate the practicality and extent of cookie hijacking , we explore multiple aspects of the online ecosystem , including mobile apps , browser security mechanisms , extensions and search bars . to estimate the extent of the threat , we run irb - approved measurements on a subset of our university 's public wireless network for 30 days , and detect over 282 k accounts exposing the cookies required for our hijacking attacks . we also explore how users can protect themselves and find that , while mechanisms such as the eff 's https everywhere extension can reduce the attack surface , http cookies are still regularly exposed . the privacy implications of these attacks become even more alarming when considering how they can be used to deanonymize tor users . our measurements suggest that a significant portion of tor users may currently be vulnerable to cookie hijacking .

RANKING 642
QUERY
personal information in passwords and its security implications while it is not recommended , internet users tend to include personal information in their passwords for easy memorization . however , the use of personal information in passwords and its security implications have yet to be studied . in this paper , we dissect user passwords from several leaked data sets to investigate the extent to which a user ’s personal information resides in a password . then , we introduce a new metric called coverage to quantify the correlation between passwords and personal information . afterward , based on our analysis , we extend the probabilistic context - free grammars ( pcfgs ) method to be semantics - rich and propose personal - pcfg to crack passwords by generating personalized guesses . through offline and online attack scenarios , we demonstrate that personal - pcfg cracks passwords much faster than pcfg and makes online attacks much more likely to succeed . to defend against such semantics - aware attacks , we examine the use of simple distortion functions that are chosen by users to mitigate unwanted correlation between personal information and passwords .
First cited at 27
TOP CITED PAPERS
RANK 27
guess again ( and again and again ) : measuring password strength by simulating password - cracking algorithms text - based passwords remain the dominant authentication method in computer systems , despite significant advancement in attackers ' capabilities to perform password cracking . in response to this threat , password composition policies have grown increasingly complex . however , there is insufficient research defining metrics to characterize password strength and using them to evaluate password - composition policies . in this paper , we analyze 12,000 passwords collected under seven composition policies via an online study . we develop an efficient distributed method for calculating how effectively several heuristic password - guessing algorithms guess passwords . leveraging this method , we investigate ( a ) the resistance of passwords created under different conditions to guessing , ( b ) the performance of guessing algorithms under different training sets , ( c ) the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements , and ( d ) the relationship between guess ability , as measured with password - cracking algorithms , and entropy estimates . our findings advance understanding of both password - composition policies and metrics for quantifying password security .
RANK 146
the science of guessing : analyzing an anonymized corpus of 70 million passwords we report on the largest corpus of user - chosen passwords ever studied , consisting of anonymized password histograms representing almost 70 million yahoo ! users , mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics . this large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution . in place of previously used metrics such as shannon entropy and guessing entropy , which can not be estimated with any realistically sized sample , we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker 's desired success rate . our new metric is comparatively easy to approximate and directly relevant for security engineering . by comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack , we estimate that passwords provide fewer than 10 bits of security against an online , trawling attack , and only about 20 bits of security against an optimal offline dictionary attack . we find surprisingly little variation in guessing difficulty ; every identifiable group of users generated a comparably weak password distribution . security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality . even proactive efforts to nudge users towards better password choices with graphical feedback make little difference . more surprisingly , even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population - specific lists .
RANK 392
a large - scale empirical analysis of chinese web passwords users speaking different languages may prefer different patterns in creating their passwords , and thus knowledge on english passwords can not help to guess passwords from other languages well . research has already shown chinese passwords are one of the most difficult ones to guess . we believe that the conclusion is biased because , to the best of our knowledge , little empirical study has examined regional differences of passwords on a large scale , especially on chinese passwords . in this paper , we study the differences between passwords from chinese and english speaking users , leveraging over 100 million leaked and publicly available passwords from chinese and international websites in recent years . we found that chinese prefer digits when composing their passwords while english users prefer letters , especially lowercase letters . however , their strength against password guessing is similar . second , we observe that both users prefer to use the patterns that they are familiar with , e.g. , chinese pinyins for chinese and english words for english users . third , we observe that both chinese and english users prefer their conventional format when they use dates to construct passwords . based on these observations , we improve a pcfg ( probabilistic context - free grammar ) based password guessing method by inserting pinyins ( about 2.3 % more entries ) into the attack dictionary and insert our observed composition rules into the guessing rule set . as a result , our experiments show that the efficiency of password guessing increases by 34 % .
TOP UNCITED PAPERS
RANK 1
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK 2
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK 3
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
TOP 20
RANK = 1; score = 0.7644892930984497; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 2; score = 0.7281054854393005; correct = False; id = 66a6b8b5086454d2f511089ed3c157075239eb7d
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK = 3; score = 0.7230018973350525; correct = False; id = 142947116c7baec662984feb82b18c3812bc91f9
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
RANK = 4; score = 0.7224162817001343; correct = False; id = 8c56a28b6d287c93dc401470dadb6f74f240e29c
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
RANK = 5; score = 0.7214285731315613; correct = False; id = 13dcd0c2c035417b7ab9c493b05f5294b27b6de2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK = 6; score = 0.7187613248825073; correct = False; id = 4b7c753bb235b9aafd234e00300c942665e8e481
enf extraction from digital recordings using adaptive techniques and frequency tracking a novel forensic tool used for assessing the authenticity of digital audio recordings is known as the electric network frequency ( enf ) criterion . it involves extracting the embedded power line ( utility ) frequency from said recordings and matching it to a known database to verify the time the recording was made , and its authenticity . in this paper , a nonparametric , adaptive , and high resolution technique , known as the time - recursive iterative adaptive approach , is presented as a tool for the extraction of the enf from digital audio recordings . a comparison is made between this data dependent ( adaptive ) filter and the conventional short - time fourier transform ( stft ) . results show that the adaptive algorithm improves the enf estimation accuracy in the presence of interference from other signals . to further enhance the enf estimation accuracy , a frequency tracking method based on dynamic programming will be proposed . the algorithm uses the knowledge that the enf is varying slowly with time to estimate with high accuracy the frequency present in the recording .
RANK = 7; score = 0.7178828120231628; correct = False; id = 8db2f1e1986c1185c674f34b74df126c6c6bb4dc
the seaview security model 
RANK = 8; score = 0.7169408798217773; correct = False; id = 0e873a513c525fe556dd5660630bd330bf1d0be8
eon : modeling and analyzing dynamic access control systems with logic programs we present eon , a logic - programming language and tool that can be used to model and analyze dynamic access control systems . our language extends datalog with some carefully designed constructs that allow the introduction and transformation of new relations . for example , these constructs can model the creation of processes and objects , and the modification of their security labels at runtime . the information - flow properties of such systems can be analyzed by asking queries in this language . we show that query evaluation in eon can be reduced to decidable query satisfiability in a fragment of datalog , and further , under some restrictions , to efficient query evaluation in datalog . we implement these reductions in our tool , and demonstrate its scope through several case studies . in particular , we study in detail the dynamic access control models of the windows vista and asbestos operating systems . we also automatically prove the security of a webserver running on asbestos .
RANK = 9; score = 0.71664959192276; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 10; score = 0.7165884375572205; correct = False; id = 539265193da35286d4f46497755dc9cc6d51387c
digital single lens reflex camera identification from traces of sensor dust digital single lens reflex cameras suffer from a well - known sensor dust problem due to interchangeable lenses that they deploy . the dust particles that settle in front of the imaging sensor create a persistent pattern in all captured images . in this paper , we propose a novel source camera identification method based on detection and matching of these dust - spot characteristics . dust spots in the image are detected based on a ( gaussian ) intensity loss model and shape properties . to prevent false detections , lens parameter - dependent characteristics of dust spots are also taken into consideration . experimental results show that the proposed detection scheme can be used in identification of the source digital single lens reflex camera at low false positive rates , even under heavy compression and downsampling .
RANK = 11; score = 0.7162079215049744; correct = False; id = 31e4845a40cfa6a953aef78387b34ea3284cdff9
all your biases belong to us : breaking rc4 in wpa - tkip and tls we present new biases in rc4 , break the wi - fi protected access temporal key integrity protocol ( wpa - tkip ) , and design a practical plaintext recovery attack against the transport layer security ( tls ) protocol . to empirically find new biases in the rc4 keystream we use statistical hypothesis tests . this reveals many new biases in the initial keystream bytes , as well as several new longterm biases . our fixed - plaintext recovery algorithms are capable of using multiple types of biases , and return a list of plaintext candidates in decreasing likelihood . to break wpa - tkip we introduce a method to generate a large number of identical packets . this packet is decrypted by generating its plaintext candidate list , and using redundant packet structure to prune bad candidates . from the decrypted packet we derive the tkip mic key , which can be used to inject and decrypt packets . in practice the attack can be executed within an hour . we also attack tls as used by https , where we show how to decrypt a secure cookie with a success rate of 94 % using 9 · 227 ciphertexts . this is done by injecting known data around the cookie , abusing this using mantin ’s absab bias , and brute - forcing the cookie by traversing the plaintext candidates . using our traffic generation technique , we are able to execute the attack in merely 75 hours .
RANK = 12; score = 0.7159580588340759; correct = False; id = 06f16d9430d5f6213cf5399b167a3d989c3ff798
practical mitigations for timing - based side - channel attacks on modern x86 processors this paper studies and evaluates the extent to which automated compiler techniques can defend against timing - based side - channel attacks on modern x86 processors . we study how modern x86 processors can leak timing information through side - channels that relate to control flow and data flow . to eliminate key - dependent control flow and key - dependent timing behavior related to control flow , we propose the use of if - conversion in a compiler backend , and evaluate a proof - of - concept prototype implementation . furthermore , we demonstrate two ways in which programs that lack key - dependent control flow and key- dependent cache behavior can still leak timing information on modern x86 implementations such as the intel core 2 duo , and propose defense mechanisms against them .
RANK = 13; score = 0.7155458927154541; correct = False; id = d0a3852f95e51a6b730df75e153d8446d6e8a90a
easeandroid : automatic policy analysis and refinement for security enhanced android via large - scale semi - supervised learning mandatory protection systems such as selinux and seandroid harden operating system integrity . unfortunately , policy development is error prone and requires lengthy refinement using audit logs from deployed systems . while prior work has studied selinux policy in detail , seandroid is relatively new and has received little attention . seandroid policy engineering differs significantly from selinux : android fundamentally differs from traditional linux ; the same policy is used on millions of devices for which new audit logs are continually available ; and audit logs contain a mix of benign and malicious accesses . in this paper , we propose easeandroid , the first seandroid analytic platform for automatic policy analysis and refinement . our key insight is that the policy refinement process can be modeled and automated using semi - supervised learning . given an existing policy and a small set of known access patterns , easeandroid continually expands the knowledge base as new audit logs become available , producing suggestions for policy refinement . we evaluate easeandroid on 1.3 million audit logs from real - world devices . easeandroid successfully learns 2,518 new access patterns and generates 331 new policy rules . during this process , easeandroid discovers eight categories of attack access patterns in real devices , two of which are new attacks directly against the seandroid mac mechanism .
RANK = 14; score = 0.7153191566467285; correct = False; id = 605ed83a6d1f4eaf995e85830f373923b11d6c13
cover your acks : pitfalls of covert channel censorship circumvention in response to increasingly sophisticated methods of blocking access to censorship circumvention schemes such as tor , recently proposed systems such as skypemorph , freewave , and censorspoofer have used voice and video conferencing protocols as " cover channels " to hide proxy connections . we demonstrate that even with perfect emulation of the cover channel , these systems can be vulnerable to attacks that detect or disrupt the covert communications while having no effect on legitimate cover traffic . our attacks stem from differences in the channel requirements for the cover protocols , which are peer - to - peer and loss tolerant , and the covert traffic , which is client - proxy and loss intolerant . these differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumvention schemes .
RANK = 15; score = 0.7148329019546509; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 16; score = 0.7145960927009583; correct = False; id = 3f770cc7662340485f8fb328b3f2c95403a08e8d
alice in warningland : a large - scale field study of browser security warning effectiveness we empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature . we used mozilla firefox and google chrome ’s in - browser telemetry to observe over 25 million warning impressions in situ . during our field study , users continued through a tenth of mozilla firefox ’s malware and phishing warnings , a quarter of google chrome ’s malware and phishing warnings , and a third of mozilla firefox ’s ssl warnings . this demonstrates that security warnings can be effective in practice ; security experts and system architects should not dismiss the goal of communicating security information to end users . we also find that user behavior varies across warnings . in contrast to the other warnings , users continued through 70.2 % of google chrome ’s ssl warnings . this indicates that the user experience of a warning can have a significant impact on user behavior . based on our findings , we make recommendations for warning designers and researchers .
RANK = 17; score = 0.7141775488853455; correct = False; id = 56ecb04f003d76317ff2f9a2b614dd9fba317317
an analysis of covert timing channels 
RANK = 18; score = 0.7137991189956665; correct = False; id = 3b5c6faa99a454499e33d87cfaef9dfcb0d7b796
tapdance : end - to - middle anticensorship without flow blocking in response to increasingly sophisticated state - sponsored internet censorship , recent work has proposed a new approach to censorship resistance : end - to - middle proxying . this concept , developed in systems such as telex , decoy routing , and cirripede , moves anticensorship technology into the core of the network , at large isps outside the censoring country . in this paper , we focus on two technical obstacles to the deployment of certain end - to - middle schemes : the need to selectively block flows and the need to observe both directions of a connection . we propose a new construction , tapdance , that removes these requirements . tapdance employs a novel tcp - level technique that allows the anticensorship station at an isp to function as a passive network tap , without an inline blocking component . we also apply a novel steganographic encoding to embed control messages in tls ciphertext , allowing us to operate on https connections even under asymmetric routing . we implement and evaluate a tapdance prototype that demonstrates how the system could function with minimal impact on an isp ’s network operations .
RANK = 19; score = 0.7127364277839661; correct = False; id = 568c44678d2bba4ae9d735b555e847437a7e6f15
tor : the second - generation onion router we present tor , a circuit - based low - latency anonymous communication service . this second - generation onion routing system addresses limitations in the original design . tor adds perfect forward secrecy , congestion control , directory servers , integrity checking , configurable exit policies , and a practical design for rendezvous points . tor works on the real - world internet , requires no special privileges or kernel modifications , requires little synchronization or coordination between nodes , and provides a reasonable tradeoff between anonymity , usability , and efficiency . we briefly describe our experiences with an international network of more than a dozen hosts . we close with a list of open problems in anonymous communication .
RANK = 20; score = 0.7127241492271423; correct = False; id = 22652399c7fb219a093344b9b47028b5d0069711
the cracked cookie jar : http cookie hijacking and the exposure of private information the widespread demand for online privacy , also fueled by widely - publicized demonstrations of session hijacking attacks against popular websites , has spearheaded the increasing deployment of https . however , many websites still avoid ubiquitous encryption due to performance or compatibility issues . the prevailing approach in these cases is to force critical functionality and sensitive data access over encrypted connections , while allowing more innocuous functionality to be accessed over http . in practice , this approach is prone to flaws that can expose sensitive information or functionality to third parties . in this paper , we conduct an in - depth assessment of a diverse set of major websites and explore what functionality and information is exposed to attackers that have hijacked a user 's http cookies . we identify a recurring pattern across websites with partially deployed https , service personalization inadvertently results in the exposure of private information . the separation of functionality across multiple cookies with different scopes and inter - dependencies further complicates matters , as imprecise access control renders restricted account functionality accessible to non - session cookies . our cookie hijacking study reveals a number of severe flaws , attackers can obtain the user 's home and work address and visited websites from google , bing and baidu expose the user 's complete search history , and yahoo allows attackers to extract the contact list and send emails from the user 's account . furthermore , e - commerce vendors such as amazon and ebay expose the user 's purchase history ( partial and full respectively ) , and almost every website exposes the user 's name and email address . ad networks like doubleclick can also reveal pages the user has visited . to fully evaluate the practicality and extent of cookie hijacking , we explore multiple aspects of the online ecosystem , including mobile apps , browser security mechanisms , extensions and search bars . to estimate the extent of the threat , we run irb - approved measurements on a subset of our university 's public wireless network for 30 days , and detect over 282 k accounts exposing the cookies required for our hijacking attacks . we also explore how users can protect themselves and find that , while mechanisms such as the eff 's https everywhere extension can reduce the attack surface , http cookies are still regularly exposed . the privacy implications of these attacks become even more alarming when considering how they can be used to deanonymize tor users . our measurements suggest that a significant portion of tor users may currently be vulnerable to cookie hijacking .

