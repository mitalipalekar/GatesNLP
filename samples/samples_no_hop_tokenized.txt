RANKING 2636
QUERY
on the secret key capacity of sibling hidden markov models traditional approaches to secret key establishment based on common randomness have been based on certain restrictive assumptions , such as considering the available common randomness to consist of independent and identically distributed ( i.i.d ) repetitions of correlated random variables . unfortunately , the i.i.d assumption does not generally reflect the conditions of real - life scenarios . for this reason , the current paper investigates the key - establishment potential of a more pragmatic model , in which all parties have access to imperfect information about a common source modeled as a markov chain . each party ’s information thus comes in the form of a hidden markov model and , since the different parties share the same underlying markov chain , we call the overall model a sibling hidden markov model ( shmm ) . this paper studies upper and lower bounds on the secret key capacity for various types of shmm . the difficulty of the problem emerges from its prohibitive computational cost . to address this obstacle , we represent the joint probability of the observations as the $ l_{1}$ norm of a markov random matrix , and use its convergence to a lyapunov exponent .
First cited at 1036
TOP CITED PAPERS
RANK 1036
secret common randomness from routing metadata in ad hoc networks establishing secret common randomness between two or multiple devices in a network resides at the root of communication security . in its most frequent form of key establishment , the problem is traditionally decomposed into a randomness generation stage ( randomness purity is subject to employing often costly true random number generators ) and an information - exchange agreement stage , which relies either on public - key infrastructure or on symmetric encryption ( key wrapping ) . in this paper , we propose a secret - common - randomness establishment algorithm for ad hoc networks , which works by harvesting randomness directly from the network routing metadata , thus achieving both pure randomness generation and ( implicitly ) secret - key agreement . our algorithm relies on the route discovery phase of an ad hoc network employing the dynamic source routing protocol , is lightweight , and requires relatively little communication overhead . the algorithm is evaluated for various network parameters in an opnet ad hoc network simulator . our results show that , in just 10 min , thousands of secret random bits can be generated network - wide , between different pairs in a network of 50 users .
TOP UNCITED PAPERS
RANK 1
native client : a sandbox for portable , untrusted x86 native code 
RANK 2
detecting and characterizing social spam campaigns 
RANK 3
anonymous connections and onion routing 
TOP 20
RANK = 1; score = 0.9418196678161621; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 2; score = 0.9240087866783142; correct = False; id = 9b5478a7a9188ccf34a5375efac9991e2cbec4c3
detecting and characterizing social spam campaigns 
RANK = 3; score = 0.9197561144828796; correct = False; id = c5e183451e30e42d74e3f1e4d811e1156f286e51
anonymous connections and onion routing 
RANK = 4; score = 0.905384361743927; correct = False; id = d8b677c3d84ed96b1a1ad118902fb114d032ec20
amandroid : a precise and general inter - component data flow analysis framework for security vetting of android apps 
RANK = 5; score = 0.9047116637229919; correct = False; id = 881b35590eec7217dc4477bfe2f786e292283da4
users really do plug in usb drives they find we investigate the anecdotal belief that end users will pick up and plug in usb flash drives they find by completing a controlled experiment in which we drop 297 flash drives on a large university campus . we find that the attack is effective with an estimated success rate of 45 -- 98 % and expeditious with the first drive connected in less than six minutes . we analyze the types of drives users connected and survey those users to understand their motivation and security profile . we find that a drive 's appearance does not increase attack success . instead , users connect the drive with the altruistic intention of finding the owner . these individuals are not technically incompetent , but are rather typical community members who appear to take more recreational risks then their peers . we conclude with lessons learned and discussion on how social engineering attacks -- while less technical -- continue to be an effective attack vector that our community has yet to successfully address .
RANK = 6; score = 0.9045475125312805; correct = False; id = f3f41c433bf0da5dc5507f89a26713acc0efbe83
it 's no secret . measuring the security and reliability of authentication via " secret " questions 
RANK = 7; score = 0.9045355319976807; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 8; score = 0.9038375020027161; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 9; score = 0.9038248658180237; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 10; score = 0.9034944176673889; correct = False; id = 2824b6a3d0096b0b522f4b7a7659b5f792f93d8f
" you might also like : " privacy risks of collaborative filtering many commercial websites use recommender systems to help customers locate products and content . modern recommenders are based on collaborative filtering : they use patterns learned from users ' behavior to make recommendations , usually in the form of related - items lists . the scale and complexity of these systems , along with the fact that their outputs reveal only relationships between items ( as opposed to information about users ) , may suggest that they pose no meaningful privacy risk . in this paper , we develop algorithms which take a moderate amount of auxiliary information about a customer and infer this customer 's transactions from temporal changes in the public outputs of a recommender system . our inference attacks are passive and can be carried out by any internet user . we evaluate their feasibility using public data from popular websites hunch , last . fm , library thing , and amazon .
RANK = 11; score = 0.8972984552383423; correct = False; id = 4681a0116597fd0804b07e8176b8761e4f569743
password cracking using probabilistic context - free grammars choosing the most effective word - mangling rules to use when performing a dictionary - based password cracking attack can be a difficult task . in this paper we discuss a new method that generates password structures in highest probability order . we first automatically create a probabilistic context - free grammar based upon a training set of previously disclosed passwords . this grammar then allows us to generate word - mangling rules , and from them , password guesses to be used in password cracking . we will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets . in one series of experiments , training on a set of disclosed passwords , our approach was able to crack 28 % to 129 % more passwords than john the ripper , a publicly available standard password cracking program .
RANK = 12; score = 0.8971645832061768; correct = False; id = 82fb79a008dd44fa35a6c0cd55235c3ced84de1d
sok : deep packer inspection : a longitudinal study of the complexity of run - time packers run - time packers are often used by malware - writers to obfuscate their code and hinder static analysis . the packer problem has been widely studied , and several solutions have been proposed in order to generically unpack protected binaries . nevertheless , these solutions commonly rely on a number of assumptions that may not necessarily reflect the reality of the packers used in the wild . moreover , previous solutions fail to provide useful information about the structure of the packer or its complexity . in this paper , we describe a framework for packer analysis and we propose a taxonomy to measure the runtime complexity of packers . we evaluated our dynamic analysis system on two datasets , composed of both off - the - shelf packers and custom packed binaries . based on the results of our experiments , we present several statistics about the packers complexity and their evolution over time .
RANK = 13; score = 0.8963049650192261; correct = False; id = 15d80376d14be131abc5e73ffd126d4b60eccc19
burst oram : minimizing oram response times for bursty access patterns we present burst oram , the first oblivious cloud storage system to achieve both practical response times and low total bandwidth consumption for bursty workloads . for real - world workloads , burst oram can attain response times that are nearly optimal and orders of magnitude lower than the best existing oram systems by reducing online bandwidth costs and aggressively rescheduling shuffling work to delay the bulk of the io until idle periods . we evaluate our design on an enterprise file system trace with about 7,500 clients over a 15 day period , comparing to an insecure baseline encrypted block store without oram . we show that when baseline response times are low , burst oram response times are comparably low . in a 32 tb oram with 50ms network latency and sufficient bandwidth capacity to ensure 90 % of requests have baseline response times under 53ms , 90 % of burst oram requests have response times under 63ms , while requiring only 30 times the total bandwidth consumption of the insecure baseline . similarly , with sufficient bandwidth to ensure 99.9 % of requests have baseline responses under 70ms , 99.9 % of burst oram requests have response times under 76ms .
RANK = 14; score = 0.8961853981018066; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 15; score = 0.8940222263336182; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 16; score = 0.8937385678291321; correct = False; id = 08580d047002ba1f10bcbb812e92f2785a6239d9
where you are is who you are : user identification by matching statistics most users of online services have unique behavioral or usage patterns . these behavioral patterns can be exploited to identify and track users by using only the observed patterns in the behavior . we study the task of identifying users from statistics of their behavioral patterns . in particular , we focus on the setting in which we are given histograms of users ' data collected during two different experiments . we assume that , in the first data set , the users ' identities are anonymized or hidden and that , in the second data set , their identities are known . we study the task of identifying the users by matching the histograms of their data in the first data set with the histograms from the second data set . in recent works , the optimal algorithm for this user identification task is introduced . in this paper , we evaluate the effectiveness of this method on three different types of data sets with up to 50 000 users , and in multiple scenarios . using data sets such as call data records , web browsing histories , and gps trajectories , we demonstrate that a large fraction of users can be easily identified given only histograms of their data ; hence , these histograms can act as users ' fingerprints . we also verify that simultaneous identification of users achieves better performance compared with one - by - one user identification . furthermore , we show that using the optimal method for identification indeed gives higher identification accuracy than the heuristics - based approaches in the practical scenarios . the accuracy obtained under this optimal method can thus be used to quantify the maximum level of user identification that is possible in such settings . we show that the key factors affecting the accuracy of the optimal identification algorithm are the duration of the data collection , the number of users in the anonymized data set , and the resolution of the data set . we also analyze the effectiveness of k - anonymization in resisting user identification attacks on these data sets .
RANK = 17; score = 0.8935102224349976; correct = False; id = 38b764da3ce3dd291e0470318652fe89587e05f3
crossing the “ valley of death ” : transitioning research into commercial products : a personal perspective many researchers with innovative ideas just never seem to be able to bring those ideas to market . one of the biggest problems with the cyber security research community is transitioning technology into commercial product . this paper discusses these technology transition activities from the view of a program manager and offers several examples of successful transition for consideration .
RANK = 18; score = 0.8931824564933777; correct = False; id = 005b52bd01af724cc1b8f22e3b33545417a3d5fb
liar , liar , coins on fire ! : penalizing equivocation by loss of bitcoins we show that equivocation , i.e. , making conflicting statements to others in a distributed protocol , can be monetarily disincentivized by the use of crypto - currencies such as bitcoin . to this end , we design completely decentralized non - equivocation contracts , which make it possible to penalize an equivocating party by the loss of its money . at the core of these contracts , there is a novel cryptographic primitive called accountable assertions , which reveals the party 's bitcoin credentials if it equivocates . non - equivocation contracts are particularly useful for distributed systems that employ public append - only logs to protect data integrity , e.g. , in cloud storage and social networks . moreover , as double - spending in bitcoin is a special case of equivocation , the contracts enable us to design a payment protocol that allows a payee to receive funds at several unsynchronized points of sale , while being able to penalize a double - spending payer after the fact .
RANK = 19; score = 0.8928968906402588; correct = False; id = 22b716af826fa94f8d139dff4c292da61acec6ad
sok : everyone hates robocalls : a survey of techniques against telephone spam telephone spam costs united states consumers $ 8.6 billion annually . in 2014 , the federal trade commission has received over 22 million complaints of illegal and wanted calls . telephone spammers today are leveraging recent technical advances in the telephony ecosystem to distribute massive automated spam calls known as robocalls . given that anti - spam techniques and approaches are effective in the email domain , the question we address is : what are the effective defenses against spam calls ? in this paper , we first describe the telephone spam ecosystem , specifically focusing on the differences between email and telephone spam . then , we survey the existing telephone spam solutions and , by analyzing the failings of the current techniques , derive evaluation criteria that are critical to an acceptable solution . we believe that this work will help guide the development of effective telephone spam defenses , as well as provide a framework to evaluate future defenses .
RANK = 20; score = 0.8928662538528442; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix

RANKING 1424
QUERY
bridging knowledge gaps in neural entailment via symbolic models most textual entailment models focus on lexical gaps between the premise text and the hypothesis , but rarely on knowledge gaps . we focus on filling these knowledge gaps in the science entailment task , by leveraging an external structured knowledge base ( kb ) of science facts . our new architecture combines standard neural entailment models with a knowledge lookup module . to facilitate this lookup , we propose a fact - level decomposition of the hypothesis , and verifying the resulting sub - facts against both the textual premise and the structured kb . our model , nsnet , learns to aggregate predictions from these heterogeneous data formats . on the scitail dataset , nsnet outperforms a simpler combination of the two predictions by 3 % and the base entailment model by 5 % .
First cited at 79
TOP CITED PAPERS
RANK 79
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 213
a large annotated corpus for learning natural language inference understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations . however , machine learning research in this area has been dramatically limited by the lack of large - scale resources . to address this , we introduce the stanford natural language inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning . at 570 k pairs , it is two orders of magnitude larger than all other resources of its type . this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models , and it allows a neural network - based model to perform competitively on natural language inference benchmarks for the first time .
RANK 1699
a decomposable attention model for natural language inference we propose a simple neural architecture for natural language inference . our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable . on the stanford natural language inference ( snli ) dataset , we obtain state - of - the - art results with almost an order of magnitude fewer parameters than previous work and without relying on any word - order information . adding intra - sentence attention that takes a minimum amount of order into account yields further improvements .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK 3
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
TOP 20
RANK = 1; score = 0.9978793859481812; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9973402619361877; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 3; score = 0.9970471262931824; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 4; score = 0.9970136880874634; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 5; score = 0.9968649744987488; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 6; score = 0.9967076778411865; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 7; score = 0.9964780211448669; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 8; score = 0.9964269995689392; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 9; score = 0.9962999224662781; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 10; score = 0.9962592124938965; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 11; score = 0.9961410164833069; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 12; score = 0.9960029721260071; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .
RANK = 13; score = 0.9959442019462585; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 14; score = 0.9958348274230957; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 15; score = 0.9956306219100952; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 16; score = 0.9956259727478027; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 17; score = 0.9955244064331055; correct = False; id = 0b544dfe355a5070b60986319a3f51fb45d1348e
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK = 18; score = 0.9954019784927368; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 19; score = 0.9953384399414062; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 20; score = 0.9951213002204895; correct = False; id = 5b31e43f8b0490779d8013e0705ae4df8f0488c9
shallow parsing with conditional random fields conditional random fields for sequence labeling offer advantages over both generative models like hmms and classifiers applied at each sequence position . among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods . we show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the conll task , and better than any reported single model . improved training methods based on modern optimization algorithms were critical in achieving these results . we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models .

RANKING 2599
QUERY
xerox day vulnerability in the area of espionage between countries , an infiltration covert channel used to trigger a silent malware installed on a network of a critical organization ( such as 911 services and missile launching facility ) from the outside world is extremely dangerous to the target country ’s security . in order to prevent attackers from establishing such a channel , these organizations take various steps to secure their networks , to make the establishment of this type of covert channel very challenging and almost impractical to achieve ; the current state of the art methods are very limited and ineffective . in this paper , we show that even a strong isolation technique , such as air - gapping the network , can be circumvented by using an organizational multifunction printer ( mfp ) to establish an infiltration covert channel in order to communicate with a malware installed on an isolated organization from the outside . we show how an attacker can leverage the light sensitivity of an mfp and use different light sources to infiltrate commands to the malware in the organization . we analyze the influence of light intensity , distance , transmission rate , ambient light , and wavelength on the covert channel . in addition we demonstrate the attack on a real organization using : 1 ) a laser attached to a tripod stand ; 2 ) a laser carried by a drone ; and 3 ) a hijacked smart bulb that is not even connected to the organization ’s network and is accessed and controlled by an attacker in a passing car . we prove that locating the scanner in an inner room inside an organization does not prevent an attacker from establishing the covert channel . we show how our covert channel can be established from a greater distance ( 900 m ) and at a higher transmission rate of 200 bits / s than other methods used to infiltrate data to an organization , even using invisible light ( covertly ) .
First cited at 171
TOP CITED PAPERS
RANK 171
covert channels through random number generator : mechanisms , capacity estimation and mitigations covert channels present serious security threat because they allow secret communication between two malicious processes even if the system inhibits direct communication . we describe , implement and quantify a new covert channel through shared hardware random number generation ( rng ) module that is available on modern processors . we demonstrate that a reliable , high - capacity and low - error covert channel can be created through the rng module that works across cpu cores and across virtual machines . we quantify the capacity of the rng channel under different settings and show that transmission rates in the range of 7 - 200 kbit / s can be achieved depending on a particular system used for transmission , assumptions , and the load level . finally , we describe challenges in mitigating the rng channel , and propose several mitigation approaches both in software and hardware .
RANK 230
keyboards and covert channels this paper introduces jitterbugs , a class of inline interception mechanisms that covertly transmit data by perturbing the timing of input events likely to affect externally observable network traffic . jitterbugs positioned at input devices deep within the trusted environment ( e.g. , hidden in cables or connectors ) can leak sensitive data without compromising the host or its software . in particular , we show a practical keyboard jitterbugthat solves the data exfiltration problem for keystroke loggers by leaking captured passwords through small variations in the precise times at which keyboard events are delivered to the host . whenever an interactive communication application ( such as ssh , telnet , instant messaging , etc ) is running , a receiver monitoring the host ’s network traffic can recover the leaked data , even when the session or link is encrypted . our experiments suggest that simple keyboard jitterbugs can be a practical technique for capturing and exfiltrating typed secrets under conventional oses and interactive network applications , even when the receiver is many hops away on the internet .
RANK 1212
thermal covert channels on multi - core platforms side channels remain a challenge to information flow control and security in modern computing platforms . resource partitioning techniques that minimise the number of shared resources among processes are often used to address this challenge . in this work , we focus on multicore platforms and we demonstrate that even seemingly strong isolation techniques based on dedicated cores and memory can be circumvented through the use of thermal side channels . specifically , we show that the processor core temperature can be used both as a side channel as well as a covert communication channel even when the system implements strong spatial and temporal partitioning . our experiments on an x86-based platform demonstrate covert thermal channels that achieve up to 12.5 bps and a weak side channel that can detect processes executed on neighbouring cores . this work therefore shows a limitation in the isolation that can be achieved on existing multi - core systems .
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK 3
native client : a sandbox for portable , untrusted x86 native code 
TOP 20
RANK = 1; score = 0.9641276001930237; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.963281512260437; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 3; score = 0.9631156325340271; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 4; score = 0.9630646705627441; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 5; score = 0.9629762172698975; correct = False; id = 2f95e2ca11610cb334d8d777d7b0f0d5561e67bc
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK = 6; score = 0.9629578590393066; correct = False; id = 660ad810c69affa189f567e76ff83af682228703
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
RANK = 7; score = 0.9629040360450745; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 8; score = 0.9628473520278931; correct = False; id = 0c5de0e5cb46e862b933c6bd543cc15695506034
automatic patch - based exploit generation is possible : techniques and implications the automatic patch - based exploit generation problem is : given a program p and a patched version of the program p ' , automatically generate an exploit for the potentially unknown vulnerability present in p but fixed in p ' . in this paper , we propose techniques for automatic patch - based exploit generation , and show that our techniques can automatically generate exploits for 5 microsoft programs based upon patches provided via windows update . although our techniques may not work in all cases , a fundamental tenant of security is to conservatively estimate the capabilities of attackers . thus , our results indicate that automatic patch - based exploit generation should be considered practical . one important security implication of our results is that current patch distribution schemes which stagger patch distribution over long time periods , such as windows update , may allow attackers who receive the patch first to compromise the significant fraction of vulnerable hosts who have not yet received the patch .
RANK = 9; score = 0.9623478055000305; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 10; score = 0.9622894525527954; correct = False; id = 67f961f98d34fea3ab15f473429a5156b62b5c65
vigilare : toward snoop - based kernel integrity monitor in this paper , we present vigilare system , a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware . this snoop - based monitoring enabled by the vigilare system , overcomes the limitations of the snapshot - based monitoring employed in previous kernel integrity monitoring solutions . being based on inspecting snapshots collected over a certain interval , the previous hardware - based monitoring solutions can not detect transient attacks that can occur in between snapshots . we implemented a prototype of the vigilare system on gaisler 's grlib - based system - on - a - chip ( soc ) by adding snooper hardware connections module to the host system for bus snooping . to evaluate the benefit of snoop - based monitoring , we also implemented similar soc with a snapshot - based monitor to be compared with . the vigilare system detected all the transient attacks without performance degradation while the snapshot - based monitor could not detect all the attacks and induced considerable performance degradation as much as 10 % in our tuned stream benchmark test .
RANK = 11; score = 0.9622803926467896; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 12; score = 0.9621439576148987; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 13; score = 0.9611335396766663; correct = False; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 14; score = 0.9608915448188782; correct = False; id = 4681a0116597fd0804b07e8176b8761e4f569743
password cracking using probabilistic context - free grammars choosing the most effective word - mangling rules to use when performing a dictionary - based password cracking attack can be a difficult task . in this paper we discuss a new method that generates password structures in highest probability order . we first automatically create a probabilistic context - free grammar based upon a training set of previously disclosed passwords . this grammar then allows us to generate word - mangling rules , and from them , password guesses to be used in password cracking . we will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets . in one series of experiments , training on a set of disclosed passwords , our approach was able to crack 28 % to 129 % more passwords than john the ripper , a publicly available standard password cracking program .
RANK = 15; score = 0.9606996178627014; correct = False; id = 0dfe28b14b7c35fc1b876954a09ff6b7e47a8ee9
openconflict : preventing real time map hacks in online games we present a generic tool , kartograph , that lifts the fog of war in online real - time strategy games by snooping on the memory used by the game . kartograph is passive and can not be detected remotely . motivated by these passive attacks , we present secure protocols for distributing game state among players so that each client only has data it is allowed to see . our system , open conflict , runs real - time games with distributed state . to support our claim that open conflict is sufficiently fast for real - time strategy games , we show the results of an extensive study of 1000 replays of star craft ii games between expert players . at the peak of a typical game , open conflict needs only 22 milliseconds on one cpu core each time state is synchronized .
RANK = 16; score = 0.9605780243873596; correct = False; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 17; score = 0.96043461561203; correct = False; id = 64205427d0f900997ec0a22fdd4946a3ba16f1b9
hey , you have a problem : on the feasibility of large - scale web vulnerability notification large - scale discovery of thousands of vulnerable web sites has become a frequent event , thanks to recent advances in security research and the rise in maturity of internet - wide scanning tools . the issues related to disclosing the vulnerability information to the affected parties , however , have only been treated as a side note in prior research . in this paper , we systematically examine the feasibility and efficacy of large - scale notification campaigns . for this , we comprehensively survey existing communication channels and evaluate their usability in an automated notification process . using a data set of over 44,000 vulnerable web sites , we measure success rates , both with respect to the total number of fixed vulnerabilities and to reaching responsible parties , with the following highlevel results : although our campaign had a statistically significant impact compared to a control group , the increase in the fix rate of notified domains is marginal . if a notification report is read by the owner of the vulnerable application , the likelihood of a subsequent resolution of the issues is sufficiently high : about 40 % . but , out of 35,832 transmitted vulnerability reports , only 2,064 ( 5.8 % ) were actually received successfully , resulting in an unsatisfactory overall fix rate , leaving 74.5 % of web applications exploitable after our month - long experiment . thus , we conclude that currently no reliable notification channels exist , which significantly inhibits the success and impact of large - scale notification .
RANK = 18; score = 0.9603914022445679; correct = False; id = 2f7b92282d42d645ee2d4bc34aa2bf132275e82b
libfte : a toolkit for constructing practical , format - abiding encryption schemes encryption schemes where the ciphertext must abide by a specified format have diverse applications , ranging from in - place encryption in databases to per - message encryption of network traffic for censorship circumvention . despite this , a unifying framework for deploying such encryption schemes has not been developed . one consequence of this is that current schemes are ad - hoc ; another is a requirement for expert knowledge that can disuade one from using encryption at all . we present a general - purpose library ( called libfte ) that aids engineers in the development and deployment of format - preserving encryption ( fpe ) and formattransforming encryption ( fte ) schemes . it incorporates a new algorithmic approach for performing fpe / fte using the nondeterministic finite - state automata ( nfa ) representation of a regular expression when specifying formats . this approach was previously considered unworkable , and our approach closes this open problem . we evaluate libfte and show that , compared to other encryption solutions , it introduces negligible latency overhead , and can decrease diskspace usage by as much as 62.5 % when used for simultaneous encryption and compression in a postgresql database ( both relative to conventional encryption mechanisms ) . in the censorship circumvention setting we show that , using regularexpression formats lifted from the snort ids , libfte can reduce client / server memory requirements by as much as 30 % .
RANK = 19; score = 0.9603356719017029; correct = False; id = 0fd2467de521b52805eea902edc9587c87818276
discoverer : automatic protocol reverse engineering from network traces application - level protocol specifications are useful for many security applications , including intrusion prevention and detection that performs deep packet inspection and traffic normalization , and penetration testing that generates network inputs to an application to uncover potential vulnerabilities . however , current practice in deriving protocol specifications is mostly manual . in this paper , we present discoverer , a tool for automatically reverse engineering the protocol message formats of an application from its network trace . a key property of discoverer is that it operates in a protocol - independent fashion by inferring protocol idioms commonly seen in message formats of many application - level protocols . we evaluated the efficacy of discoverer over one text protocol ( http ) and two binary protocols ( rpc and cifs / smb ) by comparing our inferred formats with true formats obtained from ethereal [ 5 ] . for all three protocols , more than 90 % of our inferred formats correspond to exactly one true format ; one true format is reflected in five inferred formats on average ; our inferred formats cover over 95 % of messages , which belong to 30 - 40 % of true formats observed in the trace .
RANK = 20; score = 0.960175096988678; correct = False; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .

RANKING 1474
QUERY
duorc : towards complex language understanding with paraphrased reading comprehension we propose duorc , a novel dataset for reading comprehension ( rc ) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing rc datasets . duorc contains 186,089 unique questionanswer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie one from wikipedia and the other from imdb written by two different authors . we asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version . this unique characteristic of duorc where questions and answers are created from different versions of a document narrating the same underlying story , ensures by design , that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version . further , since the two versions have different levels of plot detail , narration style , vocabulary , etc . , answering questions from the second version requires deeper language understanding and incorporating external background knowledge . additionally , the narrative style of passages arising from movie plots ( as opposed to typical descriptive passages in existing datasets ) exhibits the need to perform complex reasoning over events across multiple sentences . indeed , we observe that state - of - the - art neural rc models which have achieved near human performance on the squad dataset ( rajpurkar et al . , 2016b ) , even when coupled with traditional nlp techniques to address the challenges presented in duorc exhibit very poor performance ( f1 score of 37.42 % on duorc v / s 86 % on squad dataset ) . this opens up several interesting research avenues wherein duorc could complement other rc datasets to explore novel neural approaches for studying language understanding .
First cited at 69
TOP CITED PAPERS
RANK 69
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 568
mctest : a challenge dataset for the open - domain machine comprehension of text we present mctest , a freely available set of stories and associated questions intended for research on the machine comprehension of text . previous work on machine comprehension ( e.g. , semantic modeling ) has made great strides , but primarily focuses either on limited - domain datasets , or on solving a more restricted goal ( e.g. , open - domain relation extraction ) . in contrast , mctest requires machines to answer multiple - choice reading comprehension questions about fictional stories , directly tackling the high - level goal of open - domain machine comprehension . reading comprehension can test advanced abilities such as causal reasoning and understanding the world , yet , by being multiple - choice , still provide a clear metric . by being fictional , the answer typically can be found only in the story itself . the stories and questions are also carefully limited to those a young child would understand , reducing the world knowledge that is required for the task . we present the scalable crowd - sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions . by screening workers ( with grammar tests ) and stories ( with grading ) , we have ensured that the data is the same quality as another set that we manually edited , but at one tenth the editing cost . by being open - domain , yet carefully restricted , we hope mctest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text . 1 reading comprehension a major goal for nlp is for machines to be able to understand text as well as people . several research disciplines are focused on this problem : for example , information extraction , relation extraction , semantic role labeling , and recognizing textual entailment . yet these techniques are necessarily evaluated individually , rather than by how much they advance us towards the end goal . on the other hand , the goal of semantic parsing is the machine comprehension of text ( mct ) , yet its evaluation requires adherence to a specific knowledge representation , and it is currently unclear what the best representation is , for open - domain text . we believe that it is useful to directly tackle the top - level task of mct . for this , we need a way to measure progress . one common method for evaluating someone ’s understanding of text is by giving them a multiple - choice reading comprehension test . this has the advantage that it is objectively gradable ( vs. essays ) yet may test a range of abilities such as causal or counterfactual reasoning , inference among relations , or just basic understanding of the world in which the passage is set . therefore , we propose a multiple - choice reading comprehension task as a way to evaluate progress on mct . we have built a reading comprehension dataset containing 500 fictional stories , with 4 multiple choice questions per story . it was built using methods which can easily scale to at least 5000 stories , since the stories were created , and the curation was done , using crowd sourcing almost entirely , at a total of $ 4.00 per story . we plan to periodically update the dataset to ensure that methods are not overfitting to the existing data . the dataset is open - domain , yet restricted to concepts and words that a 7 year old is expected to understand . this task is still beyond the capability of today ’s computers and algorithms .
RANK 3176
modeling biological processes for reading comprehension machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web - scale corpora . in this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document . the input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process . to answer the questions , we first predict a rich structure representing the process in the paragraph . then , we map the question to a formal query , which is executed against the predicted structure . we demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 3
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
TOP 20
RANK = 1; score = 0.9948023557662964; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9938837885856628; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 3; score = 0.993118166923523; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 4; score = 0.9929922819137573; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 5; score = 0.992332935333252; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9917783141136169; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 7; score = 0.9913320541381836; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 8; score = 0.9902328252792358; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 9; score = 0.9893998503684998; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 10; score = 0.9893543720245361; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 11; score = 0.989219605922699; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 12; score = 0.9892168045043945; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 13; score = 0.988386332988739; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 14; score = 0.9880140423774719; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 15; score = 0.9878359436988831; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 16; score = 0.9873591065406799; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 17; score = 0.98638516664505; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 18; score = 0.9859760999679565; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 19; score = 0.9857622385025024; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 20; score = 0.9855085015296936; correct = False; id = 10007391361e17f41aa92f3430c00830dff3be6a
three generative , lexicalised models for statistical parsing in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context - free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh - movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision / recall , an average improvement of 2.3 % over ( collins 96 ) .

RANKING 2619
QUERY
energy - constrained swipt networks : enhancing physical layer security with fd self - jamming in this paper , we investigate the secrecy performance of energy - constrained wireless - powered networks with considering the passive eavesdropping scenario , where the simultaneous wireless information and power transfer - based full - duplex self - jamming ( swipt - fdsj ) scheme is developed . the maximal ratio transmission protocol is applied at the multi - antenna source such that the wireless signals are designated to the destination directly . besides , the energy harvesting and full - duplex self - jamming operations are adopted at the energy - constrained destination to prolong its lifetime as well as to confuse the eavesdropper . specifically , the exact and asymptotic closed - form expressions of the connection outage probability ( cop ) , the secrecy outage probability ( sop ) , and the secrecy throughput of the proposed system are obtained , based on which we optimize the time - switching ratio to maximize the secrecy throughput . we also degenerate the proposed swipt - fdsj scheme to the reduced half - duplex with no self - jamming ( hdnsj ) scheme . the finds suggest that in the hdnsj scheme , adding the antenna number of the source only benefits the cop performance , but has no impact on the sop performance . by contrast , it will promote the cop and sop performance at the same time in the swipt - fdsj scheme , which eventually results in the great improvement of secrecy throughput . in addition , we present the practical application condition of the swipt - fdsj scheme . it is demonstrated that the secrecy throughput performance of the swipt - fdsj scheme is much superior to the hdnsj scheme on condition that the application condition is satisfied .
First cited at 2258
TOP CITED PAPERS
RANK 2258
physical layer security in three - tier wireless sensor networks : a stochastic geometry approach this paper develops a tractable framework for exploiting the potential benefits of physical layer security in three - tier wireless sensor networks ( wsns ) using stochastic geometry . in such networks , the sensing data from the remote sensors are collected by sinks with the help of access points , and the external eavesdroppers intercept the data transmissions . we focus on the secure transmission in two scenarios : 1 ) the active sensors transmit their sensing data to the access points and 2 ) the active access points forward the data to the sinks . we derive new compact expressions for the average secrecy rate in these two scenarios . we also derive a new compact expression for the overall average secrecy rate . numerical results corroborate our analysis and show that multiple antennas at the access points can enhance the security of three - tier wsns . our results show that increasing the number of access points decreases the average secrecy rate between the access point and its associated sink . however , we find that increasing the number of access points first increases the overall average secrecy rate , with a critical value beyond which the overall average secrecy rate then decreases . when increasing the number of active sensors , both the average secrecy rate between the sensor and its associated access point , and the overall average secrecy rate decrease . in contrast , increasing the number of sinks improves both the average secrecy rate between the access point and its associated sink , and the overall average secrecy rate .
RANK 2733
physical layer network security in the full - duplex relay system this paper investigates the secrecy performance of full - duplex relay ( fdr ) networks . the resulting analysis shows that fdr networks have better secrecy performance than half duplex relay networks , if the self - interference can be well suppressed . we also propose a full duplex jamming relay network , in which the relay node transmits jamming signals while receiving the data from the source . while the full duplex jamming scheme has the same data rate as the half duplex scheme , the secrecy performance can be significantly improved , making it an attractive scheme when the network secrecy is a primary concern . a mathematic model is developed to analyze secrecy outage probabilities for the half duplex , the full duplex and full duplex jamming schemes , and the simulation results are also presented to verify the analysis .
RANK 3299
large system secrecy rate analysis for swipt mimo wiretap channels in this paper , we study the multiple - input multiple - output wiretap channel for simultaneous wireless information and power transfer , in which there is a base station ( bs ) , an information - decoding ( id ) user , and an energy - harvesting ( eh ) user . the messages intended to the id user is required to be kept confidential to the eh user . our objective is to design the optimal transmit covariance matrix at the bs for maximizing the ergodic secrecy rate subject to the harvested energy requirement for the eh user exploiting only statistical channel state information at the bs . to this end , we begin by deriving an approximation for the ergodic secrecy rate using large - dimensional random matrix theory and the method of taylor series expansion . this approximation enables us to derive the asymptotic - optimal transmit covariance matrix that achieves the tradeoff for ergodic secrecy rate and harvested energy . the simulation results are provided to verify the accuracy of the approximation and show that a bigger rate - energy region can be achieved when the rician factor increases or the path loss exponent decreases . we also show that when the transmit correlation increases or the distance between the eavesdropper and the bs decreases , the harvested energy will be increased , while the achieved ergodic secrecy rate decreases .
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
native client : a sandbox for portable , untrusted x86 native code 
RANK 3
it 's no secret . measuring the security and reliability of authentication via " secret " questions 
TOP 20
RANK = 1; score = 0.9133365750312805; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.9105757474899292; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 3; score = 0.879349410533905; correct = False; id = f3f41c433bf0da5dc5507f89a26713acc0efbe83
it 's no secret . measuring the security and reliability of authentication via " secret " questions 
RANK = 4; score = 0.8734291195869446; correct = False; id = d8b677c3d84ed96b1a1ad118902fb114d032ec20
amandroid : a precise and general inter - component data flow analysis framework for security vetting of android apps 
RANK = 5; score = 0.864686906337738; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 6; score = 0.8643101453781128; correct = False; id = c5e183451e30e42d74e3f1e4d811e1156f286e51
anonymous connections and onion routing 
RANK = 7; score = 0.863209068775177; correct = False; id = 34944a2b185ef2ad607a4c887b415bb0b064a237
defeating tcp / ip stack fingerprinting 
RANK = 8; score = 0.8607562780380249; correct = False; id = 1bcd8e3b2b40434a6f0eef98d23817f56f5f7600
detecting covert channels in computer networks based on chaos theory covert channels via the widely used tcp / ip protocols have become a new challenging issue for network security . in this paper , we analyze the information hiding in tcp / ip protocols and propose a new effective method to detect the existence of hidden information in tcp initial sequence numbers ( isns ) , which is known as one of the most difficult covert channels to be detected . our method uses phase space reconstruction to create a processing space called reconstructed phase space , where a statistical model is proposed for detecting covert channels in tcp isns . based on the model , a classification algorithm is developed to identify the existence of information hidden in isns . simulation results have demonstrated that our proposed detection method outperforms the state - of - the - art technique in terms of high detection accuracy and greatly reduced computational complexity . instead of offline processing as the state - of - the - art does , our new scheme can be used for online detection .
RANK = 9; score = 0.8566604256629944; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 10; score = 0.8561533093452454; correct = False; id = d3c01656f6953cd3693cbe229d8d82ea3a15c735
timing attacks against trusted path 
RANK = 11; score = 0.8537359833717346; correct = False; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 12; score = 0.8529682755470276; correct = False; id = 0a7b3f3f9c7bf19fa6d817f2ddee9d5f7664053b
using model checking to analyze network vulnerabilities 
RANK = 13; score = 0.8526716232299805; correct = False; id = 21c8b3550cb5e082321c05d9c97ab5a4947c17c0
a secure environment for untrusted helper applications 
RANK = 14; score = 0.8520633578300476; correct = False; id = c11d647ada7683410d255552f935c51c2ec08b96
puppetnets : misusing web browsers as a distributed attack infrastructure 
RANK = 15; score = 0.8515011072158813; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 16; score = 0.8513563275337219; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 17; score = 0.8499558568000793; correct = False; id = eaa3716f93672db4d06c7f68088ac9145c3a418f
sendmail without the superuser 
RANK = 18; score = 0.8496978282928467; correct = False; id = 90ecaa350a7adbc4e128fcc2eafe98729f0257f3
do private and portable web browsers leave incriminating evidence ? a forensic analysis of residual artifacts from private and portable web browsing sessions 
RANK = 19; score = 0.8486436009407043; correct = False; id = 568c44678d2bba4ae9d735b555e847437a7e6f15
tor : the second - generation onion router we present tor , a circuit - based low - latency anonymous communication service . this second - generation onion routing system addresses limitations in the original design . tor adds perfect forward secrecy , congestion control , directory servers , integrity checking , configurable exit policies , and a practical design for rendezvous points . tor works on the real - world internet , requires no special privileges or kernel modifications , requires little synchronization or coordination between nodes , and provides a reasonable tradeoff between anonymity , usability , and efficiency . we briefly describe our experiences with an international network of more than a dozen hosts . we close with a list of open problems in anonymous communication .
RANK = 20; score = 0.8480634093284607; correct = False; id = 4a87acb3e2e0a82f1cb31f309e9e732ad0705d54
tipping pennies ? privately practical anonymous micropayments we design and analyze the first practical anonymous payment mechanisms for network services . we start by reporting on our experience with the implementation of a routing micropayment solution for tor . we then propose micropayment protocols of increasingly complex requirements for networked services , such as p2p or cloud - hosted services . the solutions are efficient , with bandwidth and latency overheads of under 4 % and 0.9 ms , respectively , in the orpay implementation , provide full anonymity ( for both payers and payees ) , and support thousands of transactions per second .

RANKING 822
QUERY
cross - topic argument mining from heterogeneous sources argument mining is a core technology for automating argument search in large document collections . despite its usefulness for this task , most current approaches are designed for use onlywith specific text types and fall short when applied to heterogeneous texts . in this paper , we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary web texts . we source annotations for over 25,000 instances covering eight controversial topics . we show that integrating topic information into bidirectional long short - termmemory networks outperforms vanilla bilstms by more than 3 percentage points in f1 in twoand three - label cross - topic settings . we also show that these results can be further improved by leveraging additional data for topic relevance using multi - task learning .
First cited at 31
TOP CITED PAPERS
RANK 31
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK 55
learning whom to trust with mace non - expert annotation services like amazon ’s mechanical turk ( amt ) are cheap and fast ways to evaluate systems and provide categorical annotations for training data . unfortunately , some annotators choose bad labels in order to maximize their pay . manual identification is tedious , so we experiment with an item - response model . it learns in an unsupervised fashion to a ) identify which annotators are trustworthy and b ) predict the correct underlying labels . we match performance of more complex state - of - the - art systems and perform well even under adversarial conditions . we show considerable improvements over standard baselines , both for predicted label accuracy and trustworthiness estimates . the latter can be further improved by introducing a prior on model parameters and using variational bayes inference . additionally , we can achieve even higher accuracy by focusing on the instances our model is most confident in ( trading in some recall ) , and by incorporating annotated control instances . our system , mace ( multi - annotator competence estimation ) , is available for download1 .
RANK 760
identifying argumentative discourse structures in persuasive essays in this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays . the structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations . we consider this task in two consecutive steps . first , we identify the components of arguments using multiclass classification . second , we classify a pair of argument components as either support or non - support for identifying the structure of argumentative discourse . for both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features . in our experiments , we obtain a macro f1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .
TOP UNCITED PAPERS
RANK 1
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 2
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK 3
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
TOP 20
RANK = 1; score = 0.9955794215202332; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 2; score = 0.9952150583267212; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 3; score = 0.9951794147491455; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 4; score = 0.9948849081993103; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 5; score = 0.9947230815887451; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.994584858417511; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 7; score = 0.9938335418701172; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 8; score = 0.9934984445571899; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 9; score = 0.9934300780296326; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 10; score = 0.9934176206588745; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 11; score = 0.992656409740448; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 12; score = 0.9925260543823242; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 13; score = 0.9924381375312805; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 14; score = 0.9920068383216858; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 15; score = 0.9919454455375671; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 16; score = 0.9916444420814514; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 17; score = 0.991519033908844; correct = False; id = 3cad59ed9faf5194c499ae8a3d57caece152360f
hadoopperceptron : a toolkit for distributed perceptron training and prediction with mapreduce we propose a set of open - source software modules to perform structured perceptron training , prediction and evaluation within the hadoop framework . apache hadoop is a freely available environment for running distributed applications on a computer cluster . the software is designed within the map - reduce paradigm . thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data - sets . the distributed perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial perceptron . the presented modules can be executed as stand - alone software or easily extended or integrated in complex systems . the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs .
RANK = 18; score = 0.9914374947547913; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 19; score = 0.9912272095680237; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 20; score = 0.9911403059959412; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .

RANKING 1223
QUERY
neural network based extreme classification and similarity models for product matching matching a seller listed item to an appropriate product has become a fundamental and one of the most significant step for e - commerce platforms for product based experience . it has a huge impact on making the search effective , search engine optimization , providing product reviews and product price estimation etc . along with many other advantages for a better user experience . as significant and vital it has become , the challenge to tackle the complexity has become huge with the exponential growth of individual and business sellers trading millions of products everyday . we explored two approaches ; classification based on shallow neural network and similarity based on deep siamese network . these models outperform the baseline by more than 5 % in term of accuracy and are capable of extremely efficient training and inference .
First cited at 68
TOP CITED PAPERS
RANK 68
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 7502
structuring e - commerce inventory large e - commerce enterprises feature millions of items entered daily by a large variety of sellers . while some sellers provide rich , structured descriptions of their items , a vast majority of them provide unstructured natural language descriptions . in the paper we present a 2 steps method for structuring items into descriptive properties . the first step consists in unsupervised property discovery and extraction . the second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm . we evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall .
RANK 12178
learning text similarity with siamese recurrent networks this paper presents a deep architecture for learning a similarity metric on variablelength character sequences . the model combines a stack of character - level bidirectional lstm ’s with a siamese architecture . it learns to project variablelength strings into a fixed - dimensional embedding space by using only information about the similarity between pairs of strings . this model is applied to the task of job title normalization based on a manually annotated taxonomy . a small data set is incrementally expanded and augmented with new sources of variance . the model learns a representation that is selective to differences in the input that reflect semantic differences ( e.g. , “ java developer ” vs. “ hr manager ” ) but also invariant to nonsemantic string differences ( e.g. , “ java developer ” vs. “ java programmer ” ) .
TOP UNCITED PAPERS
RANK 1
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 2
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 3
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
TOP 20
RANK = 1; score = 0.9946592450141907; correct = False; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 2; score = 0.9945213794708252; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 3; score = 0.9938874840736389; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 4; score = 0.9936162233352661; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 5; score = 0.9931870698928833; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9929250478744507; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 7; score = 0.991665780544281; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 8; score = 0.9910532832145691; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 9; score = 0.9909441471099854; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 10; score = 0.9908577799797058; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 11; score = 0.9902865886688232; correct = False; id = 3b8e7c8220d3883d54960d896a73045f3c70ac17
discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms we describe new algorithms for training tagging models , as an alternative to maximum - entropy models or conditional random elds ( crfs ) . the algorithms rely on viterbi decoding of training examples , combined with simple additive updates . we describe theory justifying the algorithms through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems . we give experimental results on part - of - speech tagging and base noun phrase chunking , in both cases showing improvements over results for a maximum - entropy tagger .
RANK = 12; score = 0.9901216626167297; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 13; score = 0.9898498058319092; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 14; score = 0.9894116520881653; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 15; score = 0.9893346428871155; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 16; score = 0.988834798336029; correct = False; id = deebb6cf201b3d591bf4f3a8d2258e1d5178999b
ontonotes : the 90 % solution we describe the ontonotes methodology and its result , a large multilingual richly - annotated corpus constructed at 90 % interannotator agreement . an initial portion ( 300 k words of english newswire and 250 k words of chinese newswire ) will be made available to the community during 2007 .
RANK = 17; score = 0.9887194037437439; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 18; score = 0.987873375415802; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 19; score = 0.9878359436988831; correct = False; id = f0e8a010e276bf10f54d3006a2b4ae89c7e2b9d5
* sem 2013 shared task : semantic textual similarity in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar . this year we set up two tasks : ( i ) a core task ( core ) , and ( ii ) a typed - similarity task ( typed ) . core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets . typed , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description . several types of similarity have been defined , including similar author , similar time period or similar location . the annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % . the core task attracted 34 participants with 89 runs , and the typed task attracted 6 teams with 14 runs .
RANK = 20; score = 0.9877346158027649; correct = False; id = 3cad59ed9faf5194c499ae8a3d57caece152360f
hadoopperceptron : a toolkit for distributed perceptron training and prediction with mapreduce we propose a set of open - source software modules to perform structured perceptron training , prediction and evaluation within the hadoop framework . apache hadoop is a freely available environment for running distributed applications on a computer cluster . the software is designed within the map - reduce paradigm . thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data - sets . the distributed perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial perceptron . the presented modules can be executed as stand - alone software or easily extended or integrated in complex systems . the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs .

RANKING 1425
QUERY
on the automatic generation of medical imaging reports medical imaging is widely used in clinical practice for diagnosis and treatment . report - writing can be error - prone for unexperienced physicians , and timeconsuming and tedious for experienced physicians . to address these issues , we study the automatic generation of medical imaging reports . this task presents several challenges . first , a complete report contains multiple heterogeneous forms of information , including findings and tags . second , abnormal regions in medical images are difficult to identify . third , the reports are typically long , containing multiple sentences . to cope with these challenges , we ( 1 ) build a multi - task learning framework which jointly performs the prediction of tags and the generation of paragraphs , ( 2 ) propose a co - attention mechanism to localize regions containing abnormalities and generate narrations for them , ( 3 ) develop a hierarchical lstm model to generate long paragraphs . we demonstrate the effectiveness of the proposed methods on two publicly available datasets .
First cited at 3
TOP CITED PAPERS
RANK 3
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK 10927
multi - timescale long short - term memory neural network for modelling sentences and documents neural network based methods have obtained great progress on a variety of natural language processing tasks . however , it is still a challenge task to model long texts , such as sentences and documents . in this paper , we propose a multi - timescale long short - term memory ( mt - lstm ) neural network to model long texts . mtlstm partitions the hidden states of the standard lstm into several groups . each group is activated at different time periods . thus , mt - lstm can model very long documents as well as short sentences . experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task .
RANK 12448
meteor universal : language specific translation evaluation for any target language this paper describes meteor universal , released for the 2014 acl workshop on statistical machine translation . meteor universal brings language specific evaluation to previously unsupported target languages by ( 1 ) automatically extracting linguistic resources ( paraphrase tables and function word lists ) from the bitext used to train mt systems and ( 2 ) using a universal parameter set learned from pooling human judgments of translation quality from several language directions . meteor universal is shown to significantly outperform baseline bleu on two new languages , russian ( wmt13 ) and hindi ( wmt14 ) .
TOP UNCITED PAPERS
RANK 1
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK 2
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK 4
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
TOP 20
RANK = 1; score = 0.9656917452812195; correct = False; id = 033b62167e7358c429738092109311af696e9137
thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , “ subtle nuances ” ) and a negative semantic orientation when it has bad associations ( e.g. , “ very cavalier ” ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “ excellent ” minus the mutual information between the given phrase and the word “ poor ” . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .
RANK = 2; score = 0.9538106322288513; correct = False; id = 6e0b2b32aa3eed696aa868386d485321a63ccebb
word association norms , mutual information , and lexicography the term word association is used in a very particular sense in the psycholinguistic literature . ( generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor . ) we will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor / nurse type ( content word / content word ) to lexico - syntactic co - occurrence constraints between verbs and prepositions ( content word / function word ) . this paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora . ( the standard method of obtaining word association norms , testing a few thousand : mbjects on a few hundred words , is both costly and unreliable . ) the proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .
RANK = 3; score = 0.950713038444519; correct = True; id = 8ff93cfd37dced279134c9d642337a2085b31f59
bleu : a method for automatic evaluation of machine translation human evaluations of machine translation are extensive but expensive . human evaluations can take months to finish and involve human labor that can not be reused . we propose a method of automatic machine translation evaluation that is quick , inexpensive , and language - independent , that correlates highly with human evaluation , and that has little marginal cost per run . we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
RANK = 4; score = 0.9492130875587463; correct = False; id = 4f1fe957a29a2e422d4034f4510644714d33fb20
thumbs up ? sentiment classification using machine learning techniques we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human - produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic - based categorization . we conclude by examining factors that make the sentiment classification problem more challenging . publication info : proceedings of emnlp 2002 , pp . 79–86 .
RANK = 5; score = 0.9467428922653198; correct = False; id = 24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK = 6; score = 0.9464161992073059; correct = False; id = 18b534c7207a1376fa92e87fe0d2cfb358d98c51
accurate unlexicalized parsing we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown , by making use of simple , linguistically motivated state splits , which break down false independence assumptions latent in a vanilla treebank grammar . indeed , its performance of 86.36 % ( lp / lr f 1 ) is better than that of earlylexicalizedpcfg models , and surprisingly close to the current state - of - theart . this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact , easier to replicate , and easier to interpret than more complex lexical models , and the parsing algorithms are simpler , more widely understood , of lower asymptotic complexity , and easier to optimize . in the early 1990s , as probabilistic methods swept nlp , parsing work revived the investigation of probabilistic context - free grammars ( pcfgs ) ( booth and thomson , 1973 ; baker , 1979 ) . however , early results on the utility ofpcfgs for parse disambiguation and language modeling were somewhat disappointing . a conviction arose that lexicalizedpcfgs ( where head words annotate phrasal nodes ) were the key tool for high performancepcfg parsing . this approach was congruent with the great success of word n - gram models in speech recognition , and drew strength from a broader interest in lexicalized grammars , as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asppattachments ( ford et al . , 1982 ; hindle and rooth , 1993 ) . in the following decade , great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models ( magerman , 1995 ; charniak , 1997 ; collins , 1999 ; charniak , 2000 ; charniak , 2001 ) . however , several results have brought into question how large a role lexicalization plays in such parsers . johnson ( 1998 ) showed that the performance of anunlexicalizedpcfgover the penn treebank could be improved enormously simply by annotating each node by its parent category . the penn treebank coveringpcfgis a poor tool for parsing because the context - freedom assumptions it embodies are far too strong , and weakening them in this way makes the model much better . more recently , gildea ( 2001 ) discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0.5 % for test text from the same domain as the training data , and not at all for test text from a different domain . 1 but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful , for example in hindle and rooth ’s demonstration fromppattachment . we take this as a reflection of the fundamental sparseness of the lexical dependency information available in the penn treebank . as a speech person would say , one million words of training data just is n’t enough . even for topics central to the treebank ’s wall street journal text , such as stocks , many very plausible dependencies occur only once , for example stocks stabilized , while many others occur not at all , for example stocks skyrocketed .2 the best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the 1there are minor differences , but all the current best - known lexicalized pcfgs employ bothmonolexicalstatistics , which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item , and bilexicalstatistics , or dependencies , which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word . 2this observation motivates various classor similaritybased approaches to combating sparseness , and this remains a promising avenue of work , but success in this area has proven somewhat elusive , and , at any rate , current lexicalized pcfgs do simply use exact word matches if available , and interpola te with syntactic category - based estimates when they are not . 3in this paper we use the term subcategorizationin the original general sense of chomsky ( 1965 ) , for where a syntactic ca tcategories appearing in the penn treebank . charniak ( 2000 ) shows the value his parser gains from parentannotation of nodes , suggesting that this information is at least partly complementary to information derivable from lexicalization , and collins ( 1999 ) uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg , such as differentiating “ basenps ” from noun phrases with phrasal modifiers , and distinguishing sentences with empty subjects from those where there is an overt subject np . while he gives incomplete experimental results as to their efficacy , we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization . in this paper , we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated , and is , indeed , much higher than community wisdom has thought possible . we describe several simple , linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models . specifically , we construct anunlexicalizedpcfg which outperforms the lexicalized pcfgs of magerman ( 1995 ) and collins ( 1996 ) ( though not more recent models , such as charniak ( 1997 ) or collins ( 1999 ) ) . one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg . to the extent that no such strong baseline has been provided , the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing , rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data . secondly , this result affirms the value of linguistic analysis for feature discovery . the result has other uses and advantages : an unlexicalized pcfgis easier to interpret , reason about , and improve than the more complex lexicalized models . the grammar representation is much more compact , no longer requiring large structures that store lexicalized probabilities . the parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories , for example di viding verb phrases into finite and non - finite verb phrases , rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators . 4o(n3 ) vs. o(n5 ) for a naive implementation , or vs. o(n4 ) if using the clever approach of eisner and satta ( 1999 ) . constants . an unlexicalizedpcfg parser is much simpler to build and optimize , including both standard code optimization techniques and the investigation of methods for search space pruning ( caraballo and charniak , 1998 ; charniak et al . , 1998 ) . it is not our goal to argue against the use of lexicalized probabilities in high - performance probabilistic parsing . it has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities , and a parser should make use of such information where possible . we focus here on using unlexicalized , tructural context because we feel that this information has been underexploited and underappreciated . we see this investigation as only one part of the foundation for state - of - the - art parsing which employsboth lexical and structural conditioning . 1 experimental setup to facilitate comparison with previous work , we trained our models on sections 2–21 of the wsjsection of the penn treebank . we used the first 20 files ( 393 sentences ) of section 22 as a development set ( devset ) . this set is small enough that there is noticeable variance in individual results , but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill - climb . all of section 23 was used as a test set for the final model . for each model , input trees were annotated or transformed in some way , as in johnson ( 1998 ) . given a set of transformed trees , we viewed the local trees as grammar rewrite rules in the standard way , and used ( unsmoothed ) maximum - likelihood estimates for rule probabilities . 5 to parse the grammar , we used a simple array - based java implementation of a generalizedcky parser , which , for our final best model , was able to exhaustively parse all sentences in section 23 in 1 gb of memory , taking approximately 3 sec for average length sentences . 6 5the tagging probabilities were smoothed to accommodate unknown words . the quantityp(tag|word ) was estimated as follows : words were split into one of several categories wordclass , based on capitalization , suffix , digit , and other character features . for each of these categories , we took th e maximum - likelihood estimate of p(tag|wordclass ) . this distribution was used as a prior against which observed tagging s , if any , were taken , givingp(tag|word ) = [ c(tag , word ) + κ p(tag|wordclass)]/[c(word)+κ ] . this was then inverted to give p(word|tag ) . the quality of this tagging model impacts all numbers ; for example the raw treebank grammar ’s devset f 1 is 72.62 with it and 72.09 without it . 6the parser is available for download as open source at : http://nlp.stanford.edu/downloads/lex-parser.shtml
RANK = 7; score = 0.936443030834198; correct = False; id = 0c6c7583687c245aedbe894edf63541fdda122ea
opinionfinder : a system for subjectivity analysis vancouver , october 2005 . opinionfinder : a system for subjectivity analysis theresa wilson‡ , paul hoffmann‡ , swapna somasundaran† , jason kessler† , janyce wiebe†‡ , yejin choi§ , claire cardie§ , ellen riloff∗ , siddharth patwardhan∗ ‡intelligent systems program , university of pittsburgh , pittsburgh , pa 15260 †department of computer science , university of pittsburgh , pittsburgh , pa 15260 § department of computer science , cornell university , ithaca , ny 14853 ∗school of computing , university of utah , salt lake city , ut 84112
RANK = 8; score = 0.9234078526496887; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 9; score = 0.9212064146995544; correct = False; id = 0aacc118744a102db0782da73b1592d0e567d960
espresso : leveraging generic patterns for automatically harvesting semantic relations in this paper , we present espresso , a weakly - supervised , general - purpose , and accurate algorithm for harvesting semantic relations . the main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm . we present an empirical comparison of espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations . experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .
RANK = 10; score = 0.9187049865722656; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 11; score = 0.9104595184326172; correct = False; id = 167e1359943b96b9e92ee73db1df69a1f65d731d
a sentimental education : sentiment analysis using subjectivity summarization based on minimum cuts sentiment analysis seeks to identify the viewpoint(s ) underlying a text span ; an example application is classifying a movie review as “ thumbs up ” or “ thumbs down ” . to determine this sentiment polarity , we propose a novel machine - learning method that applies text - categorization techniques to just the subjective portions of the document . extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross - sentence contextual constraints . publication info : proceedings of the acl , 2004 .
RANK = 12; score = 0.8935894966125488; correct = False; id = 303b0b6e6812c60944a4ac9914222ac28b0813a2
recognizing contextual polarity in phrase - level sentiment analysis this paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions . with this approach , the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions , achieving results that are significantly better than baseline .
RANK = 13; score = 0.8916491866111755; correct = False; id = 94db635f54d25bdb95edb42185aca93ba53b051b
biographies , bollywood , boom - boxes and blenders : domain adaptation for sentiment classification automatic sentiment classification has been extensively studied and applied in recent years . however , sentiment is expressed differently in different domains , and annotating corpora for every possible domain of interest is impractical . we investigate domain adaptation for sentiment classifiers , focusing on online reviews for different types of products . first , we extend to sentiment classification the recently - proposed structural correspondence learning ( scl ) algorithm , reducing the relative error due to adaptation between domains by an average of 30 % over the original scl algorithm and 46 % over a supervised baseline . second , we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another . this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains .
RANK = 14; score = 0.8905177116394043; correct = False; id = 255d6867cb5c57810c909d5e488c9ae86e0d6d3e
the nombank project : an interim report this paper describes nombank , a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus . nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus . the university of pennsylvania ’s propbank , nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . this paper describes the nombank project in detail including its specifications and the process involved in creating the resource .
RANK = 15; score = 0.8879150748252869; correct = False; id = 096d7d2e9b3fbc37f1c4e75b1896ae3797950ef9
an empirical study of smoothing techniques for language modeling we present a tutorial introduction to n - gram models for language modeling and survey the most widely - used smoothing algorithms for such models . we then present an extensive empirical comparison of several of these smoothing techniques , including those described by jelinek and mercer ( 1980 ) , katz ( 1987 ) , bell , cleary , and witten ( 1990 ) , ney , essen , and kneser ( 1994 ) , and kneser and ney ( 1995 ) . we investigate how factors such as training data size , training corpus ( e.g. , brown versus wall street journal ) , count cuto s , and n - gram order ( bigram versus trigram ) a ect the relative performance of these methods , which is measured through the cross - entropy of test data . our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance . we introduce methodologies for analyzing smoothing algorithm e cacy in detail , and using these techniques we motivate a novel variation of kneser - ney smoothing that consistently outperforms all other algorithms evaluated . finally , results showing that improved language model smoothing leads to improved speech recognition performance are presented .
RANK = 16; score = 0.8826987743377686; correct = False; id = 4c15b129a8da55127e4e2fe47f54799d0a313367
word representations : a simple and general method for semi - supervised learning if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off - the - shelf use in existing nlp systems , as well as our code , here : http://metaoptimize . com / projects / wordreprs/
RANK = 17; score = 0.8783063292503357; correct = False; id = 05bd81538cbf475f47e1a2a8f416b271ae494bbb
domain adaptation with structural correspondence learning saturday , july 22 , 2006 7:45–9:00 registration 8:15–8:25 welcome from the organizers 8:25–10:30 sessions 1a and 1b 10:30–11:00 morning coffee break 11:00–12:15 sessions 2a and 2b 12:15–1:45 lunch 1:45–2:35 sessions 3a and 3b 2:35–3:30 invited talk 3:30–4:00 afternoon coffee break 4:00–6:30 long poster session 1 and welcome reception sunday , july 23 , 2006 8:25–10:30 sessions 4a and 4b 10:30–11:00 morning coffee break 11:00–12:30 short poster session 2 12:30–1:45 lunch 1:45–3:25 sessions 5a and 5b 3:30 - 4:00 afternoon coffee break 4:00 - 5:15 sessions 6a and 6b
RANK = 18; score = 0.8760600686073303; correct = False; id = f6c035de37e797f7e5167d655b246a2f9426a9eb
reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency . however , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers . this task can be addressed with natural language processing technology to assess reading level . existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models . in this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .
RANK = 19; score = 0.8738493919372559; correct = False; id = 0ecb33ced5b0976accdf13817151f80568b6fdcb
coarse - to - fine n - best parsing and maxent discriminative reranking discriminative reranking is one method for constructing high - performance statistical parsers ( collins , 2000 ) . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( charniak , 2000 ) . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker ( johnson et al . , 1999 ; riezler et al . , 2002 ) that selects the best parse from the set of parses for each sentence , obtaining an f - score of 91.0 % on sentences of length 100 or less .
RANK = 20; score = 0.8727049231529236; correct = False; id = 2ae6014a451801671d41b6171f86e657d8b1fbaf
wordnet : : similarity - measuring the relatedness of concepts wordnet::similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts ( or word senses ) . it provides six measures of similarity , and three measures of relatedness , all of which are based on the lexical database wordnet . these measures are implemented as perl modules which take as input two concepts , and return a numeric value that represents the degree to which they are similar or related .

RANKING 1098
QUERY
release the kraken : new kracks in the 802.11 standard we improve key reinstallation attacks ( kracks ) against 802.11 by generalizing known attacks , systematically analyzing all handshakes , bypassing 802.11 's official countermeasure , auditing ( flawed ) patches , and enhancing attacks using implementation - specific bugs . last year it was shown that several handshakes in the 802.11 standard were vulnerable to key reinstallation attacks . these attacks manipulate handshake messages to reinstall an already - in - use key , leading to both nonce reuse and replay attacks . we extend this work in several directions . first , we generalize attacks against the 4-way handshake so they no longer rely on hard - to - win race conditions , and we employ a more practical method to obtain the required man - in - the - middle ( mitm ) position . second , we systematically investigate the 802.11 standard for key reinstallation vulnerabilities , and show that the fast initial link setup ( fils ) and tunneled direct - link setup peerkey ( tpk ) handshakes are also vulnerable to key reinstallations . these handshakes increase roaming speed , and enable direct connectivity between clients , respectively . third , we abuse wireless network management ( wnm ) power - save features to trigger reinstallations of the group key . moreover , we bypass ( and improve ) the official countermeasure of 802.11 . in particular , group key reinstallations were still possible by combining eapol - key and wnm - sleep frames . we also found implementation - specific flaws that facilitate key reinstallations . for example , some devices reuse the anonce and snonce in the 4-way handshake , accept replayed message 4 's , or improperly install the group key . we conclude that preventing key reinstallations is harder than expected , and believe that ( formally ) modeling 802.11 would help to better secure both implementations and the standard itself .
First cited at 715
TOP CITED PAPERS
RANK 715
predicting , decrypting , and abusing wpa2/802.11 group keys we analyze the generation and management of 802.11 group keys . these keys protect broadcast and multicast wi - fi traffic . we discovered several issues and illustrate their importance by decrypting all group ( and unicast ) traffic of a typical wi - fi network . first we argue that the 802.11 random number generator is flawed by design , and provides an insufficient amount of entropy . this is confirmed by predicting randomly generated group keys on several platforms . we then examine whether group keys are securely transmitted to clients . here we discover a downgrade attack that forces usage of rc4 to encrypt the group key when transmitted in the 4-way handshake . the per - message rc4 key is the concatenation of a public 16-byte initialization vector with a secret 16-byte key , and the first 256 keystream bytes are dropped . we study this peculiar usage of rc4 , and find that capturing 231 handshakes can be sufficient to recover ( i.e. , decrypt ) a 128-bit group key . we also examine whether group traffic is properly isolated from unicast traffic . we find that this is not the case , and show that the group key can be used to inject and decrypt unicast traffic . finally , we propose and study a new random number generator tailored for 802.11 platforms .
RANK 1498
the final nail in wep 's coffin the 802.11 encryption standard wired equivalent privacy ( wep ) is still widely used today despite the numerous discussions on its insecurity . in this paper , we present a novel vulnerability which allows an attacker to send arbitrary data on a wep network after having eavesdropped a single data packet . furthermore , we present techniques for real - time decryption of data packets , which may be used under common circumstances . vendor produced mitigation techniques which cause frequent wep re - keying prevent traditional attacks , whereas our attack remains effective even in such scenarios . we implemented a fully automatic version of this attack which demonstrates its practicality and feasibility in real networks . as even rapidly re - keyed networks can be quickly compromised , we believe wep must now be abandoned rather than patched yet again
RANK 2785
on the security of rc4 in tls 
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK 3
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
TOP 20
RANK = 1; score = 0.9627494215965271; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.9611863493919373; correct = False; id = 2f95e2ca11610cb334d8d777d7b0f0d5561e67bc
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK = 3; score = 0.9611098170280457; correct = False; id = 660ad810c69affa189f567e76ff83af682228703
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
RANK = 4; score = 0.9607051014900208; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 5; score = 0.9606536030769348; correct = False; id = 0c5de0e5cb46e862b933c6bd543cc15695506034
automatic patch - based exploit generation is possible : techniques and implications the automatic patch - based exploit generation problem is : given a program p and a patched version of the program p ' , automatically generate an exploit for the potentially unknown vulnerability present in p but fixed in p ' . in this paper , we propose techniques for automatic patch - based exploit generation , and show that our techniques can automatically generate exploits for 5 microsoft programs based upon patches provided via windows update . although our techniques may not work in all cases , a fundamental tenant of security is to conservatively estimate the capabilities of attackers . thus , our results indicate that automatic patch - based exploit generation should be considered practical . one important security implication of our results is that current patch distribution schemes which stagger patch distribution over long time periods , such as windows update , may allow attackers who receive the patch first to compromise the significant fraction of vulnerable hosts who have not yet received the patch .
RANK = 6; score = 0.9606512784957886; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 7; score = 0.9606041312217712; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 8; score = 0.9603551626205444; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 9; score = 0.9603332281112671; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 10; score = 0.9601985812187195; correct = False; id = 67f961f98d34fea3ab15f473429a5156b62b5c65
vigilare : toward snoop - based kernel integrity monitor in this paper , we present vigilare system , a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware . this snoop - based monitoring enabled by the vigilare system , overcomes the limitations of the snapshot - based monitoring employed in previous kernel integrity monitoring solutions . being based on inspecting snapshots collected over a certain interval , the previous hardware - based monitoring solutions can not detect transient attacks that can occur in between snapshots . we implemented a prototype of the vigilare system on gaisler 's grlib - based system - on - a - chip ( soc ) by adding snooper hardware connections module to the host system for bus snooping . to evaluate the benefit of snoop - based monitoring , we also implemented similar soc with a snapshot - based monitor to be compared with . the vigilare system detected all the transient attacks without performance degradation while the snapshot - based monitor could not detect all the attacks and induced considerable performance degradation as much as 10 % in our tuned stream benchmark test .
RANK = 11; score = 0.9593883752822876; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 12; score = 0.958938479423523; correct = False; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 13; score = 0.958624005317688; correct = False; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
RANK = 14; score = 0.9585959315299988; correct = False; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 15; score = 0.9584711194038391; correct = False; id = 2f7b92282d42d645ee2d4bc34aa2bf132275e82b
libfte : a toolkit for constructing practical , format - abiding encryption schemes encryption schemes where the ciphertext must abide by a specified format have diverse applications , ranging from in - place encryption in databases to per - message encryption of network traffic for censorship circumvention . despite this , a unifying framework for deploying such encryption schemes has not been developed . one consequence of this is that current schemes are ad - hoc ; another is a requirement for expert knowledge that can disuade one from using encryption at all . we present a general - purpose library ( called libfte ) that aids engineers in the development and deployment of format - preserving encryption ( fpe ) and formattransforming encryption ( fte ) schemes . it incorporates a new algorithmic approach for performing fpe / fte using the nondeterministic finite - state automata ( nfa ) representation of a regular expression when specifying formats . this approach was previously considered unworkable , and our approach closes this open problem . we evaluate libfte and show that , compared to other encryption solutions , it introduces negligible latency overhead , and can decrease diskspace usage by as much as 62.5 % when used for simultaneous encryption and compression in a postgresql database ( both relative to conventional encryption mechanisms ) . in the censorship circumvention setting we show that , using regularexpression formats lifted from the snort ids , libfte can reduce client / server memory requirements by as much as 30 % .
RANK = 16; score = 0.9581260085105896; correct = False; id = 66e5f253c0fd73548b4cec637f683b1186740f07
obfuscation of executable code to improve resistance to static disassembly a great deal of software is distributed in the form of executable code . the ability to reverse engineer such executables can create opportunities for theft of intellectual property via software piracy , as well as security breaches by allowing attackers to discover vulnerabilities in an application . the process of reverse engineering an executable program typically begins with disassembly , which translates machine code to assembly code . this is then followed by various decompilation steps that aim to recover higher - level abstractions from the assembly code . most of the work to date on code obfuscation has focused on disrupting or confusing the decompilation phase . this paper , by contrast , focuses on the initial disassembly phase . our goal is to disrupt the static disassembly process so as to make programs harder to disassemble correctly . we describe two widely used static disassembly algorithms , and discuss techniques to thwart each of them . experimental results indicate that significant portions of executables that have been obfuscated using our techniques are disassembled incorrectly , thereby showing the efficacy of our methods .
RANK = 17; score = 0.9581094980239868; correct = False; id = 0dfe28b14b7c35fc1b876954a09ff6b7e47a8ee9
openconflict : preventing real time map hacks in online games we present a generic tool , kartograph , that lifts the fog of war in online real - time strategy games by snooping on the memory used by the game . kartograph is passive and can not be detected remotely . motivated by these passive attacks , we present secure protocols for distributing game state among players so that each client only has data it is allowed to see . our system , open conflict , runs real - time games with distributed state . to support our claim that open conflict is sufficiently fast for real - time strategy games , we show the results of an extensive study of 1000 replays of star craft ii games between expert players . at the peak of a typical game , open conflict needs only 22 milliseconds on one cpu core each time state is synchronized .
RANK = 18; score = 0.9579547047615051; correct = False; id = 0fd2467de521b52805eea902edc9587c87818276
discoverer : automatic protocol reverse engineering from network traces application - level protocol specifications are useful for many security applications , including intrusion prevention and detection that performs deep packet inspection and traffic normalization , and penetration testing that generates network inputs to an application to uncover potential vulnerabilities . however , current practice in deriving protocol specifications is mostly manual . in this paper , we present discoverer , a tool for automatically reverse engineering the protocol message formats of an application from its network trace . a key property of discoverer is that it operates in a protocol - independent fashion by inferring protocol idioms commonly seen in message formats of many application - level protocols . we evaluated the efficacy of discoverer over one text protocol ( http ) and two binary protocols ( rpc and cifs / smb ) by comparing our inferred formats with true formats obtained from ethereal [ 5 ] . for all three protocols , more than 90 % of our inferred formats correspond to exactly one true format ; one true format is reflected in five inferred formats on average ; our inferred formats cover over 95 % of messages , which belong to 30 - 40 % of true formats observed in the trace .
RANK = 19; score = 0.9579468369483948; correct = False; id = 09a546c94f95c43d4878675d28c5e3b0bb27eb09
shibboleth : private mailing list manager we describe shibboleth , a program to manage private internet mailing lists . differing from other mailing list managers , shibboleth manages lists or groups of lists which are closed , or have membership by invitation only . so instead of focusing on automating the processes of subscribing and unsubscribing readers , we include features like smtp forgery detection , prevention of outsiders’ ability to harvest usable email addresses from mailing list archives , and support for cryptographic strength user authentication and nonrepudiation .
RANK = 20; score = 0.9578403830528259; correct = False; id = 64205427d0f900997ec0a22fdd4946a3ba16f1b9
hey , you have a problem : on the feasibility of large - scale web vulnerability notification large - scale discovery of thousands of vulnerable web sites has become a frequent event , thanks to recent advances in security research and the rise in maturity of internet - wide scanning tools . the issues related to disclosing the vulnerability information to the affected parties , however , have only been treated as a side note in prior research . in this paper , we systematically examine the feasibility and efficacy of large - scale notification campaigns . for this , we comprehensively survey existing communication channels and evaluate their usability in an automated notification process . using a data set of over 44,000 vulnerable web sites , we measure success rates , both with respect to the total number of fixed vulnerabilities and to reaching responsible parties , with the following highlevel results : although our campaign had a statistically significant impact compared to a control group , the increase in the fix rate of notified domains is marginal . if a notification report is read by the owner of the vulnerable application , the likelihood of a subsequent resolution of the issues is sufficiently high : about 40 % . but , out of 35,832 transmitted vulnerability reports , only 2,064 ( 5.8 % ) were actually received successfully , resulting in an unsatisfactory overall fix rate , leaving 74.5 % of web applications exploitable after our month - long experiment . thus , we conclude that currently no reliable notification channels exist , which significantly inhibits the success and impact of large - scale notification .

RANKING 1455
QUERY
misery digraphs : delaying intrusion attacks in obscure clouds when remote command injection attacks succeed at the entry points of a cloud ( servers exposed to the outside internet ) , attackers targeting a specific asset in the cloud will pursue further exploration to find their targets . attack targets , such as database servers , are often running on separate machines , forcing an extra step for a successful attack . however , compromising two or three machines is all an attacker needs to reach an isolated database through a simple attack path . the goal of this paper is to investigate the possibility of frustrating attackers by constructing a cloud network architecture that hides the path to a target asset in the network , utilizing multiple moving decoy virtual machines and confusing firewall configurations . a deceiving cloud network architecture can significantly delay attacks ( by stretching the attack path from a handful of steps to thousands ) , providing time for system administrators to intervene and resolve the intrusion . this paper introduces the concept of misery digraphs , which provide a theoretical foundation for creating intrusion deception in clouds . this paper describes the necessary steps to convert a cloud to one that includes a misery digraph , and evaluates the feasibility and effectiveness of using the approach with amazon web services . our simulation results demonstrate that for a cloud implementing misery digraphs with a simple attack path of length five , there is a 91 % probability that an attack requires at least 1000 steps to reach the target .
First cited at 246
TOP CITED PAPERS
RANK 246
from patches to honey - patches : lightweight attacker misdirection , deception , and disinformation traditional software security patches often have the unfortunate side - effect of quickly alerting attackers that their attempts to exploit patched vulnerabilities have failed . attackers greatly benefit from this information ; it expedites their search for unpatched vulnerabilities , it allows them to reserve their ultimate attack payloads for successful attacks , and it increases attacker confidence in stolen secrets or expected sabotage resulting from attacks . to overcome this disadvantage , a methodology is proposed for reformulating a broad class of security patches into honey - patches - patches that offer equivalent security but that frustrate attackers ' ability to determine whether their attacks have succeeded or failed . when an exploit attempt is detected , the honey - patch transparently and efficiently redirects the attacker to an unpatched decoy , where the attack is allowed to succeed . the decoy may host aggressive software monitors that collect important attack information , and deceptive files that disinform attackers . an implementation for three production - level web servers , including apache http , demonstrates that honey - patching can be realized for large - scale , performance - critical software applications with minimal overheads .
RANK 519
using graphic turing tests to counter automated ddos attacks against web servers we present websos , a novel overlay - based architecture that provides guaranteed access to a web server that is targeted by a denial of service ( dos ) attack . our approach exploits two key characteristics of the web environment : its design around a human - centric interface , and the extensibility inherent in many browsers through downloadable " applets . " we guarantee access to a web server for a large number of previously unknown users , without requiring pre - existing trust relationships between users and the system . our prototype requires no modifications to either servers or browsers , and makes use of graphical turing tests , web proxies , and client authentication using the ssl / tls protocol , all readily supported by modern browsers . we use the websos prototype to conduct a performance evaluation over the internet using planetlab , a testbed for experimentation with network overlays . we determine the end - to - end latency using both a chord - based approach and our shortcut extension . our evaluation shows the latency increase by a factor of 7 and 2 respectively , confirming our simulation results .
RANK 2078
an effective address mutation approach for disrupting reconnaissance attacks network reconnaissance of addresses and ports is prerequisite to a vast majority of cyber attacks . meanwhile , the static address configuration of networks and hosts simplifies adversarial reconnaissance for target discovery . although the randomization of host addresses has been suggested as a proactive disruption mechanism against such reconnaissance , the proposed approaches do not exploit the full potentials of address randomization in provision of unpredictability and attack adaptability . moreover , these approaches do not provide thorough analysis on effectiveness and limitations of address randomization against relevant threat models , including stealthy scanning and worms . in this paper , we present an effective address randomization technique , called random host address mutation ( rhm ) , that turns end - hosts into untraceable moving targets . this technique achieves maximum efficacy by allowing address randomization to be highly unpredictable and fast , and adaptive to adversarial behavior , while incurring low operational and reconfiguration overhead . our approach achieves the following objectives : ( 1 ) it achieves high uncertainty in adversary scanning by modeling address mutation randomization as a multi - level satisfiability problem ; ( 2 ) it adapts the mutation scheme by fast characterization of adversarial reconnaissance patterns ; ( 3 ) it achieves high mutation rate by separating mutation from end - hosts and managing it via network appliances ; and ( 4 ) it preserves network integrity , manageability and performance by bounding the size of routing tables , preserving end - to - end reachability , and efficient handling of reconfiguration updates . our extensive analyses and simulation show that the rhm distorts adversarial reconnaissance , slows down ( deters ) the attack , and increases its detectability . consequently , the rhm is effective in countering a significant number of sophisticated threat models , including reconnaissance , stealthy / evasive scanning methods , and targeted attacks . we also address limitations of our approach in terms of effectiveness and applicability .
TOP UNCITED PAPERS
RANK 1
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK 2
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK 3
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
TOP 20
RANK = 1; score = 0.963615357875824; correct = False; id = 2f2827630a60f1ee4d9eaff67b12013172d6fa7b
ustat : a real - time intrusion detection system for unix ustat a real - time intrusion detection system for unix
RANK = 2; score = 0.9619515538215637; correct = False; id = 71522a9de6f1c62a00f23bd4a50fbe888cbc1156
from facepalm to brain bender : exploring client - side cross - site scripting although studies have shown that at least one in ten web pages contains a client - side xss vulnerability , the prevalent causes for this class of cross - site scripting have not been studied in depth . therefore , in this paper , we present a large - scale study to gain insight into these causes . to this end , we analyze a set of 1,273 real - world vulnerabilities contained on the alexa top 10k domains using a specifically designed architecture , consisting of an infrastructure which allows us to persist and replay vulnerabilities to ensure a sound analysis . in combination with a taint - aware browsing engine , we can therefore collect important execution trace information for all flaws . based on the observable characteristics of the vulnerable javascript , we derive a set of metrics to measure the complexity of each flaw . we subsequently classify all vulnerabilities in our data set accordingly to enable a more systematic analysis . in doing so , we find that although a large portion of all vulnerabilities have a low complexity rating , several incur a significant level of complexity and are repeatedly caused by vulnerable third - party scripts . in addition , we gain insights into other factors related to the existence of client - side xss flaws , such as missing knowledge of browser - provided apis , and find that the root causes for client - side cross - site scripting range from unaware developers to incompatible first- and third - party code .
RANK = 3; score = 0.9619117379188538; correct = False; id = 2f95e2ca11610cb334d8d777d7b0f0d5561e67bc
you are what you include : large - scale evaluation of remote javascript inclusions javascript is used by web developers to enhance the interactivity of their sites , offload work to the users ' browsers and improve their sites ' responsiveness and user - friendliness , making web pages feel and behave like traditional desktop applications . an important feature of javascript , is the ability to combine multiple libraries from local and remote sources into the same page , under the same namespace . while this enables the creation of more advanced web applications , it also allows for a malicious javascript provider to steal data from other scripts and from the page itself . today , when developers include remote javascript libraries , they trust that the remote providers will not abuse the power bestowed upon them . in this paper , we report on a large - scale crawl of more than three million pages of the top 10,000 alexa sites , and identify the trust relationships of these sites with their library providers . we show the evolution of javascript inclusions over time and develop a set of metrics in order to assess the maintenance - quality of each javascript provider , showing that in some cases , top internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious javascript . in this process , we identify four , previously unknown , types of vulnerabilities that attackers could use to attack popular web sites . lastly , we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought .
RANK = 4; score = 0.9618964791297913; correct = False; id = 660ad810c69affa189f567e76ff83af682228703
q : exploit hardening made easy prior work has shown that return oriented programming ( rop ) can be used to bypass w⊕x , a software defense that stops shellcode , by reusing instructions from large libraries such as libc . modern operating systems have since enabled address randomization ( aslr ) , which randomizes the location of libc , making these techniques unusable in practice . however , modern aslr implementations leave smaller amounts of executable code unrandomized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case . in this paper , we show defenses as currently deployed can be bypassed with new techniques for automatically creating rop payloads from small amounts of unrandomized code . we propose using semantic program verification techniques for identifying the functionality of gadgets , and design a rop compiler that is resistant to missing gadget types . to demonstrate our techniques , we build q , an end - to - end system that automatically generates rop payloads for a given binary . q can produce payloads for 80 % of linux /usr / bin programs larger than 20 kb . we also show that q can automatically perform exploit hardening : given an exploit that crashes with defenses on , q outputs an exploit that bypasses both w⊕x and aslr . we show that q can harden nine realworld linux and windows exploits , enabling an attacker to automatically bypass defenses as deployed by industry for those programs .
RANK = 5; score = 0.9618182182312012; correct = False; id = a03986f4f3a8739d71b1d3269c1a2259fbaef89b
re : captchas : understanding captcha - solving services in an economic context reverse turing tests , or captchas , have become an ubiquitous defense used to protect open web resources from being exploited at scale . an effective captcha resists existing mechanistic software solving , yet can be solved with high probability by a human being . in response , a robust solving ecosystem has emerged , reselling both automated solving technology and realtime human labor to bypass these protections . thus , captchas can increasingly be understood and evaluated in purely economic terms ; the market price of a solution vs the monetizable value of the asset being protected . we examine the market - side of this question in depth , analyzing the behavior and dynamics of captcha - solving service providers , their price performance , and the underlying labor markets driving this economy .
RANK = 6; score = 0.9616736173629761; correct = False; id = 3875d1d1b623af0d640528efc9e581bc91338e35
librando : transparent code randomization for just - in - time compilers just - in - time compilers ( jits ) are here to stay . unfortunately , they also provide new capabilities to cyber attackers , namely the ability to supply input programs ( in languages such as javascript ) that will then be compiled to executable code . once this code is placed and marked as executable , it can then be leveraged by the attacker . randomization techniques such as constant blinding raise the cost to the attacker , but they significantly add to the burden of implementing a jit . there are a great many jits in use today , but not even all of the most commonly used ones randomize their outputs . we present librando , the first comprehensive technique to harden jit compilers in a completely generic manner by randomizing their output transparently ex post facto . we implement this approach as a system - wide service that can simultaneously harden multiple running jits . it hooks into the memory protections of the target os and randomizes newly generated code on the fly when marked as executable . in order to provide " black box " jit hardening , librando needs to be extremely conservative . for example , it completely preserves the contents of the calling stack , presenting each jit with the illusion that it is executing its own generated code . yet in spite of the heavy lifting that librando performs behind the scenes , the performance impact is surprisingly low . for java ( hotspot ) , we measured slowdowns by a factor of 1.15x , and for compute - intensive javascript ( v8 ) benchmarks , a slowdown of 3.5x . for many applications , this overhead is low enough to be practical for general use today .
RANK = 7; score = 0.9616603255271912; correct = False; id = 0c5de0e5cb46e862b933c6bd543cc15695506034
automatic patch - based exploit generation is possible : techniques and implications the automatic patch - based exploit generation problem is : given a program p and a patched version of the program p ' , automatically generate an exploit for the potentially unknown vulnerability present in p but fixed in p ' . in this paper , we propose techniques for automatic patch - based exploit generation , and show that our techniques can automatically generate exploits for 5 microsoft programs based upon patches provided via windows update . although our techniques may not work in all cases , a fundamental tenant of security is to conservatively estimate the capabilities of attackers . thus , our results indicate that automatic patch - based exploit generation should be considered practical . one important security implication of our results is that current patch distribution schemes which stagger patch distribution over long time periods , such as windows update , may allow attackers who receive the patch first to compromise the significant fraction of vulnerable hosts who have not yet received the patch .
RANK = 8; score = 0.9611892104148865; correct = False; id = 5606ed9fc593670af7e5d5c0e0e38ca16d997319
sending out an sms : characterizing the security of the sms ecosystem with public gateways text messages sent via the short message service ( sms ) have revolutionized interpersonal communication . recent years have also seen this service become a critical component of the security infrastructure , assisting with tasks including identity verification and second - factor authentication . at the same time , this messaging infrastructure has become dramatically more open and connected to public networks than ever before . however , the implications of this openness , the security practices of benign services , and the malicious misuse of this ecosystem are not well understood . in this paper , we provide the first longitudinal study to answer these questions , analyzing nearly 400,000 text messages sent to public online sms gateways over the course of 14 months . from this data , we are able to identify not only a range of services sending extremely sensitive plaintext data and implementing low entropy solutions for one - use codes , but also offer insights into the prevalence of sms spam and behaviors indicating that public gateways are primarily used for evading account creation policies that require verified phone numbers . this latter finding has significant implications for research combatting phone - verified account fraud and demonstrates that such evasion will continue to be difficult to detect and prevent .
RANK = 9; score = 0.9611459374427795; correct = False; id = 67f961f98d34fea3ab15f473429a5156b62b5c65
vigilare : toward snoop - based kernel integrity monitor in this paper , we present vigilare system , a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware . this snoop - based monitoring enabled by the vigilare system , overcomes the limitations of the snapshot - based monitoring employed in previous kernel integrity monitoring solutions . being based on inspecting snapshots collected over a certain interval , the previous hardware - based monitoring solutions can not detect transient attacks that can occur in between snapshots . we implemented a prototype of the vigilare system on gaisler 's grlib - based system - on - a - chip ( soc ) by adding snooper hardware connections module to the host system for bus snooping . to evaluate the benefit of snoop - based monitoring , we also implemented similar soc with a snapshot - based monitor to be compared with . the vigilare system detected all the transient attacks without performance degradation while the snapshot - based monitor could not detect all the attacks and induced considerable performance degradation as much as 10 % in our tuned stream benchmark test .
RANK = 10; score = 0.9610137343406677; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 11; score = 0.9608539938926697; correct = False; id = 4781bb515418ec2823304dee7180c78c1a11dfeb
highly predictive blacklisting the notion of blacklisting communication sources has been a well - established defensive measure since the origins of the internet community . in particular , the practice of compiling and sharing lists of the worst offenders of unwanted traffic is a blacklisting strategy that has remained virtually unquestioned over many years . but do the individuals who incorporate such blacklists into their perimeter defenses benefit from the blacklisting contents as much as they could from other list - generation strategies ? in this paper , we will argue that there exist better alternative blacklist generation strategies that can produce higher - quality results for an individual network . in particular , we introduce a blacklisting system based on a relevance ranking scheme borrowed from the linkanalysis community . the system produces customized blacklists for individuals who choose to contribute data to a centralized log - sharing infrastructure . the ranking scheme measures how closely related an attack source is to a contributor , using that attacker ’s history and the contributor ’s recent log production patterns . the blacklisting system also integrates substantive log prefiltering and a severity metric that captures the degree to which an attacker ’s alert patterns match those of common malwarepropagation behavior . our intent is to yield individualized blacklists that not only produce significantly higher hit rates , but that also incorporate source addresses that pose the greatest potential threat . we tested our scheme on a corpus of over 700 million log entries produced from the dshield data center and the result shows that our blacklists not only enhance hit counts but also can proactively incorporate attacker addresses in a timely fashion . an early form of our system have been fielded to dshield contributors over the last year .
RANK = 12; score = 0.9600775241851807; correct = False; id = 2a74da762f75eeb53f41d7c93ac1f08c62b15af9
native client : a sandbox for portable , untrusted x86 native code 
RANK = 13; score = 0.9599416851997375; correct = False; id = 3188dc28042effbd519005ec18c07e7afa51c975
the clock is still ticking : timing attacks in the modern web web - based timing attacks have been known for over a decade , and it has been shown that , under optimal network conditions , an adversary can use such an attack to obtain information on the state of a user in a cross - origin website . in recent years , desktop computers have given way to laptops and mobile devices , which are mostly connected over a wireless or mobile network . these connections often do not meet the optimal conditions that are required to reliably perform cross - site timing attacks . in this paper , we show that modern browsers expose new side - channels that can be used to acquire accurate timing measurements , regardless of network conditions . using several real - world examples , we introduce four novel web - based timing attacks against modern browsers and describe how an attacker can use them to obtain personal information based on a user 's state on a cross - origin website . we evaluate our proposed attacks and demonstrate that they significantly outperform current attacks in terms of speed , reliability , and accuracy . furthermore , we show that the nature of our attacks renders traditional defenses , i.e. , those based on randomly delaying responses , moot and discuss possible server - side defense mechanisms .
RANK = 14; score = 0.9594228267669678; correct = False; id = 6f45152ce34b4326fc0adfb7d7b6587b13d0a62c
reassembleable disassembling reverse engineering has many important applications in computer security , one of which is retrofitting software for safety and security hardening when source code is not available . by surveying available commercial and academic reverse engineering tools , we surprisingly found that no existing tool is able to disassemble executable binaries into assembly code that can be correctly assembled back in a fully automated manner , even for simple programs . actually in many cases , the resulted disassembled code is far from a state that an assembler accepts , which is hard to fix even by manual effort . this has become a severe obstacle . people have tried to overcome it by patching or duplicating new code sections for retrofitting of executables , which is not only inefficient but also cumbersome and restrictive on what retrofitting techniques can be applied to . in this paper , we present uroboros , a tool that can disassemble executables to the extent that the generated code can be assembled back to working binaries without manual effort . by empirically studying 244 binaries , we summarize a set of rules that can make the disassembled code relocatable , which is the key to reassembleable disassembling . with uroboros , the disassembly - reassembly process can be repeated thousands of times . we have implemented a prototype of uroboros and tested over the whole set of gnu coreutils , spec2006 , and a set of other real - world application and server programs . the experiment results show that our tool is effective with a very modest cost .
RANK = 15; score = 0.9593669772148132; correct = False; id = 0dfe28b14b7c35fc1b876954a09ff6b7e47a8ee9
openconflict : preventing real time map hacks in online games we present a generic tool , kartograph , that lifts the fog of war in online real - time strategy games by snooping on the memory used by the game . kartograph is passive and can not be detected remotely . motivated by these passive attacks , we present secure protocols for distributing game state among players so that each client only has data it is allowed to see . our system , open conflict , runs real - time games with distributed state . to support our claim that open conflict is sufficiently fast for real - time strategy games , we show the results of an extensive study of 1000 replays of star craft ii games between expert players . at the peak of a typical game , open conflict needs only 22 milliseconds on one cpu core each time state is synchronized .
RANK = 16; score = 0.9593297839164734; correct = False; id = 4681a0116597fd0804b07e8176b8761e4f569743
password cracking using probabilistic context - free grammars choosing the most effective word - mangling rules to use when performing a dictionary - based password cracking attack can be a difficult task . in this paper we discuss a new method that generates password structures in highest probability order . we first automatically create a probabilistic context - free grammar based upon a training set of previously disclosed passwords . this grammar then allows us to generate word - mangling rules , and from them , password guesses to be used in password cracking . we will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets . in one series of experiments , training on a set of disclosed passwords , our approach was able to crack 28 % to 129 % more passwords than john the ripper , a publicly available standard password cracking program .
RANK = 17; score = 0.95927494764328; correct = False; id = 2f7b92282d42d645ee2d4bc34aa2bf132275e82b
libfte : a toolkit for constructing practical , format - abiding encryption schemes encryption schemes where the ciphertext must abide by a specified format have diverse applications , ranging from in - place encryption in databases to per - message encryption of network traffic for censorship circumvention . despite this , a unifying framework for deploying such encryption schemes has not been developed . one consequence of this is that current schemes are ad - hoc ; another is a requirement for expert knowledge that can disuade one from using encryption at all . we present a general - purpose library ( called libfte ) that aids engineers in the development and deployment of format - preserving encryption ( fpe ) and formattransforming encryption ( fte ) schemes . it incorporates a new algorithmic approach for performing fpe / fte using the nondeterministic finite - state automata ( nfa ) representation of a regular expression when specifying formats . this approach was previously considered unworkable , and our approach closes this open problem . we evaluate libfte and show that , compared to other encryption solutions , it introduces negligible latency overhead , and can decrease diskspace usage by as much as 62.5 % when used for simultaneous encryption and compression in a postgresql database ( both relative to conventional encryption mechanisms ) . in the censorship circumvention setting we show that , using regularexpression formats lifted from the snort ids , libfte can reduce client / server memory requirements by as much as 30 % .
RANK = 18; score = 0.9591645002365112; correct = False; id = 937cb0795bf3bfb5c9cbb7387ed68301bb6995ce
mascot : faster malicious arithmetic secure computation with oblivious transfer we consider the task of secure multi - party computation of arithmetic circuits over a finite field . unlike boolean circuits , arithmetic circuits allow natural computations on integers to be expressed easily and efficiently . in the strongest setting of malicious security with a dishonest majority --- where any number of parties may deviate arbitrarily from the protocol --- most existing protocols require expensive public - key cryptography for each multiplication in the preprocessing stage of the protocol , which leads to a high total cost . we present a new protocol that overcomes this limitation by using oblivious transfer to perform secure multiplications in general finite fields with reduced communication and computation . our protocol is based on an arithmetic view of oblivious transfer , with careful consistency checks and other techniques to obtain malicious security at a cost of less than 6 times that of semi - honest security . we describe a highly optimized implementation together with experimental results for up to five parties . by making extensive use of parallelism and sse instructions , we improve upon previous runtimes for mpc over arithmetic circuits by more than 200 times .
RANK = 19; score = 0.9591156840324402; correct = False; id = 64205427d0f900997ec0a22fdd4946a3ba16f1b9
hey , you have a problem : on the feasibility of large - scale web vulnerability notification large - scale discovery of thousands of vulnerable web sites has become a frequent event , thanks to recent advances in security research and the rise in maturity of internet - wide scanning tools . the issues related to disclosing the vulnerability information to the affected parties , however , have only been treated as a side note in prior research . in this paper , we systematically examine the feasibility and efficacy of large - scale notification campaigns . for this , we comprehensively survey existing communication channels and evaluate their usability in an automated notification process . using a data set of over 44,000 vulnerable web sites , we measure success rates , both with respect to the total number of fixed vulnerabilities and to reaching responsible parties , with the following highlevel results : although our campaign had a statistically significant impact compared to a control group , the increase in the fix rate of notified domains is marginal . if a notification report is read by the owner of the vulnerable application , the likelihood of a subsequent resolution of the issues is sufficiently high : about 40 % . but , out of 35,832 transmitted vulnerability reports , only 2,064 ( 5.8 % ) were actually received successfully , resulting in an unsatisfactory overall fix rate , leaving 74.5 % of web applications exploitable after our month - long experiment . thus , we conclude that currently no reliable notification channels exist , which significantly inhibits the success and impact of large - scale notification .
RANK = 20; score = 0.9590598940849304; correct = False; id = 0fd2467de521b52805eea902edc9587c87818276
discoverer : automatic protocol reverse engineering from network traces application - level protocol specifications are useful for many security applications , including intrusion prevention and detection that performs deep packet inspection and traffic normalization , and penetration testing that generates network inputs to an application to uncover potential vulnerabilities . however , current practice in deriving protocol specifications is mostly manual . in this paper , we present discoverer , a tool for automatically reverse engineering the protocol message formats of an application from its network trace . a key property of discoverer is that it operates in a protocol - independent fashion by inferring protocol idioms commonly seen in message formats of many application - level protocols . we evaluated the efficacy of discoverer over one text protocol ( http ) and two binary protocols ( rpc and cifs / smb ) by comparing our inferred formats with true formats obtained from ethereal [ 5 ] . for all three protocols , more than 90 % of our inferred formats correspond to exactly one true format ; one true format is reflected in five inferred formats on average ; our inferred formats cover over 95 % of messages , which belong to 30 - 40 % of true formats observed in the trace .

