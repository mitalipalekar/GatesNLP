RANKING 1099
QUERY
debunking sentiment lexicons : a case of domain - specific sentiment classification for croatian sentiment lexicons are widely used as an intuitive and inexpensive way of tackling sentiment classification , often within a simple lexicon word - counting approach or as part of a supervised model . however , it is an open question whether these approaches can compete with supervised models that use only word - representation features . we address this question in the context of domain - specific sentiment classification for croatian . we experiment with the graph - based acquisition of sentiment lexicons , analyze their quality , and investigate how effectively they can be used in sentiment classification . our results indicate that , even with as few as 500 labeled instances , a supervised model substantially outperforms a word - counting model . we also observe that adding lexicon - based features does not significantly improve supervised sentiment classification .
First cited at 544
TOP CITED PAPERS
RANK 544
semi - supervised polarity lexicon induction we present an extensive study on the problem of detecting polarity of words . we consider the polarity of a word to be either positive or negative . for example , words such asgood , beautiful , and wonderful are considered as positive words ; whereas words such as bad , ugly , and sad are considered negative words . we treat polarity detection as a semi - supervised label propagation problem in a graph . in the graph , each node represents a word whose polarity is to be determined . each weighted edge encodes a relation that exists between two words . each node ( word ) can have two labels : positive or negative . we study this framework in two different resource availability scenarios using wordnet and openoffice thesaurus when wordnet is not available . we report our results on three different languages : english , french , and hindi . our results indicate that label propagation improves significantly over the baseline and other semisupervised learning methods like mincuts and randomized mincuts for this task .
RANK 902
sentiment polarity identification in financial news : a cohesion - based approach text is not unadulterated fact . a text can make you laugh or cry but can it also make you short sell your stocks in company a and buy up options in company b ? research in the domain of finance strongly suggests that it can . studies have shown that both the informational and affective aspects of news text affect the markets in profound ways , impacting on volumes of trades , stock prices , volatility and even future firm earnings . this paper aims to explore a computable metric of positive or negative polarity in financial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on financial markets . results from a preliminary evaluation are presented and discussed .
RANK 1084
pageranking wordnet synsets : an application to opinion mining this paper presents an application of pagerank , a random - walk model originally devised for ranking web search results , to ranking wordnet synsets in terms of how strongly they possess a given semantic property . the semantic properties we use for exemplifying the approach are positivity and negativity , two properties of central importance in sentiment analysis . the idea derives from the observation that wordnet may be seen as a graph in which synsets are connected through the binary relation “ a term belonging to synset sk occurs in the gloss of synset si ” , and on the hypothesis that this relation may be viewed as a transmitter of such semantic properties . the data for this relation can be obtained from extended wordnet , a publicly available sensedisambiguated version of wordnet . we argue that this relation is structurally akin to the relation between hyperlinked web pages , and thus lends itself to pagerank analysis . we report experimental results supporting our intuitions .
TOP UNCITED PAPERS
RANK 1
refined lexikon models for statistical machine translation using a maximum entropy approach typically , the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information , which often leads to problems in performing a correct word - sense disambiguation . one way to deal with this problem within the statistical framework is using maximum entropy methods . in this paper , we present how to use this information within a statistical machine translation system . we show that it is possible to significantly decrease training and test corpus perplexity of the translation models . in addition , we perform a rescoring of -best lists using our maximum entropy model and thereby yield an improvement in translation quality . experimental results are presented with the so called “ vermobil task ” .
RANK 2
multi - domain adaptation for smt using multi - task learning domain adaptation for smt usually adapts models to an individual specific domain . however , it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality . in this paper , we propose a novel multi - domain adaptation approach for smt using multi - task learning ( mtl ) , with in - domain models tailored for each specific domain and a general - domain model shared by different domains . the parameters of these models are tuned jointly via mtl so that they can learn general knowledge more accurately and exploit domain knowledge better . our experiments on a largescale english - to - chinese translation task validate that the mtl - based adaptation approach significantly and consistently improves the translation quality compared to a non - adapted baseline . furthermore , it also outperforms the individual adaptation of each specific domain .
RANK 3
udlap : sentiment analysis using a graph - based representation we present an approach for tackling the sentiment analysis problem in semeval 2015 . the approach is based on the use of a cooccurrence graph to represent existing relationships among terms in a document with the aim of using centrality measures to extract the most representative words that express the sentiment . these words are then used in a supervised learning algorithm as features to obtain the polarity of unknown documents . the best results obtained for the different datasets are : 77.76 % for positive , 100 % for negative and 68.04 % for neutral , showing that the proposed graph - based representation could be a way of extracting terms that are relevant to detect a sentiment .
TOP 20
RANK = 1; score = 0.2361111111111111; correct = False; id = 3f3c22e13cf85e7a2d44af20262c28d01d177d48
refined lexikon models for statistical machine translation using a maximum entropy approach typically , the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information , which often leads to problems in performing a correct word - sense disambiguation . one way to deal with this problem within the statistical framework is using maximum entropy methods . in this paper , we present how to use this information within a statistical machine translation system . we show that it is possible to significantly decrease training and test corpus perplexity of the translation models . in addition , we perform a rescoring of -best lists using our maximum entropy model and thereby yield an improvement in translation quality . experimental results are presented with the so called “ vermobil task ” .
RANK = 2; score = 0.2361111111111111; correct = False; id = 5116892028001b660a385b65823a00c22cbf19ee
multi - domain adaptation for smt using multi - task learning domain adaptation for smt usually adapts models to an individual specific domain . however , it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality . in this paper , we propose a novel multi - domain adaptation approach for smt using multi - task learning ( mtl ) , with in - domain models tailored for each specific domain and a general - domain model shared by different domains . the parameters of these models are tuned jointly via mtl so that they can learn general knowledge more accurately and exploit domain knowledge better . our experiments on a largescale english - to - chinese translation task validate that the mtl - based adaptation approach significantly and consistently improves the translation quality compared to a non - adapted baseline . furthermore , it also outperforms the individual adaptation of each specific domain .
RANK = 3; score = 0.22794117647058823; correct = False; id = 6bbcdcbd87000a7bef81b65f97dec12a4428c85a
udlap : sentiment analysis using a graph - based representation we present an approach for tackling the sentiment analysis problem in semeval 2015 . the approach is based on the use of a cooccurrence graph to represent existing relationships among terms in a document with the aim of using centrality measures to extract the most representative words that express the sentiment . these words are then used in a supervised learning algorithm as features to obtain the polarity of unknown documents . the best results obtained for the different datasets are : 77.76 % for positive , 100 % for negative and 68.04 % for neutral , showing that the proposed graph - based representation could be a way of extracting terms that are relevant to detect a sentiment .
RANK = 4; score = 0.2248062015503876; correct = False; id = 912a14e09b1f2715f2de4bddd652bdb8851a3ea8
discriminative approach to predicate - argument structure analysis with zero - anaphora resolution this paper presents a predicate - argument structure analysis that simultaneously conducts zero - anaphora resolution . by adding noun phrases as candidate arguments that are not only in the sentence of the target predicate but also outside of the sentence , our analyzer identifies arguments regardless of whether they appear in the sentence or not . because we adopt discriminative models based on maximum entropy for argument identification , we can easily add new features . we add language model scores as well as contextual features . we also use contextual information to restrict candidate arguments .
RANK = 5; score = 0.2222222222222222; correct = False; id = c25583d3623be2babfd7c3bfbb056573141c7071
statistical qa - classifier vs. re - ranker : what 's the difference ? in this paper , we show that we can obtain a good baseline performance for question answering ( qa ) by using only 4 simple features . using these features , we contrast two approaches used for a maximum entropy based qa system . we view the qa problem as a classification problem and as a reranking problem . our results indicate that the qa system viewed as a reranker clearly outperforms the qa system used as a classifier . both systems are trained using the same data .
RANK = 6; score = 0.2222222222222222; correct = False; id = 8a57d0864d3be2ed5397140cc68a146cfd1e01d6
# emotional tweets detecting emotions in microblogs and social media posts has applications for industry , health , and security . however , there exists no microblog corpus with instances labeled for emotions for developing supervised systems . in this paper , we describe how we created such a corpus from twitter posts using emotionword hashtags . we conduct experiments to show that the self - labeled hashtag annotations are consistent and match with the annotations of trained judges . we also show how the twitter emotion corpus can be used to improve emotion classification accuracy in a different domain . finally , we extract a word – emotion association lexicon from this twitter corpus , and show that it leads to significantly better results than the manually crafted wordnet affect lexicon in an emotion classification task.1
RANK = 7; score = 0.2206896551724138; correct = False; id = 616c6c88f5cd4254a7cf9efcb3560a5aec3a1a3e
sentiment flow - a general model of web review argumentation web reviews have been intensively studied in argumentation - related tasks such as sentiment analysis . however , due to their focus on content - based features , many sentiment analysis approaches are effective only for reviews from those domains they have been specifically modeled for . this paper puts its focus on domain independence and asks whether a general model can be found for how people argue in web reviews . our hypothesis is that people express their global sentiment on a topic with similar sequences of local sentiment independent of the domain . we model such sentiment flow robustly under uncertainty through abstraction . to test our hypothesis , we predict global sentiment based on sentiment flow . in systematic experiments , we improve over the domain independence of strong baselines . our findings suggest that sentiment flow qualifies as a general model of web review argumentation .
RANK = 8; score = 0.21830985915492956; correct = False; id = 86031fe45c9908e1a951456c33ff4462e22a5039
portable features for classifying emotional text are word - level affect lexicons useful in detecting emotions at sentence level ? some prior research finds no gain over and above what is obtained with ngram features — arguably the most widely used features in text classification . here , we experiment with two very different emotion lexicons and show that even in supervised settings , an affect lexicon can provide significant gains . we further show that while ngram features tend to be accurate , they are often unsuitable for use in new domains . on the other hand , affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain .
RANK = 9; score = 0.21794871794871795; correct = False; id = 90e8f01b0a1b3183ac8676bbe7d72e749e651f96
learning word representations from scarce and noisy data with embedding subspaces we investigate a technique to adapt unsupervised word embeddings to specific applications , when only small and noisy labeled datasets are available . current methods use pre - trained embeddings to initialize model parameters , and then use the labeled data to tailor them for the intended task . however , this approach is prone to overfitting when the training is performed with scarce and noisy data . to overcome this issue , we use the supervised data to find an embedding subspace that fits the task complexity . all the word representations are adapted through a projection into this task - specific subspace , even if they do not occur on the labeled dataset . this approach was recently used in the semeval 2015 twitter sentiment analysis challenge , attaining state - of - the - art results . here we show results improving those of the challenge , as well as additional experiments in a twitter part - of - speech tagging task .
RANK = 10; score = 0.21656050955414013; correct = False; id = 060d5bb21f318efd785e86aa50cb97aed090a0fb
a discriminative language model with pseudo - negative samples in this paper , we propose a novel discriminative language model , which can be applied quite generally . compared to the well known n - gram language models , discriminative language models can achieve more accurate discrimination because they can employ overlapping features and nonlocal information . however , discriminative language models have been used only for re - ranking in specific applications because negative examples are not available . we propose sampling pseudo - negative examples taken from probabilistic language models . however , this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples . we tackle the problem by estimating the latent information in sentences using a semimarkov class model , and then extracting features from them . we also use an online margin - based algorithm with efficient kernel computation . experimental results show that pseudo - negative examples can be treated as real negative examples and our model can classify these sentences correctly .
RANK = 11; score = 0.216; correct = False; id = 8b1430bae27cbdb1e18b7513141233c2f9ae5849
a noisy - channel approach to question answering we introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end - to - end qa system . our noisy - channel system outperforms a stateof - the - art rule - based qa system that uses similar resources . we also show that the model we propose is flexible enough to accommodate within one mathematical framework many qa - specific resources and techniques , which range from the exploitation of wordnet , structured , and semi - structured databases to reasoning , and paraphrasing .
RANK = 12; score = 0.21487603305785125; correct = False; id = 75df8d00e9633c8d28c1a6b6463950bb1dcdee14
likey : unsupervised language - independent keyphrase extraction likey is an unsupervised statistical approach for keyphrase extraction . the method is language - independent and the only language - dependent component is the reference corpus with which the documents to be analyzed are compared . in this study , we have also used another language - dependent component : an english - specific porter stemmer as a preprocessing step . in our experiments of keyphrase extraction from scientific articles , thelikey method outperforms both supervised and unsupervised baseline methods .
RANK = 13; score = 0.21333333333333335; correct = False; id = 464e8d9fe97620289e2c11654f9c1b4ed555c02d
word sense disambiguation using label propagation based semi - supervised learning shortage of manually sense - tagged data is an obstacle to supervised word sense disambiguation ( wsd ) methods . in this paper we investigate a label propagation based semi - supervised learning algorithm for wsd , which combines unlabeled data with labeled data in learning process by representing labeled and unlabeled examples as vertices in a weighted graph and iteratively propagating the label information from any vertex to nearby vertices until this process converges . this label propagation process realizes a global consistency assumption : similar examples should have similar labels . our experimental results on benchmark corpora indicate that it consistently outperforms svm when only very few labeled examples are available , and its performance is also better than monolingual bootstrapping , and comparable to bilingual bootstrapping .
RANK = 14; score = 0.2127659574468085; correct = False; id = 82be7dfbd3e5d742217296e33045786db570505d
relation extraction using label propagation based semi - supervised learning shortage of manually labeled data is an obstacle to supervised relation extraction methods . in this paper we investigate a graph based semi - supervised learning algorithm , a label propagation ( lp ) algorithm , for relation extraction . it represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph . experiment results on the ace corpus showed that this lp algorithm achieves better performance than svm when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .
RANK = 15; score = 0.2127659574468085; correct = False; id = 830746182677f9c1dc27c87222e01a0bb0ca4dab
dive deeper : deep semantics for sentiment analysis this paper illustrates the use of deep semantic processing for sentiment analysis . existing methods for sentiment analysis use supervised approaches which take into account all the subjective words and or phrases . due to this , the fact that not all of these words and phrases actually contribute to the overall sentiment of the text is ignored . we propose an unsupervised rule - based approach using deep semantic processing to identify only relevant subjective terms . we generate a unl ( universal networking language ) graph for the input text . rules are applied on the graph to extract relevant terms . the sentiment expressed in these terms is used to figure out the overall sentiment of the text . results on binary sentiment classification have shown promising results .
RANK = 16; score = 0.2125984251968504; correct = False; id = 40e05b4b2477b7b88343d3a0ffd911fe3ba94cc7
alignment model adaptation for domain - specific word alignment this paper proposes an alignment adaptation approach to improve domain - specific ( in - domain ) word alignment . the basic idea of alignment adaptation is to use out - of - domain corpus to improve in - domain word alignment results . in this paper , we first train two statistical word alignment models with the large - scale out - of - domain corpus and the small - scale in - domain corpus respectively , and then interpolate these two models to improve the domain - specific word alignment . experimental results show that our approach improves domain - specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state - of - the - art technologies .
RANK = 17; score = 0.2125; correct = False; id = 1011dc13c78e7e805ae93470ccf9053ea247772b
improving citation polarity classification with product reviews recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering . while this result confirms that citation classification is feasible , there are two drawbacks to this approach : ( i ) it requires a large annotated corpus for supervised classification , which in the case of scientific literature is quite expensive ; and ( ii ) feature engineering that is too specific to one area of scientific literature may not be portable to other domains , even within scientific literature . in this paper we address these two drawbacks . first , we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains . then , to avoid over - engineering specific citation features for a particular scientific domain , we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features . we achieve better citation classification results with this cross - domain approach than using in - domain classification .
RANK = 18; score = 0.2125; correct = False; id = a2e25c6ef2aa65189e3bdb110837be0ac8ea4af1
poster : mining elephant applications in unknown traffic by service clustering network traffic classification is of great importance for fine - grained network management and network security . however , with the rapid development of new network applications in recent years , traffic that can not be identified by classifiers accounts for an increasing ratio , which brings a great challenge for network operators . most of the unknown traffic is usually generated by only a few or some certain kinds of applications . we call this kind of traffic as the elephant traffic . it is generally recognized that traffic sharing the same server ip and server port is generated by the same application . in this paper , we say that they are belonging to the same service . therefore , we propose a novel method , in which service - based statistical features are used for cluster analysis , to classify these elephant traffic . preliminary results on a real network traffic dataset show that our method is able to automatically identify similar unknown applications . we believe that classifying unknown traffic in service perspective is a promising direction .
RANK = 19; score = 0.21212121212121213; correct = False; id = 25e45373a62782db436180d5dc14faacbc9ff258
klognlp : graph kernel - based relational learning of natural language klog is a framework for kernel - based learning that has already proven successful in solving a number of relational tasks in natural language processing . in this paper , we present klognlp , a natural language processing module for klog . this module enriches klog with nlp - specific preprocessors , enabling the use of existing libraries and toolkits within an elegant and powerful declarative machine learning framework . the resulting relational model of the domain can be extended by specifying additional relational features in a declarative way using a logic programming language . this declarative approach offers a flexible way of experimentation and a way to insert domain knowledge .
RANK = 20; score = 0.2116788321167883; correct = False; id = 76cdf028470717ea4a50215c59ee72ce96dfb115
an exploration of discourse - based sentence spaces for compositional distributional semantics this paper investigates whether the wider context in which a sentence is located can contribute to a distributional representation of sentence meaning . we compare a vector space for sentences in which the features are words occurring within the sentence , with two new vector spaces that only make use of surrounding context . experiments on simple subject - verbobject similarity tasks show that all sentence spaces produce results that are comparable with previous work . however , qualitative analysis and user experiments indicate that extra - sentential contexts capture more diverse , yet topically coherent

RANKING 2487
QUERY
a convolutional encoder model for neural machine translation the prevalent approach to neural machine translation relies on bi - directional lstms to encode the source sentence . in this paper we present a faster and simpler architecture based on a succession of convolutional layers . this allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies . on wmt’16 english - romanian translation we achieve competitive accuracy to the state - of - theart and we outperform several recently published results on the wmt’15 englishgerman task . our models obtain almost the same accuracy as a very deep lstm setup on wmt’14 english - french translation . our convolutional encoder speeds up cpu decoding by more than two times at the same or higher accuracy as a strong bi - directional lstm baseline .
First cited at 5
TOP CITED PAPERS
RANK 5
on using very large target vocabulary for neural machine translation neural machine translation , a recently proposed approach to machine translation based purely on neural networks , has shown promising results compared to the existing approaches such as phrasebased statistical machine translation . despite its recent success , neural machine translation has its limitation in handling a larger vocabulary , as training complexity as well as decoding complexity increase proportionally to the number of target words . in this paper , we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity . we show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary . the models trained by the proposed approach are empirically found to match , and in some cases outperform , the baseline models with a small vocabulary as well as the lstm - based neural machine translation models . furthermore , when we use an ensemble of a few models with very large target vocabularies , we achieve performance comparable to the state of the art ( measured by bleu ) on both the english→german and english→french translation tasks of wmt’14 .
RANK 20
encoding source language with convolutional neural network for machine translation the recently proposed neural network joint model ( nnjm ) ( devlin et al . , 2014 ) augments the n - gram target language model with a heuristically chosen source context window , achieving state - of - the - art performance in smt . in this paper , we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information . with different guiding signals during decoding , our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word , and fuse them with the context of entire source sentence to form a unified representation . this representation , together with target language words , are fed to a deep neural network ( dnn ) to form a stronger nnjm . experiments on two nist chinese - english translation tasks show that the proposed model can achieve significant improvements over the previous nnjm by up to + 1.01 bleu points on average .
RANK 81
convolutional neural network language models convolutional neural networks ( cnns ) have shown to yield very strong results in several computer vision tasks . their application to language has received much less attention , and it has mainly focused on static classification tasks , such as sentence classification for sentiment analysis or relation extraction . in this work , we study the application of cnns to language modeling , a dynamic , sequential prediction task that needs models to capture local as well as long - range dependency information . our contribution is twofold . first , we show that cnns achieve 11 - 26 % better absolute performance than feed - forward neural language models , demonstrating their potential for language representation even in sequential tasks . as for recurrent models , our model outperforms rnns but is below state of the art lstm models . second , we gain some understanding of the behavior of the model , showing that cnns in language act as feature detectors at a high level of abstraction , like in computer vision , and that the model can profitably use information from as far as 16 words before the target .
TOP UNCITED PAPERS
RANK 1
a recurrent neural networks approach for estimating the quality of machine translation output this paper presents a novel approach using recurrent neural networks for estimating the quality of machine translation output . a sequence of vectors made by the prediction method is used as the input of the final recurrent neural network . the prediction method uses bi - directional recurrent neural network architecture both on source and target sentence to fully utilize the bi - directional quality information from source and target sentence . our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof - the - art models for estimating the sentencelevel quality of english - to - spanish translation .
RANK 2
bidirectional recurrent convolutional neural network for relation classification relation classification is an important semantic processing task in the field of natural language processing ( nlp ) . in this paper , we present a novel model brcnn to classify the relation of two entities in a sentence . some state - of - the - art systems concentrate on modeling the shortest dependency path ( sdp ) between two entities leveraging convolutional or recurrent neural networks . we further explore how to make full use of the dependency relations information in the sdp , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( lstm ) units . we propose a bidirectional architecture to learn relation representations with directional information along the sdp forwards and backwards at the same time , which benefits classifying the direction of relations . experimental results show that our method outperforms the state - of - theart approaches on the semeval-2010 task 8 dataset .
RANK 3
translation modeling with bidirectional recurrent neural networks this work presents two different translation models using recurrent neural networks . the first one is a word - based approach using word alignments . second , we present phrase - based translation models that are more consistent with phrasebased decoding . moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest . we demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : iwslt 2013 german→english , bolt arabic→english and chinese→english . we obtain gains up to 1.6 % bleu and 1.7 % ter by rescoring 1000-best lists .
TOP 20
RANK = 1; score = 0.22764227642276422; correct = False; id = 79e52b16e94a82d60566028f676295540b161660
a recurrent neural networks approach for estimating the quality of machine translation output this paper presents a novel approach using recurrent neural networks for estimating the quality of machine translation output . a sequence of vectors made by the prediction method is used as the input of the final recurrent neural network . the prediction method uses bi - directional recurrent neural network architecture both on source and target sentence to fully utilize the bi - directional quality information from source and target sentence . our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof - the - art models for estimating the sentencelevel quality of english - to - spanish translation .
RANK = 2; score = 0.22727272727272727; correct = False; id = 08b63b1f9a770c1b6f8545c2e1e4a9bfb6a2de6d
bidirectional recurrent convolutional neural network for relation classification relation classification is an important semantic processing task in the field of natural language processing ( nlp ) . in this paper , we present a novel model brcnn to classify the relation of two entities in a sentence . some state - of - the - art systems concentrate on modeling the shortest dependency path ( sdp ) between two entities leveraging convolutional or recurrent neural networks . we further explore how to make full use of the dependency relations information in the sdp , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( lstm ) units . we propose a bidirectional architecture to learn relation representations with directional information along the sdp forwards and backwards at the same time , which benefits classifying the direction of relations . experimental results show that our method outperforms the state - of - theart approaches on the semeval-2010 task 8 dataset .
RANK = 3; score = 0.22535211267605634; correct = False; id = 16fe3d5fc4a671e5b853cb1601700c11a675946c
translation modeling with bidirectional recurrent neural networks this work presents two different translation models using recurrent neural networks . the first one is a word - based approach using word alignments . second , we present phrase - based translation models that are more consistent with phrasebased decoding . moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest . we demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : iwslt 2013 german→english , bolt arabic→english and chinese→english . we obtain gains up to 1.6 % bleu and 1.7 % ter by rescoring 1000-best lists .
RANK = 4; score = 0.21739130434782608; correct = False; id = d55824c7799f2d6ddf6abc73270007a6ecb9cb20
latent vector weighting for word meaning in context this paper presents a novel method for the computation of word meaning in context . we make use of a factorization model in which words , together with their window - based context words and their dependency relations , are linked to latent dimensions . the factorization model allows us to determine which dimensions are important for a particular context , and adapt the dependency - based feature vector of the word accordingly . the evaluation on a lexical substitution task – carried out for both english and french – indicates that our approach is able to reach better results than state - of - the - art methods in lexical substitution , while at the same time providing more accurate meaning representations .
RANK = 5; score = 0.21341463414634146; correct = True; id = 03b18dcde7ba5bb0e87b2bdb68ab7af951daf162
on using very large target vocabulary for neural machine translation neural machine translation , a recently proposed approach to machine translation based purely on neural networks , has shown promising results compared to the existing approaches such as phrasebased statistical machine translation . despite its recent success , neural machine translation has its limitation in handling a larger vocabulary , as training complexity as well as decoding complexity increase proportionally to the number of target words . in this paper , we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity . we show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary . the models trained by the proposed approach are empirically found to match , and in some cases outperform , the baseline models with a small vocabulary as well as the lstm - based neural machine translation models . furthermore , when we use an ensemble of a few models with very large target vocabularies , we achieve performance comparable to the state of the art ( measured by bleu ) on both the english→german and english→french translation tasks of wmt’14 .
RANK = 6; score = 0.2109375; correct = False; id = 574071a034156aea139e00ff845ef88f0249dc66
applying neural networks to english - chinese named entity transliteration this paper presents the machine transliteration systems that we employ for our participation in the news 2016 machine transliteration shared task . based on the prevalent deep learning models developed for general sequence processing tasks , we use convolutional neural networks to extract character level information from the transliteration units and stack a simple recurrent neural network on top for sequence processing . the systems are applied to the standard runs for both english to chinese and chinese to english transliteration tasks . our systems achieve competitive results according to the official evaluation .
RANK = 7; score = 0.21052631578947367; correct = False; id = 167ad306d84cca2455bc50eb833454de9f2dcd02
joint language and translation modeling with recurrent neural networks we present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words . the weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward - based language or translation models . we tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically . our joint model builds on a well known recurrent neural network language model ( mikolov , 2012 ) augmented by a layer of additional inputs from the source language . we show competitive accuracy compared to the traditional channel model features . our best results improve the output of a system trained on wmt 2012 french - english data by up to 1.5 bleu , and by 1.1 bleu on average across several test sets .
RANK = 8; score = 0.21014492753623187; correct = False; id = 83cf4b2f39bcc802b09fd59b69e23068447b26b7
multi - task learning for multiple language translation in this paper , we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages . our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem . we extend the neural machine translation to a multi - task learning framework which shares source language representation and separates the modeling of different target language translation . our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available . experiments show that our multi - task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available .
RANK = 9; score = 0.20869565217391303; correct = False; id = 567eff31138cad5accb8d51cf513fa4afea0c4b2
kernel regression based machine translation we present a novel machine translation framework based on kernel regression techniques . in our model , the translation task is viewed as a string - to - string mapping , for which a regression type learning is employed with both the source and the target sentences embedded into their kernel induced feature spaces . we report the experiments on a french - english translation task showing encouraging results .
RANK = 10; score = 0.2076923076923077; correct = False; id = 258d08e8dab99820361cd91364caded4975f5c3e
combining recurrent and convolutional neural networks for relation classification this paper investigates two different neural architectures for the task of relation classification : convolutional neural networks and recurrent neural networks . for both models , we demonstrate the effect of different architectural choices . we present a new context representation for convolutional neural networks for relation classification ( extended middle context ) . furthermore , we propose connectionist bi - directional recurrent neural networks and introduce ranking loss for their optimization . finally , we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results . our neural models achieve state - of - the - art results on the semeval 2010 relation classification task .
RANK = 11; score = 0.2072072072072072; correct = False; id = 010df54445ab5f47582eb668dc3488a3e46b55d3
unsupervised neural hidden markov models in this work , we present the first results for neuralizing an unsupervised hidden markov model . we evaluate our approach on tag induction . our approach outperforms existing generative models and is competitive with the state - of - the - art though with a simpler model easily extended to include additional context .
RANK = 12; score = 0.20714285714285716; correct = False; id = 0095c269e7d0c990249312687fc43521019809c4
modelling interaction of sentence pair with coupled - lstms recently , there is rising interest in modelling the interactions of two sentences with deep neural networks . however , most of the existing methods encode two sequences with separate encoders , in which a sentence is encoded with little or no information from the other sentence . in this paper , we propose a deep architecture to model the strong interaction of sentence pair with two coupled - lstms . specifically , we introduce two coupled ways to model the interdependences of two lstms , coupling the local contextualized interactions of two sentences . we then aggregate these interactions and use a dynamic pooling to select the most informative features . experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state - ofthe - art methods .
RANK = 13; score = 0.20714285714285716; correct = False; id = 3981b1c4fe4edba811bcbfcf743af82c768d3753
neural summarization by extracting sentences and words traditional approaches to extractive summarization rely heavily on humanengineered features . in this work we propose a data - driven approach based on neural networks and continuous sentence features . we develop a general framework for single - document summarization composed of a hierarchical document encoder and an attention - based extractor . this architecture allows us to develop different classes of summarization models which can extract sentences or words . we train our models on large scale corpora containing hundreds of thousands of document - summary pairs . experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .
RANK = 14; score = 0.20588235294117646; correct = False; id = 0e479e745374189934b57eafe637c53b490ad1c3
forest rescoring : faster decoding with integrated language models efficient decoding has been a fundamental problem in machine translation , especially with an integrated language model which is essential for achieving good translation quality . we develop faster approaches for this problem based on k - best parsing algorithms and demonstrate their effectiveness on both phrase - based and syntax - based mt systems . in both cases , our methods achieve significant speed improvements , often by more than a factor of ten , over the conventional beam - search method at the same levels of search error and translation accuracy .
RANK = 15; score = 0.2052980132450331; correct = False; id = 5d43224147a5bb8b17b6a6fc77bf86490e86991a
a recursive recurrent neural network for statistical machine translation in this paper , we propose a novel recursive recurrent neural network ( r2nn ) to model the end - to - end decoding process for statistical machine translation . r2nn is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner . a semi - supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly . experiments on a chinese to english translation task show that our proposed r2nn can outperform the stateof - the - art baseline by about 1.5 points in bleu .
RANK = 16; score = 0.2047244094488189; correct = False; id = 26781d6a1484f6a940c502ab59cedcab316d0f5d
fast consensus hypothesis regeneration for machine translation this paper presents a fast consensus hypothesis regeneration approach for machine translation . it combines the advantages of feature - based fast consensus decoding and hypothesis regeneration . our approach is more efficient than previous work on hypothesis regeneration , and it explores a wider search space than consensus decoding , resulting in improved performance . experimental results show consistent improvements across language pairs , and an improvement of up to 0.72 bleu is obtained over a competitive single - pass baseline on the chinese - toenglish nist task .
RANK = 17; score = 0.2047244094488189; correct = False; id = c31019e3fb8a3ffacb22da5744f560294486c257
supervised attentions for neural machine translation in this paper , we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs . we simply compute the distance between the machine attentions and the “ true ” alignments , and minimize this cost in the training procedure . our experiments on large - scale chinese - to - english task show that our model improves both translation and alignment qualities significantly over the large - vocabulary neural machine translation system , and even beats a state - of - the - art traditional syntax - based system .
RANK = 18; score = 0.20454545454545456; correct = False; id = b54a9d512bf269c8021b3a07030b518a85e1e799
simple and effective approach for consistent training of hierarchical phrase - based translation models in this paper , we present a simple approach for consistent training of hierarchical phrase - based translation models . in order to consistently train a translation model , we perform hierarchical phrasebased decoding on training data to find derivations between the source and target sentences . this is done by synchronous parsing the given sentence pairs . after extracting k - best derivations , we reestimate the translation model probabilities based on collected rule counts . we show the effectiveness of our procedure on the iwslt german→english and english→french translation tasks . our results show improvements of up to 1.6 points bleu .
RANK = 19; score = 0.20408163265306123; correct = False; id = 7f0232b9ffbc637cdddaed2520098cddc066e5d6
lstm neural reordering feature for statistical machine translation artificial neural networks are powerful models , which have been widely applied into many aspects of machine translation , such as language modeling and translation modeling . though notable improvements have been made in these areas , the reordering problem still remains a challenge in statistical machine translations . in this paper , we present a novel neural reordering model that directly models word pairs and their alignment . further by utilizing lstm recurrent neural networks , much longer context could be learned for reordering prediction . experimental results on nist openmt12 arabic - english and chinese - english 1000-best rescoring task show that our lstm neural reordering feature is robust , and achieves significant improvements over various baseline systems .
RANK = 20; score = 0.2037037037037037; correct = True; id = 5d3caf3dbea2247ca849ba6c3c15105fa8143f39
encoding source language with convolutional neural network for machine translation the recently proposed neural network joint model ( nnjm ) ( devlin et al . , 2014 ) augments the n - gram target language model with a heuristically chosen source context window , achieving state - of - the - art performance in smt . in this paper , we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information . with different guiding signals during decoding , our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word , and fuse them with the context of entire source sentence to form a unified representation . this representation , together with target language words , are fed to a deep neural network ( dnn ) to form a stronger nnjm . experiments on two nist chinese - english translation tasks show that the proposed model can achieve significant improvements over the previous nnjm by up to + 1.01 bleu points on average .

RANKING 11
QUERY
statistical deobfuscation of android applications this work presents a new approach for deobfuscating android apks based on probabilistic learning of large code bases ( termed " big code " ) . the key idea is to learn a probabilistic model over thousands of non - obfuscated android applications and to use this probabilistic model to deobfuscate new , unseen android apks . the concrete focus of the paper is on reversing layout obfuscation , a popular transformation which renames key program elements such as classes , packages , and methods , thus making it difficult to understand what the program does . concretely , the paper : ( i ) phrases the layout deobfuscation problem of android apks as structured prediction in a probabilistic graphical model , ( ii ) instantiates this model with a rich set of features and constraints that capture the android setting , ensuring both semantic equivalence and high prediction accuracy , and ( iii ) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions . we implemented our approach in a tool called deguard and used it to : ( i ) reverse the layout obfuscation performed by the popular proguard system on benign , open - source applications , ( ii ) predict third - party libraries imported by benign apks ( also obfuscated by proguard ) , and ( iii ) rename obfuscated program elements of android malware . the experimental results indicate that deguard is practically effective : it recovers 79.1 % of the program element names obfuscated with proguard , it predicts third - party libraries with accuracy of 91.3 % , and it reveals string decoders and classes that handle sensitive data in android malware .
First cited at 1479
TOP CITED PAPERS
RANK 1479
aletheia : improving the usability of static security analysis the scale and complexity of modern software systems complicate manual security auditing . automated analysis tools are gradually becoming a necessity . specifically , static security analyses carry the promise of efficiently verifying large code bases . yet , a critical usability barrier , hindering the adoption of static security analysis by developers , is the excess of false reports . current tools do not offer the user any direct means of customizing or cleansing the report . the user is thus left to review hundreds , if not thousands , of potential warnings , and classify them as either actionable or spurious . this is both burdensome and error prone , leaving developers disenchanted by static security checkers . we address this challenge by introducing a general technique to refine the output of static security checkers . the key idea is to apply statistical learning to the warnings output by the analysis based on user feedback on a small set of warnings . this leads to an interactive solution , whereby the user classifies a small fragment of the issues reported by the analysis , and the learning algorithm then classifies the remaining warnings automatically . an important aspect of our solution is that it is user centric . the user can express different classification policies , ranging from strong bias toward elimination of false warnings to strong bias toward preservation of true warnings , which our filtering system then executes . we have implemented our approach as the aletheia tool . our evaluation of aletheia on a diversified set of nearly 4,000 client - side javascript benchmarks , extracted from 675 popular web sites , is highly encouraging . as an example , based only on 200 classified warnings , and with a policy biased toward preservation of true warnings , aletheia is able to boost precision by a threefold factor ( x 2.868 ) , while reducing recall by a negligible factor ( x 1.006 ) . other policies are enforced with a similarly high level of efficacy .
RANK 7547
dissecting android malware : characterization and evolution the popularity and adoption of smart phones has greatly stimulated the spread of mobile malware , especially on the popular platforms such as android . in light of their rapid growth , there is a pressing need to develop effective solutions . however , our defense capability is largely constrained by the limited understanding of these emerging mobile malware and the lack of timely access to related samples . in this paper , we focus on the android platform and aim to systematize or characterize existing android malware . particularly , with more than one year effort , we have managed to collect more than 1,200 malware samples that cover the majority of existing android malware families , ranging from their debut in august 2010 to recent ones in october 2011 . in addition , we systematically characterize them from various aspects , including their installation methods , activation mechanisms as well as the nature of carried malicious payloads . the characterization and a subsequent evolution - based study of representative families reveal that they are evolving rapidly to circumvent the detection from existing mobile anti - virus software . based on the evaluation with four representative mobile security software , our experiments show that the best case detects 79.6 % of them while the worst case detects only 20.2 % in our dataset . these results clearly call for the need to better develop next - generation anti - mobile - malware solutions .
RANK 7784
recognizing functions in binaries with neural networks binary analysis facilitates many important applications like malware detection and automatically fixing vulnerable software . in this paper , we propose to apply artificial neural networks to solve important yet difficult problems in binary analysis . specifically , we tackle the problem of function identification , a crucial first step in many binary analysis techniques . although neural networks have undergone a renaissance in the past few years , achieving breakthrough results in multiple application domains such as visual object recognition , language modeling , and speech recognition , no researchers have yet attempted to apply these techniques to problems in binary analysis . using a dataset from prior work , we show that recurrent neural networks can identify functions in binaries with greater accuracy and efficiency than the state - of - the - art machine - learning - based method . we can train the model an order of magnitude faster and evaluate it on binaries hundreds of times faster . furthermore , it halves the error rate on six out of eight benchmarks , and performs comparably on the remaining two .
TOP UNCITED PAPERS
RANK 1
semi - supervised semantic tagging of conversational understanding using markov topic regression finding concepts in natural language utterances is a challenging task , especially given the scarcity of labeled data for learning semantic ambiguity . furthermore , data mismatch issues , which arise when the expected test ( target ) data does not exactly match the training data , aggravate this scarcity problem . to deal with these issues , we describe an efficient semisupervised learning ( ssl ) approach which has two components : ( i ) markov topic regression is a new probabilistic model to cluster words into semantic tags ( concepts ) . it can efficiently handle semantic ambiguity by extending standard topic models with two new features . first , it encodes word n - gram features from labeled source and unlabeled target data . second , by going beyond a bag - of - words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context . ( ii ) retrospective learner is a new learning technique that adapts to the unlabeled target data . our new ssl approach improves semantic tagging performance by 3 % absolute over the baseline models , and also compares favorably on semi - supervised syntactic tagging .
RANK 2
improve statistical machine translation with context - sensitive bilingual semantic embedding model we investigate how to improve bilingual embedding which has been successfully used as a feature in phrase - based statistical machine translation ( smt ) . despite bilingual embedding ’s success , the contextual information , which is of critical importance to translation quality , was ignored in previous work . to employ the contextual information , we propose a simple and memory - efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account . bilingual translation scores generated from our proposed bilingual embedding model are used as features in our smt system . experimental results show that the proposed method achieves significant improvements on large - scale chinese - english translation task .
RANK 3
dual training and dual prediction for polarity classification bag - of - words ( bow ) is now the most popular way to model text in machine learning based sentiment classification . however , the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the bow model . in this paper , we focus on the polarity shift problem , and propose a novel approach , called dual training and dual prediction ( dtdp ) , to address it . the basic idea of dtdp is to first generate artificial samples that are polarity - opposite to the original samples by polarity reversion , and then leverage both the original and opposite samples for ( dual ) training and ( dual ) prediction . experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification .
TOP 20
RANK = 1; score = 0.18859649122807018; correct = False; id = b4ecca89aacb66b163f76d32e6e0c18368441c0a
semi - supervised semantic tagging of conversational understanding using markov topic regression finding concepts in natural language utterances is a challenging task , especially given the scarcity of labeled data for learning semantic ambiguity . furthermore , data mismatch issues , which arise when the expected test ( target ) data does not exactly match the training data , aggravate this scarcity problem . to deal with these issues , we describe an efficient semisupervised learning ( ssl ) approach which has two components : ( i ) markov topic regression is a new probabilistic model to cluster words into semantic tags ( concepts ) . it can efficiently handle semantic ambiguity by extending standard topic models with two new features . first , it encodes word n - gram features from labeled source and unlabeled target data . second , by going beyond a bag - of - words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context . ( ii ) retrospective learner is a new learning technique that adapts to the unlabeled target data . our new ssl approach improves semantic tagging performance by 3 % absolute over the baseline models , and also compares favorably on semi - supervised syntactic tagging .
RANK = 2; score = 0.18652849740932642; correct = False; id = 58ad5b84b17fb70b353b26f41715197ecb77e5cf
improve statistical machine translation with context - sensitive bilingual semantic embedding model we investigate how to improve bilingual embedding which has been successfully used as a feature in phrase - based statistical machine translation ( smt ) . despite bilingual embedding ’s success , the contextual information , which is of critical importance to translation quality , was ignored in previous work . to employ the contextual information , we propose a simple and memory - efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account . bilingual translation scores generated from our proposed bilingual embedding model are used as features in our smt system . experimental results show that the proposed method achieves significant improvements on large - scale chinese - english translation task .
RANK = 3; score = 0.18518518518518517; correct = False; id = 65688d7a888d975c64878cf9e0e31a8182f05f64
dual training and dual prediction for polarity classification bag - of - words ( bow ) is now the most popular way to model text in machine learning based sentiment classification . however , the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the bow model . in this paper , we focus on the polarity shift problem , and propose a novel approach , called dual training and dual prediction ( dtdp ) , to address it . the basic idea of dtdp is to first generate artificial samples that are polarity - opposite to the original samples by polarity reversion , and then leverage both the original and opposite samples for ( dual ) training and ( dual ) prediction . experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification .
RANK = 4; score = 0.1827956989247312; correct = False; id = 5e80938dedf6c8b8fe6d05278ef47eb957ca931e
improving dependency parsing with subtrees from auto - parsed data this paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto - parsed data . first , we use a baseline parser to parse large - scale unannotated data . then we extract subtrees from dependency parse trees in the auto - parsed data . finally , we construct new subtree - based features for parsing algorithms . to demonstrate the effectiveness of our proposed approach , we present the experimental results on the english penn treebank and the chinese penn treebank . these results show that our approach significantly outperforms baseline systems . and , it achieves the best accuracy for the chinese data and an accuracy which is competitive with the best known systems for the english data .
RANK = 5; score = 0.1822429906542056; correct = False; id = 82c5c16e79bb6dd115a0b5d3986b93dbd523355e
obfuscation resilient binary code reuse through trace - oriented programming with the wide existence of binary code , it is desirable to reuse it in many security applications , such as malware analysis and software patching . while prior approaches have shown that binary code can be extracted and reused , they are often based on static analysis and face challenges when coping with obfuscated binaries . this paper introduces trace - oriented programming ( top ) , a general framework for generating new software from existing binary code by elevating the low - level binary code to c code with templates and inlined assembly . different from existing work , top gains benefits from dynamic analysis such as resilience against obfuscation and avoidance of points - to analysis . thus , top can be used for malware analysis , especially for malware function analysis and identification . we have implemented a proof - of - concept of top and our evaluation results with a range of benign and malicious software indicate that top is able to reconstruct source code from binary execution traces in malware analysis and identification , and binary function transplanting .
RANK = 6; score = 0.18134715025906736; correct = False; id = 12ca0368e1b63b27f3d8d8b227fb4a4bddfd907a
discovery of dependency tree patterns for relation extraction relation extraction is to identify the relations between pairs of named entities . in this paper , we try to solve the problem of relation extraction by discovering dependency tree patterns ( a pattern is an embedded sub dependency tree indicating a relation instance ) . our approach is to find an optimal rule ( pattern ) set automatically based on the proposed dependency tree pattern mining algorithm . the experimental results show that the extracted patterns can achieve a high precision and a reasonable recall rate when used as rules to extract relation instances . furthermore , an additional experiment shows that other machine learning based relation extraction methods can also benefit from the extracted patterns by using them as features .
RANK = 7; score = 0.18095238095238095; correct = False; id = f2670fcc8a3e944d796775d1a2d96e5b81d79909
sensespotting : never let your parallel data tie you to an old domain words often gain new senses in new domains . being able to automatically identify , from a corpus of monolingual text , which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics . we define a task , sensespotting , in which we build systems to spot tokens that have new senses in new domain text . instead of difficult and expensive annotation , we build a goldstandard by leveraging cheaply available parallel corpora , targeting our approach to the problem of domain adaptation for machine translation . our system is able to achieve f - measures of as much as 80 % , when applied to word types it has never seen before . our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains .
RANK = 8; score = 0.1793478260869565; correct = False; id = 33e5a26842bcc37f5333bf808b2b6c9f56151d80
partial matching strategy for phrase - based statistical machine translation this paper presents a partial matching strategy for phrase - based statistical machine translation ( pbsmt ) . source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases . the advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited . we incorporate our approach into the state - of - the - art pbsmt system moses and achieve statistically significant improvements on both small and large corpora .
RANK = 9; score = 0.1792452830188679; correct = False; id = 0b8633797e534b161a81e1666d79c2080d06ed00
a generic approach to automatic deobfuscation of executable code malicious software are usually obfuscated to avoid detection and resist analysis . when new malware is encountered , such obfuscations have to be penetrated or removed ( " deobfuscated " ) in order to understand the internal logic of the code and devise countermeasures . this paper discusses a generic approach for deobfuscation of obfuscated executable code . our approach does not make any assumptions about the nature of the obfuscations used , but instead uses semantics - preserving program transformations to simplify away obfuscation code . we have applied a prototype implementation of our ideas to a variety of different kinds of obfuscation , including emulation - based obfuscation , emulation - based obfuscation with runtime code unpacking , and return - oriented programming . our experimental results are encouraging and suggest that this approach can be effective in extracting the internal logic from code obfuscated using a variety of obfuscation techniques , including tools such as themida that previous approaches could not handle .
RANK = 10; score = 0.17889908256880735; correct = False; id = 8cd086d63d0bfc0986c9b56a732f34e00234da7b
direct translation model 2 this paper presents a maximum entropy machine translation system using a minimal set of translation blocks ( phrase - pairs ) . while recent phrase - based statistical machine translation ( smt ) systems achieve significant improvement over the original source - channel statistical translation models , they 1 ) use a large inventory of blocks which have significant overlap and 2 ) limit the use of training to just a few parameters ( on the order of ten ) . in contrast , we show that our proposed minimalist system ( dtm2 ) achieves equal or better performance by 1 ) recasting the translation problem in the traditional statistical modeling approach using blocks with no overlap and 2 ) relying on training most system parameters ( on the order of millions or larger ) . the new model is a direct translation model ( dtm ) formulation which allows easy integration of additional / alternative views of both source and target sentences such as segmentation for a source language such as arabic , part - of - speech of both source and target , etc . we show improvements over a state - of - the - art phrase - based decoder in arabic - english translation .
RANK = 11; score = 0.178743961352657; correct = False; id = 0182702cf31e1eac04bbc1a770a30d48f25ba29d
learning to rank definitions to generate quizzes for interactive information presentation this paper proposes the idea of ranking definitions of a person ( a set of biographical facts ) to automatically generate “ who is this ? ” quizzes . the definitions are ordered according to how difficult they make it to name the person . such ranking would enable users to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation , which is useful for educational systems . in our approach , we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content . experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual informa-
RANK = 12; score = 0.17857142857142858; correct = False; id = 6e098378352714b979195734ad42cf84807da379
measure word generation for english - chinese smt systems measure words in chinese are used to indicate the count of nouns . conventional statistical machine translation ( smt ) systems do not perform well on measure word generation due to data sparseness and the potential long distance dependency between measure words and their corresponding head words . in this paper , we propose a statistical model to generate appropriate measure words of nouns for an english - to - chinese smt system . we model the probability of measure word generation by utilizing lexical and syntactic knowledge from both source and target sentences . our model works as a post - processing procedure over output of statistical machine translation systems , and can work with any smt system . experimental results show our method can achieve high precision and recall in measure word generation .
RANK = 13; score = 0.1783783783783784; correct = False; id = ae83cc2a1425196215a06aec25032b305896d223
an extensible probabilistic transformation - based approach to the third recognizing textual entailment challenge we introduce a system for textual entailment that is based on a probabilistic model of entailment . the model is defined using some calculus of transformations on dependency trees , which is characterized by the fact that derivations in that calculus preserve the truth only with a certain probability . we also describe a possible set of transformations ( and with it implicitly a calculus ) that was successfully applied to the rte3 challenge data . however , our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment .
RANK = 14; score = 0.1774193548387097; correct = False; id = 0669ebcf43c8230b525626c1e8eab3951fecd2b4
non - projective parsing for statistical machine translation we describe a novel approach for syntaxbased statistical mt , which builds on a variant of tree adjoining grammar ( tag ) . inspired by work in discriminative dependency parsing , the key idea in our approach is to allow highly flexible reordering operations during parsing , in combination with a discriminative model that can condition on rich features of the sourcelanguage string . experiments on translation from german to english show improvements over phrase - based systems , both in terms of bleu scores and in human evaluations .
RANK = 15; score = 0.17703349282296652; correct = False; id = 8355fac830f62f4eca6ea80f6e0666d20704bb8a
efficient disfluency detection with transition - based parsing automatic speech recognition ( asr ) outputs often contain various disfluencies . it is necessary to remove these disfluencies before processing downstream tasks . in this paper , an efficient disfluency detection approach based on right - to - left transitionbased parsing is proposed , which can efficiently identify disfluencies and keep asr outputs grammatical . our method exploits a global view to capture long - range dependencies for disfluency detection by integrating a rich set of syntactic and disfluency features with linear complexity . the experimental results show that our method outperforms state - of - the - art work and achieves a 85.1 % f - score on the commonly used english switchboard test set . we also apply our method to in - house annotated chinese data and achieve a significantly higher f - score compared to the baseline of crf - based approach .
RANK = 16; score = 0.1761904761904762; correct = False; id = da771a53f09788c36bb899fe059a58e79bb991ad
smt helps bitext dependency parsing we propose a method to improve the accuracy of parsing bilingual texts ( bitexts ) with the help of statistical machine translation ( smt ) systems . previous bitext parsing methods use human - annotated bilingual treebanks that are hard to obtain . instead , our approach uses an auto - generated bilingual treebank to produce bilingual constraints . however , because the auto - generated bilingual treebank contains errors , the bilingual constraints are noisy . to overcome this problem , we use large - scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results . the experimental results show that our new parsers significantly outperform state - of - theart baselines . moreover , our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline . especially notable is that our approach can be used in a purely monolingual setting with the help of smt .
RANK = 17; score = 0.17587939698492464; correct = False; id = d73a2dbdd0292ed03275e0741dacf6eb964ef170
the feature subspace method for smt system combination recently system combination has been shown to be an effective way to improve translation quality over single machine translation systems . in this paper , we present a simple and effective method to systematically derive an ensemble of smt systems from one baseline linear smt model for use in system combination . each system in the resulting ensemble is based on a feature set derived from the features of the baseline model ( typically a subset of it ) . we will discuss the principles to determine the feature sets for derived systems , and present in detail the system combination model used in our work . evaluation is performed on the data sets for nist 2004 and nist 2005 chinese - to - english machine translation tasks . experimental results show that our method can bring significant improvements to baseline systems with state - of - the - art performance .
RANK = 18; score = 0.17560975609756097; correct = False; id = 1884d543fcc522c1fd2d16052d7844af97500578
structured prediction with output embeddings for semantic image annotation we address the task of annotating images with semantic tuples . solving this problem requires an algorithm which is able to deal with hundreds of classes for each argument of the tuple . in such contexts , data sparsity becomes a key challenge , as there will be a large number of classes for which only a few examples are available . we propose handling this by incorporating feature representations of both the inputs ( images ) and outputs ( argument classes ) into a factorized log - linear model , and exploiting the flexibility of scoring functions based on bilinear forms . experiments show that integrating feature representations of the outputs in the structured prediction model leads to better overall predictions . we also conclude that the best output representation is specific for each type of argument .
RANK = 19; score = 0.17543859649122806; correct = False; id = debbb3ccf3ea3396e5cc511dc2a546c7f99f4a09
learning to adapt to unknown users : referring expression generation in spoken dialogue systems we present a data - driven approach to learn user - adaptive referring expression generation ( reg ) policies for spoken dialogue systems . referring expressions can be difficult to understand in technical domains where users may not know the technical ‘ jargon’ names of the domain entities . in such cases , dialogue systems must be able to model the user ’s ( lexical ) domain knowledge and use appropriate referring expressions . we present a reinforcement learning ( rl ) framework in which the system learns reg policies which can adapt to unknown users online . furthermore , unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on , we show that effective adaptive policies can be learned from a small dialogue corpus of non - adaptive human - machine interaction , by using a rl framework and a statistical user simulation . we show that in comparison to adaptive hand - coded baseline policies , the learned policy performs significantly better , with an 18.6 % average increase in adaptation accuracy . the best learned policy also takes less dialogue time ( average 1.07 min less ) than the best hand - coded policy . this is because the learned policies can adapt online to changing evidence about the user ’s domain expertise .
RANK = 20; score = 0.17535545023696683; correct = False; id = 4fb7fbfad0d50af17db260e9da9fc68762b37eac
vocabulary manipulation for neural machine translation in order to capture rich language phenomena , neural machine translation models have to use a large vocabulary size , which requires high computing time and large memory usage . in this paper , we alleviate this issue by introducing a sentence - level or batch - level vocabulary , which is only a very small sub - set of the full output vocabulary . for each sentence or batch , we only predict the target words in its sentencelevel or batch - level vocabulary . thus , we reduce both the computing time and the memory usage . our method simply takes into account the translation options of each word or phrase in the source sentence , and picks a very small target vocabulary for each sentence based on a wordto - word translation model or a bilingual phrase library learned from a traditional machine translation model . experimental results on the large - scale english - tofrench task show that our method achieves better translation performance by 1 bleu point over the large vocabulary neural machine translation system of jean et al . ( 2015 ) .

RANKING 413
QUERY
mit at semeval-2017 task 10 : relation extraction with convolutional neural networks over 50 million scholarly articles have been published : they constitute a unique repository of knowledge . in particular , one may infer from them relations between scientific concepts . artificial neural networks have recently been explored for relation extraction . in this work , we continue this line of work and present a system based on a convolutional neural network to extract relations . our model ranked first in the semeval-2017 task 10 ( scienceie ) for relation extraction in scientific articles ( subtask c ) .
First cited at 4
TOP CITED PAPERS
RANK 4
sequential short - text classification with recurrent and convolutional neural networks recent approaches based on artificial neural networks ( anns ) have shown promising results for short - text classification . however , many short texts occur in sequences ( e.g. , sentences in a document or utterances in a dialog ) , and most existing ann - based systems do not leverage the preceding short texts when classifying a subsequent one . in this work , we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts . our model achieves state - of - the - art results on three different datasets for dialog act prediction .
RANK 21
simple customization of recursive neural networks for semantic relation classification in this paper , we present a recursive neural network ( rnn ) model that works on a syntactic tree . our model differs from previous rnn models in that the model allows for an explicit weighting of important phrases for the target task . we also propose to average parameters in training . our experimental results on semantic relation classification show that both phrase categories and task - specific weighting significantly improve the prediction accuracy of the model . we also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity . the proposed model marks scores competitive with state - of - the - art rnn - based models .
RANK 125
relation extraction : perspective from convolutional neural networks up to now , relation extraction systems have made extensive use of features generated by linguistic analysis modules . errors in these features lead to errors of relation detection and classification . in this work , we depart from these traditional approaches with complicated feature engineering by introducing a convolutional neural network for relation extraction that automatically learns features from sentences and minimizes the dependence on external toolkits and resources . our model takes advantages of multiple window sizes for filters and pre - trained word embeddings as an initializer on a non - static architecture to improve the performance . we emphasize the relation extraction problem with an unbalanced corpus . the experimental results show that our system significantly outperforms not only the best baseline systems for relation extraction but also the state - of - the - art systems for relation classification .
TOP UNCITED PAPERS
RANK 1
chinese open relation extraction for knowledge acquisition this study presents the chinese open relation extraction ( core ) system that is able to extract entity - relation triples from chinese free texts based on a series of nlp techniques , i.e. , word segmentation , pos tagging , syntactic parsing , and extraction rules . we employ the proposed core techniques to extract more than 13 million entity - relations for an open domain question answering application . to our best knowledge , core is the first chinese open ie system for knowledge acquisition .
RANK 2
combining syntactic patterns and wikipedia 's hierarchy of hyperlinks to extract meronym relations we present here two methods for extraction o , meronymic relation : ( a ) the first one relies solely on syntactic information . unlike other approaches based on simple patterns , we determine their optimal combination to extract word pairs linked via a given semantic relation ; ( b ) the second approach consists in combining syntactic patterns with the semantic information extracted from the wikipedia hyperlink hierarchy ( whh ) of the constituent words . by comparing our work with semeval 2007 ( task 4 test set ) and wordnet ( wn ) we found that our system clearly outperforms its competitors .
RANK 3
uttime : temporal relation classification using deep syntactic features in this paper , we present a system , uttime , which we submitted to tempeval-3 for task c : annotating temporal relations . the system uses logistic regression classifiers and exploits features extracted from a deep syntactic parser , including paths between event words in phrase structure trees and their path lengths , and paths between event words in predicateargument structures and their subgraphs . uttime achieved an f1 score of 34.9 based on the graphed - based evaluation for task c ( ranked 2 ) and 56.45 for task c - relationonly ( ranked 1 ) in the tempeval-3 evaluation .
TOP 20
RANK = 1; score = 0.23076923076923078; correct = False; id = 19f83e125aa6384197b7a1fb138f0de531ab73eb
chinese open relation extraction for knowledge acquisition this study presents the chinese open relation extraction ( core ) system that is able to extract entity - relation triples from chinese free texts based on a series of nlp techniques , i.e. , word segmentation , pos tagging , syntactic parsing , and extraction rules . we employ the proposed core techniques to extract more than 13 million entity - relations for an open domain question answering application . to our best knowledge , core is the first chinese open ie system for knowledge acquisition .
RANK = 2; score = 0.22950819672131148; correct = False; id = c39ab0733afcb58ceba3051b31b79d454fce9452
combining syntactic patterns and wikipedia 's hierarchy of hyperlinks to extract meronym relations we present here two methods for extraction o , meronymic relation : ( a ) the first one relies solely on syntactic information . unlike other approaches based on simple patterns , we determine their optimal combination to extract word pairs linked via a given semantic relation ; ( b ) the second approach consists in combining syntactic patterns with the semantic information extracted from the wikipedia hyperlink hierarchy ( whh ) of the constituent words . by comparing our work with semeval 2007 ( task 4 test set ) and wordnet ( wn ) we found that our system clearly outperforms its competitors .
RANK = 3; score = 0.22522522522522523; correct = False; id = 80dcba393ad80614ea4d0d679c2ea0596e77de9a
uttime : temporal relation classification using deep syntactic features in this paper , we present a system , uttime , which we submitted to tempeval-3 for task c : annotating temporal relations . the system uses logistic regression classifiers and exploits features extracted from a deep syntactic parser , including paths between event words in phrase structure trees and their path lengths , and paths between event words in predicateargument structures and their subgraphs . uttime achieved an f1 score of 34.9 based on the graphed - based evaluation for task c ( ranked 2 ) and 56.45 for task c - relationonly ( ranked 1 ) in the tempeval-3 evaluation .
RANK = 4; score = 0.22522522522522523; correct = True; id = 0a7c04c252621633992810bf0f184f287610c461
sequential short - text classification with recurrent and convolutional neural networks recent approaches based on artificial neural networks ( anns ) have shown promising results for short - text classification . however , many short texts occur in sequences ( e.g. , sentences in a document or utterances in a dialog ) , and most existing ann - based systems do not leverage the preceding short texts when classifying a subsequent one . in this work , we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts . our model achieves state - of - the - art results on three different datasets for dialog act prediction .
RANK = 5; score = 0.2222222222222222; correct = False; id = 66ed861a6e47041be45e65aef0a73a44c4d35f76
ontologizing semantic relations many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories . in this paper , we propose two algorithms for automatically ontologizing ( attaching ) semantic relations into wordnet . we present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on f - score over a baseline model .
RANK = 6; score = 0.22115384615384615; correct = False; id = 5a535b64c4a178fd065354c1fab8312c8ac36de5
hhu at semeval-2016 task 1 : multiple approaches to measuring semantic textual similarity this paper describes our participation in the semeval-2016 task 1 : semantic textual similarity ( sts ) . we developed three methods for the english subtask ( sts core ) . the first method is unsupervised and uses wordnet and word2vec to measure a token - based overlap . in our second approach , we train a neural network on two features . the third method uses word2vec and lda with regression splines .
RANK = 7; score = 0.22115384615384615; correct = False; id = 8797763a3dedce3f40e4ee69efdb3b55d6f985b8
aueb.twitter.sentiment at semeval-2016 task 4 : a weighted ensemble of svms for twitter sentiment analysis this paper describes the system with which we participated in semeval-2016 task 4 ( sentiment analysis in twitter ) and specifically the message polarity classification subtask . our system is a weighted ensemble of two systems . the first one is based on a previous sentiment analysis system and uses manually crafted features . the second system of our ensemble uses features based on word embeddings . our ensemble was ranked 5th among 34 teams . the source code of our system is publicly available .
RANK = 8; score = 0.22033898305084745; correct = False; id = c27cff0fca6d12ef1d6390d4a27f34103c478472
m2l at semeval-2016 task 8 : amr parsing with neural networks this paper describes our contribution to the semeval 2016 workshop . we participated in the shared task 8 on meaning representation parsing using a transition - based approach , which builds upon the system of wang et al . ( 2015a ) and wang et al . ( 2015b ) , with additions that utilize a feedforward neural network classifier and an enriched feature set . we observed that exploiting neural networks in abstract meaning representation parsing is challenging and we could not benefit from it , while the feature enhancements yielded an improved performance over the baseline model .
RANK = 9; score = 0.2184873949579832; correct = False; id = a14758da64e24d4524cacd70567a18e47d8311a5
mte - nn at semeval-2016 task 3 : can machine translation evaluation help community question answering ? we present a system for answer ranking ( semeval-2016 task 3 , subtask a ) that is a direct adaptation of a pairwise neural network model for machine translation evaluation ( mte ) . in particular , the network incorporates mte features , as well as rich syntactic and semantic embeddings , and it efficiently models complex non - linear interactions between them . with the addition of lightweight task - specific features , we obtained very encouraging experimental results , with sizeable contributions from both the mte features and from the pairwise network architecture . we also achieved good results on subtask c.
RANK = 10; score = 0.21739130434782608; correct = False; id = ceac2fee6da39ff03316e988b7cc5526ca395afa
toward a discourse theory for annotating causal relations in japanese we present a revised discourse theory based on segmented discourse representation theory and provide a method for building a japanese corpus suitable for causal relation extraction . this extends and refines the framework proposed in kaneko and bekki ( 2014 ) , and we evaluate our corpus and compare it with that work .
RANK = 11; score = 0.21551724137931033; correct = False; id = 2e703e13835c49ce73c6ec6712afab84991a0c0d
the uned systems at senseval-2 we have participated in the senseval-2 english tasks ( all words and lexical sample ) with an unsupervised system based on mutual information measured over a large corpus ( 277 million words ) and some additional heuristics . a supervised extension of the system was also presented to the lexical sample task . our system scored first among unsupervised systems in both tasks : 56.9 % recall in all words , 40.2 % in lexical sample . this is slightly worse than the first sense heuristic for all . words and 3.6 % better for the lexical sample , a strong indication that unsupervised word sense disambiguation remains being a strong challenge .
RANK = 12; score = 0.21487603305785125; correct = False; id = d11d03dedaff05616f1fccef2b936d79e158b88a
icrc - hit : a deep learning based comment sequence labeling system for answer selection challenge in this paper , we present a comment labeling system based on a deep learning strategy . we treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments . in the recurrent architecture of our system , our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question - comment pair , and assigns the labels to the comment sequence with a recurrent neural network over cnn . compared with the conditional random fields based method , our approach performs better performance on macro - f1 ( 53.82 % ) , and achieves the highest accuracy ( 73.18 % ) , f1-value ( 79.76 % ) on predicting the good class in this answer selection challenge .
RANK = 13; score = 0.21428571428571427; correct = False; id = 57aaa7549575dbdd4005b31a475cecd2336508a8
applying umls for distantly supervised relation detection this paper describes first results using the unified medical language system ( umls ) for distantly supervised relation extraction . umls is a large knowledge base which contains information about millions of medical concepts and relations between them . our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in umls .
RANK = 14; score = 0.2127659574468085; correct = False; id = c64337577fb065cb11e7425d277a93313aa59de7
sagan : an approach to semantic textual similarity based on textual entailment in this paper we report the results obtained in the semantic textual similarity ( sts ) task , with a system primarily developed for textual entailment . our results are quite promising , getting a run ranked 39 in the official results with overall pearson , and ranking 29 with the mean metric .
RANK = 15; score = 0.21232876712328766; correct = False; id = 1e7a77ef375a4f48ec5f592e9b9cc8345c4d44d9
niletmrg at semeval-2016 task 5 : deep convolutional neural networks for aspect category and sentiment extraction this paper describes our participation in the semeval-2016 task 5 , aspect based sentiment analysis ( absa ) . we participated in two slots in the sentence level absa ( subtask 1 ) namely : aspect category extraction ( slot 1 ) and sentiment polarity extraction ( slot 3 ) in english restaurants and laptops reviews . for slot 1 , we applied different models for each domain . in the restaurants domain , we used an ensemble classifier for each aspect which is a combination of a convolutional neural network ( cnn ) classifier initialized with pretrained word vectors , and a support vector machine ( svm ) classifier based on the bag of words model . for the laptops domain , we used only one cnn classifier that predicts the aspects based on a probability threshold . for slot 3 , we incorporated domain and aspect knowledge in one ensemble cnn classifier initialized with fine - tuned word vectors and used it in both domains . in the restaurants domain , our system achieved the 2 and the 3 places in slot 1 and slot 3 respectively . however , we ranked the 8 in slot 1 and the 5 in slot 3 in the laptops domain . our extended experiments show our system could have ranked 2 in the laptops domain in slot 1 and slot 3 , had we followed the same approach we followed in the restaurants domain in slot 1 and trained each domain separately in slot 3 . 1 http://alt.qcri.org/semeval2016/task5/
RANK = 16; score = 0.21212121212121213; correct = False; id = 6d1817c05cc494e61e0e0b6dd553f6e4337db366
a lexicalized tree kernel for open information extraction in contrast with traditional relation extraction , which only considers a fixed set of relations , open information extraction ( open ie ) aims at extracting all types of relations from text . because of data sparseness , open ie systems typically ignore lexical information , and instead employ parse trees and part - of - speech ( pos ) tags . however , the same syntactic structure may correspond to different relations . in this paper , we propose to use a lexicalized tree kernel based on the word embeddings created by a neural network model . we show that the lexicalized tree kernel model surpasses the unlexicalized model . experiments on three datasets indicate that our open ie system performs better on the task of relation extraction than the stateof - the - art open ie systems of xu et al . ( 2013 ) and mesquita et al . ( 2013 ) .
RANK = 17; score = 0.21153846153846154; correct = False; id = db2368d8387ee9c381fd7ea3686a0ca4a297e73f
limsi - cot at semeval-2016 task 12 : temporal relation identification using a pipeline of classifiers semeval 2016 task 12 addresses temporal reasoning in the clinical domain . in this paper , we present our participation for relation extraction based on gold standard entities ( subtasks dr and cr ) . we used a supervised approach comparing plain lexical features to word embeddings for temporal relation identification , and obtained above - median scores .
RANK = 18; score = 0.21100917431192662; correct = False; id = 03068fc6c52d26111d4b745d0ad0fb2f9c9d3c1c
wvali : temporal relation identification by syntactico - semantic analysis this paper reports on the participation of university of wolverhampton and university of alicante at the semeval-2007 tempeval evaluation exercise . tempeval consisted of three tasks involving the identification of event - time and event - event temporal relations . we participated in all three tasks with tictac ( syntacticosemantic temporal annotation cluster ) , a system comprising both knowledge based and statistical techniques . our system achieved the highest strict and relaxed scores for tasks a and b , and the highest relaxed score for task c.
RANK = 19; score = 0.21100917431192662; correct = False; id = afb6c55a1c46c8e6c749b45ec78ce237f7bcc973
melodi : semantic similarity of words and compositional phrases using latent vector weighting in this paper we present our system for the semeval 2013 task 5a on semantic similarity of words and compositional phrases . our system uses a dependency - based vector space model , in combination with a technique called latent vector weighting . the system computes the similarity between a particular noun instance and the head noun of a particular noun phrase , which was weighted according to the semantics of the modifier . the system is entirely unsupervised ; one single parameter , the similarity threshold , was tuned using the training data .
RANK = 20; score = 0.21052631578947367; correct = False; id = 90aa6db47425277d1488ff18e214bddd9a733d76
lsis at semeval-2016 task 7 : using web search engines for english and arabic unsupervised sentiment intensity prediction in this paper , we present our contribution in semeval2016 task71 : determining sentiment intensity of english and arabic phrases , where we use web search engines for english and arabic unsupervised sentiment intensity prediction . our work is based , first , on a group of classic sentiment lexicons ( e.g. sentiment140 lexicon , sentiwordnet ) . second , on web search engines’ ability to find the cooccurrence of sentences with predefined negative and positive words . the use of web search engines ( e.g. google search api ) enhance the results on phrases built from opposite polarity terms .

RANKING 1316
QUERY
real - time digital signatures for time - critical networks the secure and efficient operation of time - critical networks , such as vehicular networks , smart - grid , and other smart - infrastructures , is of primary importance in today ’s society . it is crucial to minimize the impact of security mechanisms over such networks so that the safe and reliable operations of time - critical systems are not being interfered . for instance , if the delay introduced by the crypto operations negatively affects the time available for braking a car before a collision , the car may not be able to safely stop in time . in particular , as a primary authentication mechanism , existing digital signatures introduce a significant computation and communication overhead , and therefore are unable to fully meet the real - time processing requirements of such time - critical networks . in this paper , we introduce a new suite of real - time digital signatures referred to as < italic > structure - free and compact real - time authentication</italic > ( < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > ) , supported by hardware acceleration , to provide delay - aware authentication in time - critical networks . < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > is a novel signature framework that can transform any secure aggregate signature into a signer efficient signature . we instantiate < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > framework with condensed - rsa , bgls , and ntru signatures . our analytical and experimental evaluation validates the significant performance advantages of < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > schemes over their base signatures and the state - of - the - art schemes . moreover , we push the performance of < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > schemes to the edge via highly optimized implementations on vehicular capable system - on - chip as well as server - grade general purpose graphics processing units . we prove that < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > is secure ( in random oracle model ) and show that < inline - formula > < tex - math notation="latex">${scra}$ < /tex - math></inline - formula > can offer an ideal alternative for authentication in time - critical applications .
First cited at 1
TOP CITED PAPERS
RANK 1
an efficient real - time broadcast authentication scheme for command and control messages broadcast ( multicast ) authentication is crucial for large and distributed systems , such as cyber - physical infrastructures ( e.g. , power - grid / smart - grid ) and wireless networks ( e.g. , intervehicle networks , military ad hoc networks ) . these time - critical systems require real - time authentication of command and control messages in a highly efficient , secure , and scalable manner . however , existing solutions are either computationally costly ( e.g. , asymmetric cryptography ) or unscalable / impractical ( e.g. , symmetric cryptography , one - time signatures , delayed key disclosure methods ) . in this paper , we develop a new broadcast authentication scheme that we call rapid authentication ( ra ) , which is suitable for time - critical authentication of command and control messages in large and distributed systems . we exploit the semistructured nature of command and control messages to construct special digital signatures , which are computationally efficient both at the signer and verifier sides . we show that ra achieves several desirable properties that are not available in existing alternatives simultaneously : 1 ) fast signature generation and verification ; 2 ) immediate verification ; 3 ) constant size public key ; 4 ) compact authenticating tag ; 5 ) packet loss tolerance ; 6 ) being free from time synchronization requirement ; and 7 ) provable security .
RANK 768
multicast authentication in fully adversarial networks we study a general version of the multicast authentication problem where the underlying network , controlled by an adversary , may drop chosen packets , rearrange the order of the packets in an arbitrary way , and inject new packets into the transmitted stream . prior work on the problem has focused on less general models , where random , rather than adversarially - selected packets may be dropped and altered , or no additional packets may be injected into the stream . we describe an efficient and scalable authentication scheme that is based on a novel combination of error - correcting codes with standard cryptographic primitives . we prove the security of our scheme and analyze its performance in terms of the computational effort at the sender and receiver and the communication overhead . we also discuss specific design and implementation choices and compare our scheme with previously proposed approaches .
RANK 2760
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
TOP UNCITED PAPERS
RANK 2
noninteractive self - certification for long - lived mobile ad hoc networks mobile ad hoc networks ( manets ) have many well - known applications in military settings as well as in emergency and rescue operations . however , a lack of infrastructure and centralized control make manets inherently insecure , and therefore specialized security services are needed for their deployment . self - certification is an essential and fundamental security service in manets . it is needed to securely cope with dynamic membership and topology , and to bootstrap other important security primitives and services ( such as secure routing and group key management ) without the assistance of any centralized trusted authority . an ideal protocol must involve minimal interaction among the manet nodes , since connectivity can be unstable . also , since manets are often composed of weak or resource - limited devices , a self - certification protocol must be efficient in terms of computation and communication . in this paper , we propose a power - aware and fully noninteractive self - certification protocol based on bivariate polynomial secret sharing and a noninteractive threshold signature scheme . in contrast with prior work , our techniques do not require any interaction and do not involve any costly reliable broadcast communication among manet nodes . we thoroughly analyze our proposal and show that it compares favorably to previous mechanisms .
RANK 3
randomized component and its application to ( < inline - formula > < tex - math notation="latex">$t$ < /tex - math></inline - formula>,<inline - formula > < tex - math notation="latex">$m$ < /tex - math></inline - formula>,<inline - formula > < tex - math notation="latex">$n$ < /tex - math></inline - formula>)-group oriented secret s a basic ( t , n)-secret sharing ( ss ) scheme allows a secret s to be divided into n shares and shared among n shareholders . in the scheme , any t or more than t shareholders can recover the secret while fewer than t shareholders can not obtain the secret s. but an adversary without any valid share may obtain the secret if there are over t participants in the secret reconstruction . to address this type of attack , we first introduce the notion of randomized component ( rc ) , which binds a share with all participants and protects the share from being exposed to outside without any computational assumption ; at the same time , rcs can be used to reconstruct the secret . as one of the applications of rcs , a ( t , m , n)-group oriented ss scheme is proposed to cope with the attack in basic ( t , n)-sss , in which once m ( m ≥ t ) participants form a tightly couple group by generating rcs , the secret can be recovered only if all m rcs are correct , which requires each participant to have a valid share in advance . moreover , the scheme can secure the secret without any user authentication or share verification . analyses show the proposed ( t , m , n)-group oriented ss is asymptotically perfect and unconditionally secure . rcs can also be applied to build other schemes in a simple way , such as multi - ss , group authentication , and so on .
RANK 4
an abuse - free fair contract - signing protocol based on the rsa signature a fair contract signing protocol allows two potentially mistrusted parities to exchange their commitments ( i.e. , digital signatures ) to an agreed contract over the internet in a fair way , so that either each of them obtains the other 's signature , or neither party does . based on the rsa signature scheme , a new digital contract signing protocol is proposed in this paper . like the existing rsa - based solutions for the same problem , our protocol is not only fair , but also optimistic , since the third trusted party is involved only in the situations where one party is cheating or the communication channel is interrupted . furthermore , the proposed protocol satisfies a new property , i.e. , it is abuse - free . that is , if the protocol is executed unsuccessfully , none of the two parties can show the validity of intermediate results to others . technical details are provided to analyze the security and performance of the proposed protocol . in summary , we present the first abuse - free fair contract signing protocol based on the rsa signature , and show that it is both secure and efficient .
TOP 20
RANK = 1; score = 0.18951612903225806; correct = True; id = a14a53de3a44ee327a94a0298cc1aa60057b393a
an efficient real - time broadcast authentication scheme for command and control messages broadcast ( multicast ) authentication is crucial for large and distributed systems , such as cyber - physical infrastructures ( e.g. , power - grid / smart - grid ) and wireless networks ( e.g. , intervehicle networks , military ad hoc networks ) . these time - critical systems require real - time authentication of command and control messages in a highly efficient , secure , and scalable manner . however , existing solutions are either computationally costly ( e.g. , asymmetric cryptography ) or unscalable / impractical ( e.g. , symmetric cryptography , one - time signatures , delayed key disclosure methods ) . in this paper , we develop a new broadcast authentication scheme that we call rapid authentication ( ra ) , which is suitable for time - critical authentication of command and control messages in large and distributed systems . we exploit the semistructured nature of command and control messages to construct special digital signatures , which are computationally efficient both at the signer and verifier sides . we show that ra achieves several desirable properties that are not available in existing alternatives simultaneously : 1 ) fast signature generation and verification ; 2 ) immediate verification ; 3 ) constant size public key ; 4 ) compact authenticating tag ; 5 ) packet loss tolerance ; 6 ) being free from time synchronization requirement ; and 7 ) provable security .
RANK = 2; score = 0.1889763779527559; correct = False; id = 45c28445607e56b3e848bb8f4aa84e012e6d685e
noninteractive self - certification for long - lived mobile ad hoc networks mobile ad hoc networks ( manets ) have many well - known applications in military settings as well as in emergency and rescue operations . however , a lack of infrastructure and centralized control make manets inherently insecure , and therefore specialized security services are needed for their deployment . self - certification is an essential and fundamental security service in manets . it is needed to securely cope with dynamic membership and topology , and to bootstrap other important security primitives and services ( such as secure routing and group key management ) without the assistance of any centralized trusted authority . an ideal protocol must involve minimal interaction among the manet nodes , since connectivity can be unstable . also , since manets are often composed of weak or resource - limited devices , a self - certification protocol must be efficient in terms of computation and communication . in this paper , we propose a power - aware and fully noninteractive self - certification protocol based on bivariate polynomial secret sharing and a noninteractive threshold signature scheme . in contrast with prior work , our techniques do not require any interaction and do not involve any costly reliable broadcast communication among manet nodes . we thoroughly analyze our proposal and show that it compares favorably to previous mechanisms .
RANK = 3; score = 0.18532818532818532; correct = False; id = 1bfd8176e0e44c953232cc6f0b357fd24670281d
randomized component and its application to ( < inline - formula > < tex - math notation="latex">$t$ < /tex - math></inline - formula>,<inline - formula > < tex - math notation="latex">$m$ < /tex - math></inline - formula>,<inline - formula > < tex - math notation="latex">$n$ < /tex - math></inline - formula>)-group oriented secret s a basic ( t , n)-secret sharing ( ss ) scheme allows a secret s to be divided into n shares and shared among n shareholders . in the scheme , any t or more than t shareholders can recover the secret while fewer than t shareholders can not obtain the secret s. but an adversary without any valid share may obtain the secret if there are over t participants in the secret reconstruction . to address this type of attack , we first introduce the notion of randomized component ( rc ) , which binds a share with all participants and protects the share from being exposed to outside without any computational assumption ; at the same time , rcs can be used to reconstruct the secret . as one of the applications of rcs , a ( t , m , n)-group oriented ss scheme is proposed to cope with the attack in basic ( t , n)-sss , in which once m ( m ≥ t ) participants form a tightly couple group by generating rcs , the secret can be recovered only if all m rcs are correct , which requires each participant to have a valid share in advance . moreover , the scheme can secure the secret without any user authentication or share verification . analyses show the proposed ( t , m , n)-group oriented ss is asymptotically perfect and unconditionally secure . rcs can also be applied to build other schemes in a simple way , such as multi - ss , group authentication , and so on .
RANK = 4; score = 0.18340611353711792; correct = False; id = 10801329521d45e56a4bb6681201759bf38d2a23
an abuse - free fair contract - signing protocol based on the rsa signature a fair contract signing protocol allows two potentially mistrusted parities to exchange their commitments ( i.e. , digital signatures ) to an agreed contract over the internet in a fair way , so that either each of them obtains the other 's signature , or neither party does . based on the rsa signature scheme , a new digital contract signing protocol is proposed in this paper . like the existing rsa - based solutions for the same problem , our protocol is not only fair , but also optimistic , since the third trusted party is involved only in the situations where one party is cheating or the communication channel is interrupted . furthermore , the proposed protocol satisfies a new property , i.e. , it is abuse - free . that is , if the protocol is executed unsuccessfully , none of the two parties can show the validity of intermediate results to others . technical details are provided to analyze the security and performance of the proposed protocol . in summary , we present the first abuse - free fair contract signing protocol based on the rsa signature , and show that it is both secure and efficient .
RANK = 5; score = 0.182648401826484; correct = False; id = 4ee89af71b604090e98d543f9847210aae2b7e53
robust and secure image hashing image hash functions find extensive applications in content authentication , database search , and watermarking . this paper develops a novel algorithm for generating an image hash based on fourier transform features and controlled randomization . we formulate the robustness of image hashing as a hypothesis testing problem and evaluate the performance under various image processing operations . we show that the proposed hash function is resilient to content - preserving modifications , such as moderate geometric and filtering distortions . we introduce a general framework to study and evaluate the security of image hashing systems . under this new framework , we model the hash values as random variables and quantify its uncertainty in terms of differential entropy . using this security framework , we analyze the security of the proposed schemes and several existing representative methods for image hashing . we then examine the security versus robustness tradeoff and show that the proposed hashing methods can provide excellent security and robustness .
RANK = 6; score = 0.18218623481781376; correct = False; id = 1f045885112a48bca602e095856f250663b4be55
software puzzle : a countermeasure to resource - inflated denial - of - service attacks denial - of - service ( dos ) and distributed dos ( ddos ) are among the major threats to cyber - security , and client puzzle , which demands a client to perform computationally expensive operations before being granted services from a server , is a well - known countermeasure to them . however , an attacker can inflate its capability of dos / ddos attacks with fast puzzle - solving software and/or built - in graphics processing unit ( gpu ) hardware to significantly weaken the effectiveness of client puzzles . in this paper , we study how to prevent dos / ddos attackers from inflating their puzzle - solving capabilities . to this end , we introduce a new client puzzle referred to as software puzzle . unlike the existing client puzzle schemes , which publish their puzzle algorithms in advance , a puzzle algorithm in the present software puzzle scheme is randomly generated only after a client request is received at the server side and the algorithm is generated such that : 1 ) an attacker is unable to prepare an implementation to solve the puzzle in advance and 2 ) the attacker needs considerable effort in translating a central processing unit puzzle software to its functionally equivalent gpu version such that the translation can not be done in real time . moreover , we show how to implement software puzzle in the generic server - browser model .
RANK = 7; score = 0.1814814814814815; correct = False; id = 75b8096ac9fd952a043db8c777f9e29c5e2433f1
synchronized aggregate signatures : new definitions , constructions and applications an aggregate signature scheme is a digital signature scheme where anyone given n signatures on n messages from n users can aggregate all these signatures into a single short signature . unfortunately , no " fully non - interactive " aggregate signature schemes are known outside of the random oracle heuristic ; that is , signers must pass messages between themselves , sequentially or otherwise , to generate the signature . interaction is too costly for some interesting applications . in this work , we consider the task of realizing aggregate signatures in the model of gentry and ramzan ( pkc 2006 ) when all signers share a synchronized clock , but do not need to be aware of or interactive with one another . each signer may issue at most one signature per time period and signatures aggregate only if they were created during the same time period . we call this synchronized aggregation . we present a surprisingly efficient synchronized aggregate signature scheme secure under the computational diffie - hellman assumption in the standard model . our construction is based on the stateful signatures of hohenberger and waters ( eurocrypt 2009 ) . those signatures do not aggregate since each signature includes unique randomness for a chameleon hash and those random values do not compress . to overcome this challenge , we remove the chameleon hash from their scheme and find an alternative method for moving from weak to full security that enables aggregation . we conclude by discussing applications of this construction to sensor networks and software authentication .
RANK = 8; score = 0.17928286852589642; correct = False; id = 57cfb1dc2a5f12715129fa6153c25e214cf00b06
securely combining public - key cryptosystems it is a maxim of sound computer - security practice that a cryptographic key should have only a single use . for example , an rsa key pair should be used only for public - key encryption or only for digital signatures , and not for both . in this paper we show that in many cases , the simultaneous use of related keys for two cryptosystems , e.g. for a public - key encryption system and for a public - key signature system , does not compromise their security . we demonstrate this for a variety of public - key encryption schemes that are secure against chosen - ciphertext attacks , and for a variety of digital signature schemes that are secure against forgery under chosen - message attacks . the precise form of the statement of security that we are able to prove depends on the particular cryptographic schemes in question and on the cryptographic assumptions needed for their proofs of security ; but in every case , our proof of security does not require any additional cryptographic assumptions . among the cryptosystems that we analyze in this manner are the public - key encryption schemes of cramer and shoup , naor and yung , and dolev , dwork , and naor , which are all defined in them standard model , while in the random - oracle model we analyze plaintext - aware encryption schemes ( as defined by bellare and rogaway ) and in particular the oaep+ cryptosystem . among public - key signature schemes , we analyze those of cramer and shoup and of gennaro , halevi , and rabin in the standard model , while in the random - oracle model we analyze the rsa pss scheme as well as variants of the el gamal and schnorr schemes . ( see references within . )
RANK = 9; score = 0.1762295081967213; correct = False; id = f6212d5f7603038e0536bf3f3dc489fdc3809dad
revocable and scalable certificateless remote authentication protocol with anonymity for wireless body area networks to ensure the security and privacy of the patient 's health status in the wireless body area networks ( wbans ) , it is critical to secure the extra - body communication between the smart portable device held by the wban client and the application providers , such as the hospital , physician or medical staff . based on certificateless cryptography , this paper proposes a remote authentication protocol featured with nonrepudiation , client anonymity , key escrow resistance , and revocability for extra - body communication in the wbans . first , we present a certificateless encryption scheme and a certificateless signature scheme with efficient revocation against short - term key exposure , which we believe are of independent interest . then , a certificateless anonymous remote authentication with revocation is constructed by incorporating the proposed encryption scheme and signature scheme . our revocation mechanism is highly scalable , which is especially suitable for the large - scale wbans , in the sense that the key - update overhead on the side of trusted party increased logarithmically in the number of users . as far as we know , this is the first time considering the revocation functionality of anonymous remote authentication for the wbans . both theoretic analysis and experimental simulations show that the proposed authentication protocol is provably secure in the random oracle model and highly practical .
RANK = 10; score = 0.17479674796747968; correct = False; id = 2cf87d750412600b79c038e033f28227235c9477
an efficient identity - based conditional privacy - preserving authentication scheme for vehicular ad hoc networks by broadcasting messages about traffic status to vehicles wirelessly , a vehicular ad hoc network ( vanet ) can improve traffic safety and efficiency . to guarantee secure communication in vanets , security and privacy issues must be addressed before their deployment . the conditional privacy - preserving authentication ( cppa ) scheme is suitable for solving security and privacy - preserving problems in vanets , because it supports both mutual authentication and privacy protection simultaneously . many identity - based cppa schemes for vanets using bilinear pairings have been proposed over the last few years to enhance security or to improve performance . however , it is well known that the bilinear pairing operation is one of the most complex operations in modern cryptography . to achieve better performance and reduce computational complexity of information processing in vanet , the design of a cppa scheme for the vanet environment that does not use bilinear paring becomes a challenge . to address this challenge , we propose a cppa scheme for vanets that does not use bilinear paring and we demonstrate that it could supports both the mutual authentication and the privacy protection simultaneously . our proposed cppa scheme retains most of the benefits obtained with the previously proposed cppa schemes . moreover , the proposed cppa scheme yields a better performance in terms of computation cost and communication cost making it be suitable for use by the vanet safety - related applications .
RANK = 11; score = 0.1736842105263158; correct = False; id = 31762a49721cccf2a73dfc1aa1cdba71e7bc2f55
signature schemes based on the strong rsa assumption we describe and analyze a new digital signature scheme . the new scheme is quite efficient , does not require the the signer to maintain any state , and can be proven secure against adaptive chosen message attack under a reasonable intractability assumption , the so - called strong rsa assumption . moreover , a hash function can be incorporated into the scheme in such a way that it is also secure in the random oracle model under the standard rsa assumption .
RANK = 12; score = 0.172; correct = False; id = d0d61f34db1ead5859deb3f945f351a7e125a17e
white - box traceable ciphertext - policy attribute - based encryption supporting any monotone access structures in a ciphertext - policy attribute - based encryption ( cp - abe ) system , decryption keys are defined over attributes shared by multiple users . given a decryption key , it may not be always possible to trace to the original key owner . as a decryption privilege could be possessed by multiple users who own the same set of attributes , malicious users might be tempted to leak their decryption privileges to some third parties , for financial gain as an example , without the risk of being caught . this problem severely limits the applications of cp - abe . several traceable cp - abe ( t - cp - abe ) systems have been proposed to address this problem , but the expressiveness of policies in those systems is limited where only and gate with wildcard is currently supported . in this paper we propose a new t - cp - abe system that supports policies expressed in any monotone access structures . also , the proposed system is as efficient and secure as one of the best ( non - traceable ) cp - abe systems currently available , that is , this work adds traceability to an existing expressive , efficient , and secure cp - abe scheme without weakening its security or setting any particular trade - off on its performance .
RANK = 13; score = 0.17180616740088106; correct = False; id = a62e696efb0db5b5e43d5469f7a8c5a0868565fc
mimicry attacks against wireless link signature and new defense using time - synched link signature wireless link signature is a physical layer authentication mechanism , using the multipath effect between a transmitter and a receiver to provide authentication of wireless signals . this paper identifies a new attack , called mimicry attack , against the existing wireless link signature schemes . an attacker can forge a legitimate transmitter 's link signature as long as it knows the legitimate signal at the receiver 's location , and the attacker does not have to be at exactly the same location as the legitimate transmitter . we also extend the mimicry attack to multiple - input multiple - output ( mimo ) systems , and conclude that the mimicry attack is feasible only when the number of attacker ' antennas is equal to or larger than that of the receiver 's antennas . to defend against the mimicry attack , this paper proposes a novel construction for wireless link signature , called time - synched link signature , by integrating cryptographic protection and time factor into wireless physical layer features . experimental results confirm that the mimicry attack is a real threat and the newly proposed time - synched link signatures are effective in physical layer authentication .
RANK = 14; score = 0.17131474103585656; correct = False; id = cd391ccbe4ca8d5fb44436bdd90ea05e5e69b359
providing witness anonymity under peer - to - peer settings in this paper , we introduce the concept of witness anonymity for peer - to - peer systems , as well as other systems with the peer - to - peer nature . witness anonymity combines the seemingly conflicting requirements of anonymity ( for honest peers who report on the misbehavior of other peers ) and accountability ( for malicious peers that attempt to misuse the anonymity feature to slander honest peers ) . we propose the secure deep throat ( sdt ) protocol to provide anonymity for the witnesses of malicious or selfish behavior to enable such peers to report on this behavior without fear of retaliation . on the other hand , in sdt , the misuse of anonymity is restrained in such a way that any malicious peer attempting to send multiple claims against the same innocent peer for the same reason ( i.e. , the same misbehavior type ) can be identified . we also describe how sdt can be used in two modes . the active mode can be used in scenarios with real - time requirements , e.g. , detecting and preventing the propagation of peer - to - peer worms , whereas the passive mode is suitable for scenarios without strict real - time requirements , e.g. , query - based reputation systems . we analyze the security and overhead of sdt , and present countermeasures that can be used to mitigate various attacks on the protocol . moreover , we show how sdt can be easily integrated with existing protocols / mechanisms with a few examples . our analysis shows that the communication , storage , and computation overheads of sdt are acceptable in peer - to - peer systems .
RANK = 15; score = 0.17100371747211895; correct = False; id = 5e7d19d64c509efd6e478ffeef144efec0de9416
memory safety for low - level software / hardware interactions systems that enforce memory safety for today ’s operating system kernels and other system software do not account for the behavior of low - level software / hardware interactions such as memory - mapped i / o , mmu configuration , and context switching . bugs in such low - level interactions can lead to violations of the memory safety guarantees provided by a safe execution environment and can lead to exploitable vulnerabilities in system software . in this work , we present a set of program analysis and run - time instrumentation techniques that ensure that errors in these low - level operations do not violate the assumptions made by a safety checking system . our design introduces a small set of abstractions and interfaces for manipulating processor state , kernel stacks , memory mapped i / o objects , mmu mappings , and self modifying code to achieve this goal , without moving resource allocation and management decisions out of the kernel . we have added these techniques to a compiler - based virtual machine called secure virtual architecture ( sva ) , to which the standard linux kernel has been ported previously . our design changes to sva required only an additional 100 lines of code to be changed in this kernel . our experimental results show that our techniques prevent reported memory safety violations due to low - level linux operations and that these violations are not prevented by sva without our techniques . moreover , the new techniques in this paper introduce very little overhead over and above the existing overheads of sva . taken together , these results indicate that it is clearly worthwhile to add these techniques to an existing memory safety system .
RANK = 16; score = 0.17040358744394618; correct = False; id = 6535fb1263cd1930c49bab574bd0666d0f08ac35
new blind signatures equivalent to factorization ( extended abstract ) in this paper , we present new blind signature schemes based on the factorization problem . they are the first blind signature schemes proved secure relatively to factorization . by security , we mean that no “ one - more forgery ” is possible even under a parallel attack . in other terms , a user that receives k electronic coins can not manufacture k + 1 . those security definitions have been introduced by pointcheval and stern [ 18 ] for use in electronic cash . in fact , blind signatures were defined with this aim and it is still their most important application , together with anonymous voting . in the following , we will present an efficient reduction of an attack to a factorization algorithm in the random oracle model [ 1 ] .
RANK = 17; score = 0.1703056768558952; correct = False; id = 3b253a9d2ff59a1b5b3aacdf4b724c847f170911
a domain - specific language for low - level secure multiparty computation protocols sharemind is an efficient framework for secure multiparty computations ( smc ) . its efficiency is in part achieved through a large set of primitive , optimized smc protocols that it makes available to applications built on its top . the size of this set has brought with it an issue not present in frameworks with a small number of supported operations : the set of protocols must be maintained , as new protocols are still added to it and possible optimizations for a particular sub - protocol should be propagated into larger protocols working with data of different types . to ease the maintenance of existing and implementation of new protocols , we have devised a domain - specific language ( dsl ) and its optimizing compiler for specifying protocols for secure computation . in this paper , we give the rationale of the design , describe the translation steps , the location of the compiler in the whole sharemind protocol stack , and the results obtained with this system .
RANK = 18; score = 0.16996047430830039; correct = False; id = 54b6ca3c289a8dcf7f7bbabea088b75cc0de4d69
integrity ( i ) codes : message integrity protection and authentication over insecure channels inspired by unidirectional error detecting codes that are used in situations where only one kind of bit errors are possible ( e.g. , it is possible to change a bit " 0 " into a bit " 1 " , but not the contrary ) , we propose integrity codes ( i - codes ) for a radio communication channel , which enable integrity protection of messages exchanged between entities that do not hold any mutual authentication material ( i.e. public keys or shared secret keys ) . the construction of i - codes enables a sender to encode any message such that if its integrity is violated in transmission over a radio channel , the receiver is able to detect it . in order to achieve this , we rely on the physical properties of the radio channel . we analyze in detail the use of i - codes on a radio communication channel and we present their implementation on a mica2 wireless sensor platform as a " proof of concept " . we finally introduce a novel concept called " authentication through presence " that can be used for several applications , including for key establishment and for broadcast authentication over an insecure radio channel . we perform a detailed analysis of the security of our coding scheme and we show that it is secure with respect to a realistic attacker model
RANK = 19; score = 0.16981132075471697; correct = False; id = c880bf3fae6e4cbe678bed94e7650e6015123e90
attribute - based encryption with efficient verifiable outsourced decryption attribute - based encryption ( abe ) with outsourced decryption not only enables fine - grained sharing of encrypted data , but also overcomes the efficiency drawback ( in terms of ciphertext size and decryption cost ) of the standard abe schemes . in particular , an abe scheme with outsourced decryption allows a third party ( e.g. , a cloud server ) to transform an abe ciphertext into a ( short ) el gamal - type ciphertext using a public transformation key provided by a user so that the latter can be decrypted much more efficiently than the former by the user . however , a shortcoming of the original outsourced abe scheme is that the correctness of the cloud server 's transformation can not be verified by the user . that is , an end user could be cheated into accepting a wrong or maliciously transformed output . in this paper , we first formalize a security model of abe with verifiable outsourced decryption by introducing a verification key in the output of the encryption algorithm . then , we present an approach to convert any abe scheme with outsourced decryption into an abe scheme with verifiable outsourced decryption . the new approach is simple , general , and almost optimal . compared with the original outsourced abe , our verifiable outsourced abe neither increases the user 's and the cloud server 's computation costs except some nondominant operations ( e.g. , hash computations ) , nor expands the ciphertext size except adding a hash value ( which is < ; 20 byte for 80-bit security level ) . we show a concrete construction based on green et al . 's ciphertext - policy abe scheme with outsourced decryption , and provide a detailed performance evaluation to demonstrate the advantages of our approach .
RANK = 20; score = 0.16964285714285715; correct = False; id = 8b15793f9307b620d7fc85577f88435ad41db2c5
online learning for interactive statistical machine translation state - of - the - art machine translation ( mt ) systems are still far from being perfect . an alternative is the so - called interactive machine translation ( imt ) framework . in this framework , the knowledge of a human translator is combined with a mt system . the vast majority of the existing work on imt makes use of the well - known batch learning paradigm . in the batch learning paradigm , the training of the imt system and the interactive translation process are carried out in separate stages . this paradigm is not able to take advantage of the new knowledge produced by the user of the imt system . in this paper , we present an application of the online learning paradigm to the imt framework . in the online learning paradigm , the training and prediction stages are no longer separated . this feature is particularly useful in imt since it allows the user feedback to be taken into account . the online learning techniques proposed here incrementally update the statistical models involved in the translation process . empirical results show the great potential of online learning in the imt framework .

RANKING 2217
QUERY
an empirical study of adequate vision span for attention - based neural machine translation recently , the attention mechanism plays a key role to achieve high performance for neural machine translation models . however , as it computes a score function for the encoder states in all positions at each decoding step , the attention model greatly increases the computational complexity . in this paper , we investigate the adequate vision span of attention models in the context of machine translation , by proposing a novel attention framework that is capable of reducing redundant score computation dynamically . the term “ vision span ” means a window of the encoder states considered by the attention model in one step . in our experiments , we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on englishjapanese and german - english translation tasks .
First cited at 23
TOP CITED PAPERS
RANK 23
effective approaches to attention - based neural machine translation an attentional mechanism has lately been used to improve neural machine translation ( nmt ) by selectively focusing on parts of the source sentence during translation . however , there has been little work exploring useful architectures for attention - based nmt . this paper examines two simple and effective classes of attentional mechanism : a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . we demonstrate the effectiveness of both approaches on the wmt translation tasks between english and german in both directions . with local attention , we achieve a significant gain of 5.0 bleu points over non - attentional systems that already incorporate known techniques such as dropout . our ensemble model using different attention architectures yields a new state - of - the - art result in the wmt’15 english to german translation task with 25.9 bleu points , an improvement of 1.0 bleu points over the existing best system backed by nmt and an n - gram reranker.1
RANK 173
a character - level decoder without explicit segmentation for neural machine translation the existing machine translation systems , whether phrase - based or neural , have relied almost exclusively on word - level modelling with explicit segmentation . in this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ? to answer this question , we evaluate an attention - based encoder– decoder with a subword - level encoder and a character - level decoder on four language pairs – en - cs , en - de , en - ru and en - fi– using the parallel corpora from wmt’15 . our experiments show that the models with a character - level decoder outperform the ones with a subword - level decoder on all of the four language pairs . furthermore , the ensembles of neural models with a character - level decoder outperform the state - of - the - art non - neural machine translation systems on en - cs , en - de and en - fi and perform comparably on en - ru .
RANK 619
addressing the rare word problem in neural machine translation neural machine translation ( nmt ) has recently attracted a l ot of attention due to the very high performance achieved by deep neural network s in other domains . an inherent weakness in existing nmt systems is their inabil ity to correctly translate rare words : end - to - end nmts tend to have relatively sma ll vocabularies with a single “ unknown - word ” symbol representing every possibl e out - of - vocabulary ( oov ) word . in this paper , we propose and implement a simple t echnique to address this problem . we train an nmt system on data that is augm ented by the output of a word alignment algorithm , allowing the nmt syste m to output , for each oov word in the target sentence , its corresponding word in the source sentence . this information is later utilized in a post - process ing step that translates every oov word using a dictionary . our experiments on the wmt ’14 english to french translation task show that this simple method prov ides a substantial improvement over an equivalent nmt system that does not use thi technique . the performance of our system achieves a bleu score of 36.9 , whic h improves the previous best end - to - end nmt by 2.1 points . our model matche s t performance of the state - of - the - art system while using three times less data .
TOP UNCITED PAPERS
RANK 1
a discriminative training procedure for continuous translation models continuous - space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems . a simple , yet effective way to integrate such models in inference is to use them in an n -best rescoring step . in this paper , we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters , using an appropriate objective function . our approach is validated on two domains , where it outperforms strong baselines .
RANK 2
coverage embedding models for neural machine translation in this paper , we enhance the attention - based neural machine translation ( nmt ) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in nmt . for each source word , our model starts with a full coverage embedding vector to track the coverage status , and then keeps updating it with neural networks as the translation goes . experiments on the large - scale chinese - to - english task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system .
RANK 3
multi - task learning for multiple language translation in this paper , we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages . our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem . we extend the neural machine translation to a multi - task learning framework which shares source language representation and separates the modeling of different target language translation . our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available . experiments show that our multi - task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available .
TOP 20
RANK = 1; score = 0.23741007194244604; correct = False; id = 1182139c6120874b153bc9a24f3ca68b482c0ee9
a discriminative training procedure for continuous translation models continuous - space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems . a simple , yet effective way to integrate such models in inference is to use them in an n -best rescoring step . in this paper , we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters , using an appropriate objective function . our approach is validated on two domains , where it outperforms strong baselines .
RANK = 2; score = 0.23484848484848486; correct = False; id = a9b741b6a6159866d60b7937ef245dd089db036c
coverage embedding models for neural machine translation in this paper , we enhance the attention - based neural machine translation ( nmt ) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in nmt . for each source word , our model starts with a full coverage embedding vector to track the coverage status , and then keeps updating it with neural networks as the translation goes . experiments on the large - scale chinese - to - english task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system .
RANK = 3; score = 0.2318840579710145; correct = False; id = 83cf4b2f39bcc802b09fd59b69e23068447b26b7
multi - task learning for multiple language translation in this paper , we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages . our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem . we extend the neural machine translation to a multi - task learning framework which shares source language representation and separates the modeling of different target language translation . our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available . experiments show that our multi - task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available .
RANK = 4; score = 0.23129251700680273; correct = False; id = a2d1c378c6979339bf308771484225dbab4a6905
a graph - based bilingual corpus selection approach for smt in statistical machine translation , the number of sentence pairs in the bilingual corpus is very important to the quality of translation . however , when the quantity reaches some extent , enlarging the corpus has less effect on the translation quality ; whereas increasing greatly the time and space complexity to train the translation model , which hinders the development of statistical machine translation . in this paper , we propose a graph - based bilingual corpus selection approach , which makes use of the structural information of corpus to measure and update the importance of each sentence pair , and then selects a sentence pair with the highest importance each time . our experiments in a chinese - english translation task show that , selecting only 50 % of the whole corpus by the graph - based selection approach as training set , we can obtain the near translation result with the one using the whole corpus .
RANK = 5; score = 0.2283464566929134; correct = False; id = c31019e3fb8a3ffacb22da5744f560294486c257
supervised attentions for neural machine translation in this paper , we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs . we simply compute the distance between the machine attentions and the “ true ” alignments , and minimize this cost in the training procedure . our experiments on large - scale chinese - to - english task show that our model improves both translation and alignment qualities significantly over the large - vocabulary neural machine translation system , and even beats a state - of - the - art traditional syntax - based system .
RANK = 6; score = 0.22580645161290322; correct = False; id = d244e8ff7eeb0773d317d623af7f468f66aeb168
reducing smt rule table with monolingual key phrase this paper presents an effective approach to discard most entries of the rule table for statistical machine translation . the rule table is filtered by monolingual key phrases , which are extracted from source text using a technique based on term extraction . experiments show that 78 % of the rule table is reduced without worsening translation performance . in most cases , our approach results in measurable improvements in bleu score .
RANK = 7; score = 0.22448979591836735; correct = False; id = d73a2dbdd0292ed03275e0741dacf6eb964ef170
the feature subspace method for smt system combination recently system combination has been shown to be an effective way to improve translation quality over single machine translation systems . in this paper , we present a simple and effective method to systematically derive an ensemble of smt systems from one baseline linear smt model for use in system combination . each system in the resulting ensemble is based on a feature set derived from the features of the baseline model ( typically a subset of it ) . we will discuss the principles to determine the feature sets for derived systems , and present in detail the system combination model used in our work . evaluation is performed on the data sets for nist 2004 and nist 2005 chinese - to - english machine translation tasks . experimental results show that our method can bring significant improvements to baseline systems with state - of - the - art performance .
RANK = 8; score = 0.224; correct = False; id = 4ac80f8a837a7e0c15ea19e42310f1423edaebcd
advancements in reordering models for statistical machine translation in this paper , we propose a novel reordering model based on sequence labeling techniques . our model converts the reordering problem into a sequence labeling problem , i.e. a tagging task . results on five chinese - english nist tasks show that our model improves the baseline system by 1.32 bleu and 1.53 ter on average . results of comparative study with other seven widely used reordering models will also be reported .
RANK = 9; score = 0.22142857142857142; correct = False; id = bc852342b3fc6a0cee0ac50fbdbeabf57daccc76
knowledge - based semantic embedding for machine translation in this paper , with the help of knowledge base , we build and formulate a semantic space to connect the source and target languages , and apply it to the sequence - to - sequence framework to propose a knowledge - based semantic embedding ( kbse ) method . in our kbse method , the source sentence is firstly mapped into a knowledge based semantic space , and the target sentence is generated using a recurrent neural network with the internal meaning preserved . experiments are conducted on two translation tasks , the electric business data and movie data , and the results show that our proposed method can achieve outstanding performance , compared with both the traditional smt methods and the existing encoder - decoder models .
RANK = 10; score = 0.22077922077922077; correct = False; id = 8e0dccbba2aa4e58b79b419a596775a6fba86a26
insertion position selection model for flexible non - terminals in dependency tree - to - tree machine translation dependency tree - to - tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs . the translation process is easy if it can be accomplished only by replacing non - terminals in translation rules with other rules . however it is sometimes necessary to adjoin translation rules . flexible non - terminals have been proposed as a promising solution for this problem . a flexible non - terminal provides several insertion position candidates for the rules to be adjoined , but it increases the computational cost of decoding . in this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions . the experimental results show the proposed model can select the appropriate insertion position with a high accuracy . it reduces the decoding time and improves the translation quality owing to reduced search space .
RANK = 11; score = 0.22077922077922077; correct = False; id = 3006a62f0de6c665e74d800608e95d5ee7ec47e7
memory - enhanced decoder for neural machine translation . we propose to enhance the rnn decoder in a neural machine translator ( nmt ) with external memory , as a natural but powerful extension to the state in the decoding rnn . this memory - enhanced rnn decoder is called memdec . at each time during decoding , memdec will read from this memory and write to this memory once , both with content - based addressing . unlike the unbounded memory in previous work(bahdanau et al . , 2014 ) to store the representation of source sentence , the memory in memdec is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step . our empirical study on chinese - english translation shows that it can improve by 4.8 bleu upon groundhog and 5.3 bleu upon on moses , yielding the best performance achieved with the same training set .
RANK = 12; score = 0.22; correct = False; id = 519f4c30a692a43ac725fe9f925326492497321a
hierarchical machine translation with discontinuous phrases we present a hierarchical statistical machine translation system which supports discontinuous constituents . it is based on synchronous linear context - free rewriting systems ( slcfrs ) , an extension to synchronous context - free grammars in which synchronized non - terminals span k ≥ 1 continuous blocks on either side of the bitext . this extension beyond contextfreeness is motivated by certain complex alignment configurations that are beyond the alignment capacity of current translation models and their relatively frequent occurrence in hand - aligned data . our experiments for translating from german to english demonstrate the feasibility of training and decoding with more expressive translation models such as slcfrs and show a modest improvement over a context - free baseline .
RANK = 13; score = 0.21830985915492956; correct = False; id = 8e361f3b03b70ab096d186de97ef15b2e42a9697
using rbmt systems to produce bilingual corpus for smt this paper proposes a method using the existing rule - based machine translation ( rbmt ) system as a black box to produce synthetic bilingual corpus , which will be used as training data for the statistical machine translation ( smt ) system . we use the existing rbmt system to translate the monolingual corpus into synthetic bilingual corpus . with the synthetic bilingual corpus , we can build an smt system even if there is no real bilingual corpus . in our experiments using bleu as a metric , the system achieves a relative improvement of 11.7 % over the best rbmt system that is used to produce the synthetic bilingual corpora . we also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora . the interpolated model achieves an absolute improvement of 0.0245 bleu score ( 13.1 % relative ) as compared with the individual model trained on the real bilingual corpus .
RANK = 14; score = 0.21739130434782608; correct = False; id = 5c5b7820405e0395dfdb362baa31dc6a2808e62e
a generalized framework for hierarchical word sequence language model language modeling is a fundamental research problem that has wide application for many nlp tasks . for estimating probabilities of natural language sentences , most research on language modeling use n - gram based approaches to factor sentence probabilities . however , the assumption under n - gram models is not robust enough to cope with the data sparseness problem , which affects the final performance of language models . at the point , hierarchical word sequence ( abbreviated as hws ) language models can be viewed as an effective alternative to normal n - gram method . in this paper , we generalize hws models into a framework , where different assumptions can be adopted to rearrange word sequences in a totally unsupervised fashion , which greatly increases the expandability of hws models . for evaluation , we compare our rearranged word sequences to conventional n - gram word sequences . both intrinsic and extrinsic experiments verify that our framework can achieve better performance , proving that our method can be considered as a better alternative for ngram language models .
RANK = 15; score = 0.21678321678321677; correct = False; id = 01e38e1824b4fc5da2801d32b185a93a08581e34
constructing information networks using one single model in this paper , we propose a new framework that unifies the output of three information extraction ( ie ) tasks entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction . this novel formulation allows different parts of the information network fully interact with each other . for example , many relations can now be considered as the resultant states of events . our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state - of - the - art end - toend event argument extraction .
RANK = 16; score = 0.21678321678321677; correct = False; id = 024091a3c0223f27d6456b1a27db18fb08d41e5a
a binarized neural network joint model for machine translation the neural network joint model ( nnjm ) , which augments the neural network language model ( nnlm ) with an m - word source context window , has achieved large gains in machine translation accuracy , but also has problems with high normalization cost when using large vocabularies . training the nnjm with noise - contrastive estimation ( nce ) , instead of standard maximum likelihood estimation ( mle ) , can reduce computation cost . in this paper , we propose an alternative to nce , the binarized nnjm ( bnnjm ) , which learns a binary classifier that takes both the context and target words as input , and can be efficiently trained using mle . we compare the bnnjm and nnjm trained by nce on various translation tasks .
RANK = 17; score = 0.21678321678321677; correct = False; id = cdd2906f29d8103632dba24484571a8a05c09076
multi - timescale long short - term memory neural network for modelling sentences and documents neural network based methods have obtained great progress on a variety of natural language processing tasks . however , it is still a challenge task to model long texts , such as sentences and documents . in this paper , we propose a multi - timescale long short - term memory ( mt - lstm ) neural network to model long texts . mtlstm partitions the hidden states of the standard lstm into several groups . each group is activated at different time periods . thus , mt - lstm can model very long documents as well as short sentences . experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task .
RANK = 18; score = 0.21666666666666667; correct = False; id = 40b336a9bc3f8418e18dc6ea74118b4e8ac02292
phrase - based transliteration with simple heuristics this paper presents modeling of transliteration as a phrase - based machine translation system . we used a popular phrasebased machine translation system for english - hindi machine transliteration . we have achieved an accuracy of 38.1 % on the test set . we used some basic rules to modulate the existing phrased - based transliteration system . our experiments show that phrase - based machine translation systems can be adopted by modulating the system to fit the transliteration problem .
RANK = 19; score = 0.2147239263803681; correct = False; id = 2ca5677714be8a0336193fd587d1d9dd57895c16
cluster - specific named entity transliteration existing named entity ( ne ) transliteration approaches often exploit a general model to transliterate nes , regardless of their origins . as a result , both a chinese name and a french name ( assuming it is already translated into chinese ) will be translated into english using the same model , which often leads to unsatisfactory performance . in this paper we propose a cluster - specific ne transliteration framework . we group name origins into a smaller number of clusters , then train transliteration and language models for each cluster under a statistical machine translation framework . given a source ne , we first select appropriate models by classifying it into the most likely cluster , then we transliterate this ne with the corresponding models . we also propose a phrasebased name transliteration model , which effectively combines context information for transliteration . our experiments showed substantial improvement on the transliteration accuracy over a state - of - the - art baseline system , significantly reducing the transliteration character error rate from 50.29 % to 12.84 % .
RANK = 20; score = 0.21379310344827587; correct = False; id = 353bba6c8507c5be196d35034339a6c86a054fa1
encouraging consistent translation choices it has long been observed that monolingual text exhibits a tendency toward “ one sense per discourse , ” and it has been argued that a related “ one translation per discourse ” constraint is operative in bilingual contexts as well . in this paper , we introduce a novel method using forced decoding to confirm the validity of this constraint , and we demonstrate that it can be exploited in order to improve machine translation quality . three ways of incorporating such a preference into a hierarchical phrase - based mt model are proposed , and the approach where all three are combined yields the greatest improvements for both arabic - english and chineseenglish translation experiments .

RANKING 683
QUERY
rotated word vector representations and their interpretability vector representation of words improves performance in various nlp tasks , but the high - dimensional word vectors are very difficult to interpret . we apply several rotation algorithms to the vector representation of words to improve the interpretability . unlike previous approaches that induce sparsity , the rotated vectors are interpretable while preserving the expressive performance of the original vectors . furthermore , any pre - built word vector representation can be rotated for improved interpretability . we apply rotation to skipgrams and glove and compare the expressive power and interpretability with the original vectors and the sparse overcomplete vectors . the results show that the rotated vectors outperform the original and the sparse overcomplete vectors for interpretability and expressiveness tasks .
First cited at 1
TOP CITED PAPERS
RANK 1
distributed representations for unsupervised semantic role labeling we present a new approach for unsupervised semantic role labeling that leverages distributed representations . we induce embeddings to represent a predicate , its arguments and their complex interdependence . argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments , while predicate embeddings are learned from argument contexts . the induced representations are clustered into roles using a linear programming formulation of hierarchical clustering , where we can model task - specific knowledge . experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models .
RANK 3
sparse overcomplete word vector representations current distributed representations of words show little resemblance to theories of lexical semantics . the former are dense and uninterpretable , the latter largely based on familiar , discrete classes ( e.g. , supersenses ) and relations ( e.g. , synonymy and hypernymy ) . we propose methods that transform word vectors into sparse ( and optionally binary ) vectors . the resulting representations are more similar to the interpretable features typically used in nlp , though they are discovered automatically from raw corpora . because the vectors are highly sparse , they are computationally easy to work with . most importantly , we find that they outperform the original vectors on benchmark tasks .
RANK 14
improving sparse word representations with distributional inference for semantic composition distributional models are derived from cooccurrences in a corpus , where only a small proportion of all possible plausible cooccurrences will be observed . this results in a very sparse vector space , requiring a mechanism for inferring missing knowledge . most methods face this challenge in ways that render the resulting word representations uninterpretable , with the consequence that semantic composition becomes hard to model . in this paper we explore an alternative which involves explicitly inferring unobserved co - occurrences using the distributional neighbourhood . we show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state - of - the - art for adjectivenoun , noun - noun and verb - object compositions while being fully interpretable .
TOP UNCITED PAPERS
RANK 2
improving the lexical function composition model with pathwise optimized elastic - net regression in this paper , we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique . we use the pathwise coordinate - descent optimized elastic - net regression method to estimate the composition parameters , and compare the resulting model with several recent alternative approaches in the task of composing simple intransitive sentences , adjective - noun phrases and determiner phrases . experimental results demonstrate that the lexical function model estimated by elastic - net regression achieves better performance , and it provides good qualitative interpretability through sparsity constraints on model parameters .
RANK 4
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK 5
topic identification and discovery on text and speech we compare the multinomial i - vector framework from the speech community with lda , sage , and lsa as feature learners for topic id on multinomial speech and text data . we also compare the learned representations in their ability to discover topics , quantified by distributional similarity to gold - standard topics and by human interpretability . we find that topic id and topic discovery are competing objectives . we argue that lsa and i - vectors should be more widely considered by the text processing community as pre - processing steps for downstream tasks , and also speculate about speech processing tasks that could benefit from more interpretable representations like sage .
TOP 20
RANK = 1; score = 0.21296296296296297; correct = True; id = 4b713bc1fa91cf5659334a526a9c580fb26e4acb
distributed representations for unsupervised semantic role labeling we present a new approach for unsupervised semantic role labeling that leverages distributed representations . we induce embeddings to represent a predicate , its arguments and their complex interdependence . argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments , while predicate embeddings are learned from argument contexts . the induced representations are clustered into roles using a linear programming formulation of hierarchical clustering , where we can model task - specific knowledge . experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models .
RANK = 2; score = 0.20869565217391303; correct = False; id = 5b7d92a289b098ce753579b8f950a1a51e263ec4
improving the lexical function composition model with pathwise optimized elastic - net regression in this paper , we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique . we use the pathwise coordinate - descent optimized elastic - net regression method to estimate the composition parameters , and compare the resulting model with several recent alternative approaches in the task of composing simple intransitive sentences , adjective - noun phrases and determiner phrases . experimental results demonstrate that the lexical function model estimated by elastic - net regression achieves better performance , and it provides good qualitative interpretability through sparsity constraints on model parameters .
RANK = 3; score = 0.20512820512820512; correct = True; id = 02806b69896db8ccd704c9529ccaa0ff82e03efc
sparse overcomplete word vector representations current distributed representations of words show little resemblance to theories of lexical semantics . the former are dense and uninterpretable , the latter largely based on familiar , discrete classes ( e.g. , supersenses ) and relations ( e.g. , synonymy and hypernymy ) . we propose methods that transform word vectors into sparse ( and optionally binary ) vectors . the resulting representations are more similar to the interpretable features typically used in nlp , though they are discovered automatically from raw corpora . because the vectors are highly sparse , they are computationally easy to work with . most importantly , we find that they outperform the original vectors on benchmark tasks .
RANK = 4; score = 0.2; correct = False; id = 6a473e9e0a2183928b2d78bddf4b3d01ff46c454
chunking with support vector machines we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .
RANK = 5; score = 0.19642857142857142; correct = False; id = 1af7e2732c63a7cc852c98bea4d22d7337931087
topic identification and discovery on text and speech we compare the multinomial i - vector framework from the speech community with lda , sage , and lsa as feature learners for topic id on multinomial speech and text data . we also compare the learned representations in their ability to discover topics , quantified by distributional similarity to gold - standard topics and by human interpretability . we find that topic id and topic discovery are competing objectives . we argue that lsa and i - vectors should be more widely considered by the text processing community as pre - processing steps for downstream tasks , and also speculate about speech processing tasks that could benefit from more interpretable representations like sage .
RANK = 6; score = 0.1958762886597938; correct = False; id = 22d185c7ba066468f9ff1df03f1910831076e943
learning better embeddings for rare words using distributional representations there are two main types of word representations : low - dimensional embeddings and high - dimensional distributional vectors , in which each dimension corresponds to a context word . in this paper , we initialize an embedding - learning model with distributional vectors . evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words .
RANK = 7; score = 0.19491525423728814; correct = False; id = cb9bb57578d6759fcbbef7e436581ea45f08c367
addressing ambiguity in unsupervised part - of - speech induction with substitute vectors we study substitute vectors to solve the part - of - speech ambiguity problem in an unsupervised setting . part - of - speech tagging is a crucial preliminary process in many natural language processing applications . because many words in natural languages have more than one part - of - speech tag , resolving part - of - speech ambiguity is an important task . we claim that partof - speech ambiguity can be solved using substitute vectors . a substitute vector is constructed with possible substitutes of a target word . this study is built on previous work which has proven that word substitutes are very fruitful for part - ofspeech induction . experiments show that our methodology works for words with high ambiguity .
RANK = 8; score = 0.1926605504587156; correct = False; id = 43984fff49ba33698837b8576a2d9ffc72f56b96
sparsifying word representations for deep unordered sentence modeling sparsity often leads to efficient and interpretable representations for data . in this paper , we introduce an architecture to infer the appropriate sparsity pattern for the word embeddings while learning the sentence composition in a deep network . the proposed approach produces competitive results in sentiment and topic classification tasks with high degree of sparsity . it is computationally cheaper to compute sparse word representations than existing approaches . the imposed sparsity is directly controlled by the task considered and leads to more interpretability .
RANK = 9; score = 0.19230769230769232; correct = False; id = 23e2addd93ef43b0638d6045cd4e9724344045c4
improving sparse word similarity models with asymmetric measures we show that asymmetric models based on tversky ( 1977 ) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle - rank words . in accord with tversky ’s discovery that asymmetric similarity judgments arise when comparing sparse and rich representations , improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing highand midfrequency words .
RANK = 10; score = 0.19230769230769232; correct = False; id = 591b22bd49c7b7947018d4e08442c6ec4cdd07cc
non - distributional word vector representations data - driven representation learning for words is a technique of central importance in nlp . while indisputably useful as a source of features in downstream tasks , such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best . we present a method for constructing interpretable word vectors from hand - crafted linguistic resources like wordnet , framenet etc . these vectors are binary ( i.e , contain only 0 and 1 ) and are 99.9 % sparse . we analyze their performance on state - of - the - art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches .
RANK = 11; score = 0.1919191919191919; correct = False; id = dc848eb77988367c69f35d4b4ff765ae4f419214
distributed word representations improve ner for e - commerce this paper presents a case study of using distributed word representations , word2vec in particular , for improving performance of named entity recognition for the ecommerce domain . we also demonstrate that distributed word representations trained on a smaller amount of in - domain data are more effective than word vectors trained on very large amount of out - of - domain data , and that their combination gives the best results .
RANK = 12; score = 0.19090909090909092; correct = False; id = 35d2767d47fab881092516a40053dd6ec389b12a
learning the curriculum with bayesian optimization for task - specific word representation learning we use bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features . the curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus . we show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .
RANK = 13; score = 0.19083969465648856; correct = False; id = 6b57643fb2c1b0a94409c2d30f76602c32583a80
prior disambiguation of word tensors for constructing sentence vectors recent work has shown that compositionaldistributional models using element - wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step . the purpose of this paper is to generalise these ideas to tensor - based models , where relational words such as verbs and adjectives are represented by linear maps ( higher order tensors ) acting on a number of arguments ( vectors ) . we propose disambiguation algorithms for a number of tensor - based models , which we then test on a variety of tasks . the results show that disambiguation can provide better compositional representation even for the case of tensor - based models . furthermore , we confirm previous findings regarding the positive effect of disambiguation on vector mixture models , and we compare the effectiveness of the two approaches .
RANK = 14; score = 0.1893939393939394; correct = True; id = b462b3739aee93ec09deca5663aad2631820b9f3
improving sparse word representations with distributional inference for semantic composition distributional models are derived from cooccurrences in a corpus , where only a small proportion of all possible plausible cooccurrences will be observed . this results in a very sparse vector space , requiring a mechanism for inferring missing knowledge . most methods face this challenge in ways that render the resulting word representations uninterpretable , with the consequence that semantic composition becomes hard to model . in this paper we explore an alternative which involves explicitly inferring unobserved co - occurrences using the distributional neighbourhood . we show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state - of - the - art for adjectivenoun , noun - noun and verb - object compositions while being fully interpretable .
RANK = 15; score = 0.18867924528301888; correct = False; id = b048984681b4b5de21896561dbcd1036065490da
syntactic dependencies and distributed word representations for analogy detection and mining distributed word representations capture relational similarities by means of vector arithmetics , giving high accuracies on analogy detection . we empirically investigate the use of syntactic dependencies on improving chinese analogy detection based on distributed word representations , showing that a dependency - based embeddings does not perform better than an ngram - based embeddings , but dependency structures can be used to improve analogy detection by filtering candidates . in addition , we show that distributed representations of dependency structure can be used for measuring relational similarities , thereby help analogy mining .
RANK = 16; score = 0.1875; correct = False; id = 61f3b2cd3b388c8307a48c56f94ea63b12f73b5d
word 's vector representations meet machine translation distributed vector representations of words are useful in various nlp tasks . we briefly review the cbow approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of machine translation . the primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup .
RANK = 17; score = 0.1864406779661017; correct = False; id = 367516f95e77fa17757837b8be2f76495649712b
improved semantic representation for domain - specific entities most existing corpus - based approaches to semantic representation suffer from inaccurate modeling of domain - specific lexical items which either have low frequencies or are non - existent in open - domain corpora . we put forward a technique that improves word embeddings in specific domains by first transforming a given lexical item to a sorted list of representative words and then modeling the item by combining the embeddings of these words . our experiments show that the proposed technique can significantly improve some of the recent word embedding techniques while modeling a set of lexical items in the biomedical domain , i.e. , phenotypes .
RANK = 18; score = 0.18627450980392157; correct = False; id = 30c6ce423db5ca40969b57d96ba029089946c4e6
correlation - based intrinsic evaluation of word vector representations we introduce qvec - cca — an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources . we show that qveccca scores are an effective proxy for a range of extrinsic semantic and syntactic tasks . we also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks , compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity .
RANK = 19; score = 0.18446601941747573; correct = False; id = 6805513ee9e24a9ca3542118ae74eda99d649df3
event role extraction using domain - relevant word representations the efficiency of information extraction systems is known to be heavily influenced by domain - specific knowledge but the cost of developing such systems is considerably high . in this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain - specific data and using them for representing event roles enable to outperform previous state - of - the - art event extraction models on the muc-4 data set .
RANK = 20; score = 0.18446601941747573; correct = False; id = 1b4316e3bcb4bfe87a67ad7ab46ce3ae68073c7f
optimising agile social media analysis agile social media analysis involves building bespoke , one - off classification pipelines tailored to the analysis of specific datasets . in this study we investigate how the dualist architecture can be optimised for agile social media analysis . we evaluate several semi - supervised learning algorithms in conjunction with a naı̈ve bayes model , and show how these modifications can improve the performance of bespoke classifiers for a variety of tasks on a large range of datasets .

RANKING 2143
QUERY
which is the effective way for gaokao : information retrieval or neural networks ? as one of the most important test of china , gaokao is designed to be difficult enough to distinguish the excellent high school students . in this work , we detailed the gaokao history multiple choice questions(gkhmc ) and proposed two different approaches to address them using various resources . one approach is based on entity search technique ( ir approach ) , the other is based on text entailment approach where we specifically employ deep neural networks(nn approach ) . the result of experiment on our collected real gaokao questions showed that they are good at different categories of questions , i.e. ir approach performs much better at entity questions(eqs ) while nn approach shows its advantage on sentence questions(sqs ) . our new method achieves state - of - the - art performance and show that it ’s indispensable to apply hybrid method when participating in the real - world tests .
First cited at 77
TOP CITED PAPERS
RANK 77
key - value memory networks for directly reading documents directly reading documents and being able to answer questions from them is an unsolved challenge . to avoid its inherent difficulty , question answering ( qa ) has been directed towards using knowledge bases ( kbs ) instead , which has proven effective . unfortunately kbs often suffer from being too restrictive , as the schema can not support certain types of answers , and too sparse , e.g. wikipedia contains much more information than freebase . in this work we introduce a new method , key - value memory networks , that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation . to compare using kbs , information extraction or wikipedia documents directly in a single framework we construct an analysis tool , wikimovies , a qa dataset that contains raw text alongside a preprocessed kb , in the domain of movies . our method reduces the gap between all three settings . it also achieves state - of - the - art results on the existing wikiqa benchmark .
RANK 193
on the properties of neural machine translation : encoder - decoder approaches neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks . the neural machine translation models often consist of an encoder and a decoder . the encoder extracts a fixed - length representation from a variable - length input sentence , and the decoder generates a correct translation from this representation . in this paper , we focus on analyzing the properties of the neural machine translation using two models ; rnn encoder – decoder and a newly proposed gated recursive convolutional neural network . we show that the neural machine translation performs relatively well on short sentences without unknown words , but its performance degrades rapidly as the length of the sentence and the number of unknown words increase . furthermore , we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically .
RANK 3996
mctest : a challenge dataset for the open - domain machine comprehension of text we present mctest , a freely available set of stories and associated questions intended for research on the machine comprehension of text . previous work on machine comprehension ( e.g. , semantic modeling ) has made great strides , but primarily focuses either on limited - domain datasets , or on solving a more restricted goal ( e.g. , open - domain relation extraction ) . in contrast , mctest requires machines to answer multiple - choice reading comprehension questions about fictional stories , directly tackling the high - level goal of open - domain machine comprehension . reading comprehension can test advanced abilities such as causal reasoning and understanding the world , yet , by being multiple - choice , still provide a clear metric . by being fictional , the answer typically can be found only in the story itself . the stories and questions are also carefully limited to those a young child would understand , reducing the world knowledge that is required for the task . we present the scalable crowd - sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions . by screening workers ( with grammar tests ) and stories ( with grading ) , we have ensured that the data is the same quality as another set that we manually edited , but at one tenth the editing cost . by being open - domain , yet carefully restricted , we hope mctest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text . 1 reading comprehension a major goal for nlp is for machines to be able to understand text as well as people . several research disciplines are focused on this problem : for example , information extraction , relation extraction , semantic role labeling , and recognizing textual entailment . yet these techniques are necessarily evaluated individually , rather than by how much they advance us towards the end goal . on the other hand , the goal of semantic parsing is the machine comprehension of text ( mct ) , yet its evaluation requires adherence to a specific knowledge representation , and it is currently unclear what the best representation is , for open - domain text . we believe that it is useful to directly tackle the top - level task of mct . for this , we need a way to measure progress . one common method for evaluating someone ’s understanding of text is by giving them a multiple - choice reading comprehension test . this has the advantage that it is objectively gradable ( vs. essays ) yet may test a range of abilities such as causal or counterfactual reasoning , inference among relations , or just basic understanding of the world in which the passage is set . therefore , we propose a multiple - choice reading comprehension task as a way to evaluate progress on mct . we have built a reading comprehension dataset containing 500 fictional stories , with 4 multiple choice questions per story . it was built using methods which can easily scale to at least 5000 stories , since the stories were created , and the curation was done , using crowd sourcing almost entirely , at a total of $ 4.00 per story . we plan to periodically update the dataset to ensure that methods are not overfitting to the existing data . the dataset is open - domain , yet restricted to concepts and words that a 7 year old is expected to understand . this task is still beyond the capability of today ’s computers and algorithms .
TOP UNCITED PAPERS
RANK 1
a dependency - based neural network for relation classification previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees . in this paper , we further explore how to make full use of the combination of these dependency information . we first propose a new structure , termed augmented dependency path ( adp ) , which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path . to exploit the semantic representation behind the adp structure , we develop dependency - based neural networks ( depnn ) : a recursive neural network designed to model the subtrees , and a convolutional neural network to capture the most important features on the shortest path . experiments on the semeval-2010 dataset show that our proposed method achieves state - of - art results .
RANK 2
docchat : an information retrieval approach for chatbot engines using unstructured documents most current chatbot engines are designed to reply to user utterances based on existing utterance - response ( or q - r)1 pairs . in this paper , we present docchat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of q - r pairs , to respond to utterances . a learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly . we evaluate our proposed approach in both english and chinese : ( i ) for english , we evaluate docchat on wikiqa and qasent , two answer sentence selection tasks , and compare it with state - of - the - art methods . reasonable improvements and good adaptability are observed . ( ii ) for chinese , we compare docchat with xiaoice2 , a famous chitchat engine in china , and side - by - side evaluation shows that docchat is a perfect complement for chatbot engines using q - r pairs as main source of responses .
RANK 3
statistical machine translation improves question retrieval in community question answering via matrix factorization community question answering ( cqa ) has become an increasingly popular research topic . in this paper , we focus on the problem of question retrieval . question retrieval in cqa can automatically find the most relevant and recent questions that have been solved by other users . however , the word ambiguity and word mismatch problems bring about new challenges for question retrieval in cqa . state - of - the - art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models . while useful , the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora ( e.g. , question - answer pairs ) in the absence of which they are troubled by noise issue . in this work , we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages . our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization . experiments conducted on a real cqa data show that our proposed approach is promising .
TOP 20
RANK = 1; score = 0.22641509433962265; correct = False; id = 72b88705083fda3df98e114507ad76094a0465ec
a dependency - based neural network for relation classification previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees . in this paper , we further explore how to make full use of the combination of these dependency information . we first propose a new structure , termed augmented dependency path ( adp ) , which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path . to exploit the semantic representation behind the adp structure , we develop dependency - based neural networks ( depnn ) : a recursive neural network designed to model the subtrees , and a convolutional neural network to capture the most important features on the shortest path . experiments on the semeval-2010 dataset show that our proposed method achieves state - of - art results .
RANK = 2; score = 0.2215909090909091; correct = False; id = 91e9387c92b7c9d295c1188719d30dd179cc81e8
docchat : an information retrieval approach for chatbot engines using unstructured documents most current chatbot engines are designed to reply to user utterances based on existing utterance - response ( or q - r)1 pairs . in this paper , we present docchat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of q - r pairs , to respond to utterances . a learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly . we evaluate our proposed approach in both english and chinese : ( i ) for english , we evaluate docchat on wikiqa and qasent , two answer sentence selection tasks , and compare it with state - of - the - art methods . reasonable improvements and good adaptability are observed . ( ii ) for chinese , we compare docchat with xiaoice2 , a famous chitchat engine in china , and side - by - side evaluation shows that docchat is a perfect complement for chatbot engines using q - r pairs as main source of responses .
RANK = 3; score = 0.21025641025641026; correct = False; id = 51d035e400979c1634bb20872a053fb3dc4bd743
statistical machine translation improves question retrieval in community question answering via matrix factorization community question answering ( cqa ) has become an increasingly popular research topic . in this paper , we focus on the problem of question retrieval . question retrieval in cqa can automatically find the most relevant and recent questions that have been solved by other users . however , the word ambiguity and word mismatch problems bring about new challenges for question retrieval in cqa . state - of - the - art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models . while useful , the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora ( e.g. , question - answer pairs ) in the absence of which they are troubled by noise issue . in this work , we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages . our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization . experiments conducted on a real cqa data show that our proposed approach is promising .
RANK = 4; score = 0.20987654320987653; correct = False; id = 93620abb165927f517200e69b5806fa39b19dc34
dkpro wsd : a generalized uima - based framework for word sense disambiguation implementations of word sense disambiguation ( wsd ) algorithms tend to be tied to a particular test corpus format and sense inventory . this makes it difficult to test their performance on new data sets , or to compare them against past algorithms implemented for different data sets . in this paper we present dkpro wsd , a freely licensed , general - purpose framework for wsd which is both modular and extensible . dkpro wsd abstracts the wsd process in such a way that test corpora , sense inventories , and algorithms can be freely swapped . its uima - based architecture makes it easy to add support for new resources and algorithms . related tasks such as word sense induction and entity linking are also supported .
RANK = 5; score = 0.20987654320987653; correct = False; id = 8746bedca9067c703c29d5dce3563eda443faa18
learning to recognize ancillary information for automatic paraphrase identification previous work on automatic paraphrase identification ( pi ) is mainly based on modeling text similarity between two sentences . in contrast , we study methods for automatically detecting whether a text fragment only appearing in a sentence of the evaluated sentence pair is important or ancillary information with respect to the paraphrase identification task . engineering features for this new task is rather difficult , thus , we approach the problem by representing text with syntactic structures and applying tree kernels on them . the results show that the accuracy of our automatic ancillary text classifier ( atc ) is promising , i.e. , 68.6 % , and its output can be used to improve the state of the art in pi .
RANK = 6; score = 0.20855614973262032; correct = False; id = 63cf3fd2994747f781ccd2e1e488165d7e060682
classifying relations by ranking with convolutional neural networks relation classification is an important semantic processing task for which state - ofthe - art systems still rely on costly handcrafted features . in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking ( cr - cnn ) . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes . we perform experiments using the the semeval-2010 task 8 dataset , which is designed for the task of classifying the relationship between two nominals marked in a sentence . using crcnn , we outperform the state - of - the - art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features . additionally , our experimental results show that : ( 1 ) our approach is more effective than cnn followed by a softmax classifier ; ( 2 ) omitting the representation of the artificial class other improves both precision and recall ; and ( 3 ) using only word embeddings as input features is enough to achieve state - of - the - art results if we consider only the text between the two target nominals .
RANK = 7; score = 0.2080536912751678; correct = False; id = dd60e6f912bf15a68c6a1724f8253cbf74692b45
application - driven statistical paraphrase generation paraphrase generation ( pg ) is important in plenty of nlp applications . however , the research of pg is far from enough . in this paper , we propose a novel method for statistical paraphrase generation ( spg ) , which can ( 1 ) achieve various applications based on a uniform statistical model , and ( 2 ) naturally combine multiple resources to enhance the pg performance . in our experiments , we use the proposed method to generate paraphrases for three different applications . the results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases .
RANK = 8; score = 0.20772946859903382; correct = False; id = 3b7a6291712f9796a3add49a3f65f0acaa3e1335
discriminant correlation analysis : real - time feature level fusion for multimodal biometric recognition information fusion is a key step in multimodal biometric systems . the fusion of information can occur at different levels of a recognition system , i.e. , at the feature level , matching - score level , or decision level . however , feature level fusion is believed to be more effective owing to the fact that a feature set contains richer information about the input biometric data than the matching score or the output decision of a classifier . the goal of feature fusion for recognition is to combine relevant information from two or more feature vectors into a single one with more discriminative power than any of the input feature vectors . in pattern recognition problems , we are also interested in separating the classes . in this paper , we present discriminant correlation analysis ( dca ) , a feature level fusion technique that incorporates the class associations into the correlation analysis of the feature sets . dca performs an effective feature fusion by maximizing the pairwise correlations across the two feature sets and , at the same time , eliminating the between - class correlations and restricting the correlations to be within the classes . our proposed method can be used in pattern recognition applications for fusing the features extracted from multiple modalities or combining different feature vectors extracted from a single modality . it is noteworthy that dca is the first technique that considers class structure in feature fusion . moreover , it has a very low computational complexity and it can be employed in real - time applications . multiple sets of experiments performed on various biometric databases and using different feature extraction techniques , show the effectiveness of our proposed method , which outperforms other state - of - the - art approaches .
RANK = 9; score = 0.20710059171597633; correct = False; id = 4a4060be350d9c2de6e06aebb62b500fa8731306
word alignment in english - hindi parallel corpus using recency - vector approach : some studies word alignment using recency - vector based approach has recently become popular . one major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small . this makes these algorithms worth - studying for languages where resources are scarce . in this work we studied the performance of two very popular recency - vector based approaches , proposed in ( fung and mckeown , 1994 ) and ( somers , 1998 ) , respectively , for word alignment in english - hindi parallel corpus . but performance of the above algorithms was not found to be satisfactory . however , subsequent addition of some new constraints improved the performance of the recency - vector based alignment technique significantly for the said corpus . the present paper discusses the new version of the algorithm and its performance in detail .
RANK = 10; score = 0.20689655172413793; correct = False; id = 697f3908dedd119ce93cf6087101804f92484c77
phrase - based query degradation modeling for vocabulary - independent ranked utterance retrieval this paper introduces a new approach to ranking speech utterances by a system ’s confidence that they contain a spoken word . multiple alternate pronunciations , or degradations , of a query word ’s phoneme sequence are hypothesized and incorporated into the ranking function . we consider two methods for hypothesizing these degradations , the best of which is constructed using factored phrasebased statistical machine translation . we show that this approach is able to significantly improve upon a state - of - the - art baseline technique in an evaluation on held - out speech . we evaluate our systems using three different methods for indexing the speech utterances ( using phoneme , phoneme multigram , and word recognition ) , and find that degradation modeling shows particular promise for locating out - of - vocabulary words when the underlying indexing system is constructed with standard word - based speech recognition .
RANK = 11; score = 0.20418848167539266; correct = False; id = 4324d03686dc13c0706bc08776f7b2195e7dbca4
bridge correlational neural networks for multilingual multimodal representation learning recently there has been a lot of interest in learning common representations for multiple views of data . typically , such common representations are learned using a parallel corpus between the two views ( say , 1 m images and their english captions ) . in this work , we address a real - world scenario where no direct parallel data is available between two views of interest ( say , v1 and v2 ) but parallel data is available between each of these views and a pivot view ( v3 ) . we propose a model for learning a common representation for v1 , v2 and v3 using only the parallel data available between v1v3 and v2v3 . the proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them . there are two specific downstream applications that we focus on ( i ) transfer learning between languages l1,l2, ... ,ln using a pivot language l and ( ii ) cross modal access between images and a language l1 using a pivot language l2 . our model achieves state - of - the - art performance in multilingual document classification on the publicly available multilingual ted corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work .
RANK = 12; score = 0.203125; correct = False; id = 111501ce052a6f4f323214f081b26f04528207b4
summarizing source code using a neural attention model high quality source code is often paired with high level summaries of the computation it performs , for example in code documentation or in descriptions posted in online forums . such summaries are extremely useful for applications such as code search but are expensive to manually author , hence only done for a small fraction of all code that is produced . in this paper , we present the first completely datadriven approach for generating high level summaries of source code . our model , code - nn , uses long short term memory ( lstm ) networks with attention to produce sentences that describe c # code snippets and sql queries . code - nn is trained on a new corpus that is automatically collected from stackoverflow , which we release . experiments demonstrate strong performance on two tasks : ( 1 ) code summarization , where we establish the first end - to - end learning results and outperform strong baselines , and ( 2 ) code retrieval , where our learned model improves the state of the art on a recently introduced c # benchmark by a large margin .
RANK = 13; score = 0.20297029702970298; correct = False; id = da67d999b555f2f782b1cd4a80765509215044ce
fingerprint combination for privacy protection we propose here a novel system for protecting fingerprint privacy by combining two different fingerprints into a new identity . in the enrollment , two fingerprints are captured from two different fingers . we extract the minutiae positions from one fingerprint , the orientation from the other fingerprint , and the reference points from both fingerprints . based on this extracted information and our proposed coding strategies , a combined minutiae template is generated and stored in a database . in the authentication , the system requires two query fingerprints from the same two fingers which are used in the enrollment . a two - stage fingerprint matching process is proposed for matching the two query fingerprints against a combined minutiae template . by storing the combined minutiae template , the complete minutiae feature of a single fingerprint will not be compromised when the database is stolen . furthermore , because of the similarity in topology , it is difficult for the attacker to distinguish a combined minutiae template from the original minutiae templates . with the help of an existing fingerprint reconstruction approach , we are able to convert the combined minutiae template into a real - look alike combined fingerprint . thus , a new virtual identity is created for the two different fingerprints , which can be matched using minutiae - based fingerprint matching algorithms . the experimental results show that our system can achieve a very low error rate with frr = 0.4 % at far = 0.1 % . compared with the state - of - the - art technique , our work has the advantage in creating a better new virtual identity when the two different fingerprints are randomly chosen .
RANK = 14; score = 0.20245398773006135; correct = False; id = 34cd587d1e7c871eb0cd11437629ce62cb4ffd94
opinion mining in newspaper articles by entropy - based word connections a very valuable piece of information in newspaper articles is the tonality of extracted statements . for the analysis of tonality of newspaper articles either a big human effort is needed , when it is carried out by media analysts , or an automated approach which has to be as accurate as possible for a media response analysis ( mra ) . to this end , we will compare several state - of - the - art approaches for opinion mining in newspaper articles in this paper . furthermore , we will introduce a new technique to extract entropy - based word connections which identifies the word combinations which create a tonality . in the evaluation , we use two different corpora consisting of news articles , by which we show that the new approach achieves better results than the four state - of - the - art methods .
RANK = 15; score = 0.20224719101123595; correct = False; id = 9d0c28d20de2ff4f429e95f5247e938acff4f01c
clustering sentences with density peaks for multi - document summarization multi - document summarization ( mds ) is of great value to many real world applications . many scoring models are proposed to select appropriate sentences from documents to form the summary , in which the clustering - based methods are popular . in this work , we propose a unified sentence scoring model which measures representativeness and diversity at the same time . experimental results on duc04 demonstrate that our mds method outperforms the duc04 best method and the existing clustering - based methods , and it yields close results compared to the state - of - the - art generic mds methods . advantages of the proposed mds method are two - fold : ( 1 ) the density peaks clustering algorithm is firstly adopted , which is effective and fast . ( 2 ) no external resources such as wordnet and wikipedia or complex language parsing algorithms is used , making reproduction and deployment very easy in real environment .
RANK = 16; score = 0.20224719101123595; correct = False; id = 26362bf123d93f1d2cc268b3086473ad7373119d
generating abbreviations for chinese named entities using recurrent neural network with dynamic dictionary chinese named entities occur frequently in formal and informal environments . various approaches have been formalized the problem as a sequence labelling task and utilize a character - based methodology , in which character is treated as the basic classification unit . one of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of chinese . to address this problem , we propose a novel neural network architecture to perform task . it combines recurrent neural network ( rnn ) with an architecture determining whether a given sequence of characters can be a word or not . for demonstrating the effectiveness of the proposed method , we evaluate it on chinese named entity generation and opinion target extraction tasks . experimental results show that the proposed method can achieve better performance than state - ofthe - art methods .
RANK = 17; score = 0.2; correct = False; id = 1bcd8e3b2b40434a6f0eef98d23817f56f5f7600
detecting covert channels in computer networks based on chaos theory covert channels via the widely used tcp / ip protocols have become a new challenging issue for network security . in this paper , we analyze the information hiding in tcp / ip protocols and propose a new effective method to detect the existence of hidden information in tcp initial sequence numbers ( isns ) , which is known as one of the most difficult covert channels to be detected . our method uses phase space reconstruction to create a processing space called reconstructed phase space , where a statistical model is proposed for detecting covert channels in tcp isns . based on the model , a classification algorithm is developed to identify the existence of information hidden in isns . simulation results have demonstrated that our proposed detection method outperforms the state - of - the - art technique in terms of high detection accuracy and greatly reduced computational complexity . instead of offline processing as the state - of - the - art does , our new scheme can be used for online detection .
RANK = 18; score = 0.19875776397515527; correct = False; id = 12e3151e1231537f25e38144d6350cb09bfb3586
mint : a method for effective and scalable mining of named entity transliterations from large comparable corpora in this paper , we address the problem of mining transliterations of named entities ( nes ) from large comparable corpora . we leverage the empirical fact that multilingual news articles with similar news content are rich in named entity transliteration equivalents ( netes ) . our mining algorithm , mint , uses a cross - language document similarity model to align multilingual news articles and then mines netes from the aligned articles using a transliteration similarity model . we show that our approach is highly effective on 6 different comparable corpora between english and 4 languages from 3 different language families . furthermore , it performs substantially better than a state - of - the - art competitor .
RANK = 19; score = 0.19875776397515527; correct = False; id = 01e38e1824b4fc5da2801d32b185a93a08581e34
constructing information networks using one single model in this paper , we propose a new framework that unifies the output of three information extraction ( ie ) tasks entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction . this novel formulation allows different parts of the information network fully interact with each other . for example , many relations can now be considered as the resultant states of events . our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state - of - the - art end - toend event argument extraction .
RANK = 20; score = 0.19786096256684493; correct = False; id = 2e7d3e9a705c80fa3b7f6678697ee45dc4016225
function - based question classification for general qa in contrast with the booming increase of internet data , state - of - art qa ( question answering ) systems , otherwise , concerned data from specific domains or resources such as search engine snippets , online forums and wikipedia in a somewhat isolated way . users may welcome a more general qa system for its capability to answer questions of various sources , integrated from existed specialized sub - qa engines . in this framework , question classification is the primary task . however , the current paradigms of question classification were focused on some specified type of questions , i.e. factoid questions , which are inappropriate for the general qa . in this paper , we propose a new question classification paradigm , which includes a question taxonomy suitable to the general qa and a question classifier based on mln ( markov logic network ) , where rule - based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach . experiments show that our method outperforms traditional question classification approaches .

RANKING 1481
QUERY
initializing convolutional filters with semantic features for text classification the crux of our initialization technique is n - gram selection , which assists neural networks to extract important n - gram features at the beginning of the training process . in the following tables , we illustrate those selected n - grams of different classes and datasets to understand our technique intuitively . since all of mr , sst-1 , sst-2 , cr , and mpqa are sentiment classification datasets , we only report the selected n - grams of sst-1 ( table 1 ) . n - grams selected by our method in subj and trec are shown in table 2 and table 3 .
First cited at 152
TOP CITED PAPERS
RANK 152
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK 1367
dependency sensitive convolutional neural networks for modeling sentences and documents the goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various natural language processing tasks . in this work , we present dependency sensitive convolutional neural networks ( dscnn ) as a generalpurpose classification system for both sentences and documents . dscnn hierarchically builds textual representations by processing pretrained word embeddings via long shortterm memory networks and subsequently extracting features with convolution operators . compared with existing recursive neural models with tree structures , dscnn does not rely on parsers and expensive phrase labeling , and thus is not restricted to sentencelevel tasks . moreover , unlike other cnnbased models that analyze sentences locally by sliding windows , our system captures both the dependency information within each sentence and relationships across sentences in the same document . experiment results demonstrate that our approach is achieving state - ofthe - art performance on several tasks , including sentiment analysis , question type classification , and subjectivity classification .
RANK 1504
a fast and accurate dependency parser using neural networks almost all current dependency parsers classify based on millions of sparse indicator features . not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly . in this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition - based dependency parser . because this classifier learns and uses just a small number of dense features , it can work very fast , while achieving an about 2 % improvement in unlabeled and labeled attachment scores on both english and chinese datasets . concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the english penn treebank .
TOP UNCITED PAPERS
RANK 1
classifying relations by ranking with convolutional neural networks relation classification is an important semantic processing task for which state - ofthe - art systems still rely on costly handcrafted features . in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking ( cr - cnn ) . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes . we perform experiments using the the semeval-2010 task 8 dataset , which is designed for the task of classifying the relationship between two nominals marked in a sentence . using crcnn , we outperform the state - of - the - art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features . additionally , our experimental results show that : ( 1 ) our approach is more effective than cnn followed by a softmax classifier ; ( 2 ) omitting the representation of the artificial class other improves both precision and recall ; and ( 3 ) using only word embeddings as input features is enough to achieve state - of - the - art results if we consider only the text between the two target nominals .
RANK 2
ucd : diachronic text classification with character , word , and syntactic n - grams we present our submission to semeval-2015 task 7 : diachronic text evaluation , in which we approach the task of assigning a date to a text as a multi - class classification problem . we extract n - gram features from the text at the letter , word , and syntactic level , and use these to train a classifier on date - labeled training data . we also incorporate date probabilities of syntactic features as estimated from a very large external corpus of books . our system achieved the highest performance of all systems on subtask 2 : identifying texts by specific time language use .
RANK 3
swisscheese at semeval-2016 task 4 : sentiment classification using an ensemble of convolutional neural networks with distant supervision in this paper , we propose a classifier for predicting message - level sentiments of english micro - blog messages from twitter . our method builds upon the convolutional sentence embedding approach proposed by ( severyn and moschitti , 2015a ; severyn and moschitti , 2015b ) . we leverage large amounts of data with distant supervision to train an ensemble of 2-layer convolutional neural networks whose predictions are combined using a random forest classifier . our approach was evaluated on the datasets of the semeval-2016 competition ( task 4 ) outperforming all other approaches for the message polarity classification task .
TOP 20
RANK = 1; score = 0.20261437908496732; correct = False; id = 63cf3fd2994747f781ccd2e1e488165d7e060682
classifying relations by ranking with convolutional neural networks relation classification is an important semantic processing task for which state - ofthe - art systems still rely on costly handcrafted features . in this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking ( cr - cnn ) . we propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes . we perform experiments using the the semeval-2010 task 8 dataset , which is designed for the task of classifying the relationship between two nominals marked in a sentence . using crcnn , we outperform the state - of - the - art for this dataset and achieve a f1 of 84.1 without using any costly handcrafted features . additionally , our experimental results show that : ( 1 ) our approach is more effective than cnn followed by a softmax classifier ; ( 2 ) omitting the representation of the artificial class other improves both precision and recall ; and ( 3 ) using only word embeddings as input features is enough to achieve state - of - the - art results if we consider only the text between the two target nominals .
RANK = 2; score = 0.20168067226890757; correct = False; id = f51b3c730c7bd5b91022147be6fc3fe0cfe403aa
ucd : diachronic text classification with character , word , and syntactic n - grams we present our submission to semeval-2015 task 7 : diachronic text evaluation , in which we approach the task of assigning a date to a text as a multi - class classification problem . we extract n - gram features from the text at the letter , word , and syntactic level , and use these to train a classifier on date - labeled training data . we also incorporate date probabilities of syntactic features as estimated from a very large external corpus of books . our system achieved the highest performance of all systems on subtask 2 : identifying texts by specific time language use .
RANK = 3; score = 0.20161290322580644; correct = False; id = 5aa2b42e13c9011b00397b09e5e4619263ce65ed
swisscheese at semeval-2016 task 4 : sentiment classification using an ensemble of convolutional neural networks with distant supervision in this paper , we propose a classifier for predicting message - level sentiments of english micro - blog messages from twitter . our method builds upon the convolutional sentence embedding approach proposed by ( severyn and moschitti , 2015a ; severyn and moschitti , 2015b ) . we leverage large amounts of data with distant supervision to train an ensemble of 2-layer convolutional neural networks whose predictions are combined using a random forest classifier . our approach was evaluated on the datasets of the semeval-2016 competition ( task 4 ) outperforming all other approaches for the message polarity classification task .
RANK = 4; score = 0.2; correct = False; id = a3a4c93947f7928571dd047c272df68305fb0ab5
improving wsd with multi - level view of context monitored by similarity measure the approach presented in this paper for word sense disambiguation ( wsd ) is based on a combination of different views of the context . semantic classification trees ( sct ) are employed over a short and a multi - level view of context , including rough semantic features , while a similarity measure is used in some particular cases to rely on a larger view of the context . we also describe our two - step approach based on hmm for the all - word task .
RANK = 5; score = 0.2; correct = False; id = 9170b6d3df910ed879f5b615980e09acdf3602c6
tgb at semeval-2016 task 5 : multi - lingual constraint system for aspect based sentiment analysis . this paper gives the description of the tgb system submitted to the aspect based sentiment analysis task of semeval-2016 ( task 5 ) . the system is built on linear binary classifiers for aspect category classification ( slot 1 ) , on lexicon - based detection for opinion target expressions extraction ( slot 2 ) , and on linear multi - class classifiers for sentiment polarity detection ( slot 3 ) . we conducted several different approaches for feature selection to improve classification performance on both slot 1 and slot 3 . our proposed methods are easily adaptable to all languages and domains since they are built as constrained systems which do not use any additional resources other than the provided datasets and which uses standard preprocessing methods .
RANK = 6; score = 0.1951219512195122; correct = False; id = 9827a169826e169d71df72e13ce90fadd5958f5c
an approach to improve the smoothing process based on non - uniform redistribution in the paper , an effective technique , based on the non - uniform redistribution probability for novel events ( the unknown events ) , to improve the smoothing method in language models is proposed . basically , there are two processes in the smoothing methods : 1 ) discounting and 2 ) redistributing . instead of uniform probability assignment to each unseen events used by most smoothing methods , we propose new technique to improve the redistribution process . referring to the probabilistic behavior of all seen events , the redistribution process for novel events in our method is non - uniform . the proposed technique is exploited on well - known and frequently - used good - turing smoothing method . the empirical results are demonstrated and analyzed for two n - gram models . the improvement is apparent and effective for smoothing methods , especially on higher unseen event rate .
RANK = 7; score = 0.19298245614035087; correct = False; id = 6ecab145439147393dfc71d5e6df2a8ca5273964
sls at semeval-2016 task 3 : neural - based approaches for ranking in community question answering community question answering platforms need to automatically rank answers and questions with respect to a given question . in this paper , we present the approaches for the answer selection and question retrieval tasks of semeval-2016 ( task 3 ) . we develop a bag - of - vectors approach with various vectorand text - based features , and different neural network approaches including cnns and lstms to capture the semantic similarity between questions and answers for ranking purpose . our evaluation demonstrates that our approaches significantly outperform the baselines .
RANK = 8; score = 0.19148936170212766; correct = False; id = e69b691fc78a3ca094a3df1d6a041dde006b303e
review classification using semantic features and run - time weighting we introduce a method for learning to assign suitable sentiment ratings to review articles . in our approach , reviews are transformed into collections of n - gram and semantic word class features aimed at maximizing the probability of classifying them into accurate ratings . the method involves automatically segmenting review articles into sentences and automatically estimating associations between features and sentiment ratings via machine learning techniques . at run - time , a simple weighting strategy is performed to give extra weights to features in potential evaluative sentences ( e.g. , the first , the last sentences and sentences with adverbs ) from others . experiments show that word class information alleviates data sparseness problem facing higher - level n - grams ( e.g. , bigrams and trigrams ) and that our model using both training - time n - gram and semantic features and run - time weighting mechanism outperforms a strong baseline with surface n - gram features by 2.5 % relatively .
RANK = 9; score = 0.19117647058823528; correct = False; id = 7277fdf51bb3fc38f254a5546c53caa4169653c9
domain adaptation for neural networks by parameter augmentation we propose a simple domain adaptation method for neural networks in a supervised setting . supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset , assuming that both of the datasets are labeled . recently , recurrent neural networks have been shown to be successful on a variety of nlp tasks such as caption generation ; however , the existing domain adaptation techniques are limited to ( 1 ) tune the model parameters by the target dataset after the training by the source dataset , or ( 2 ) design the network to have dual output , one for the source domain and the other for the target domain . reformulating the idea of the domain adaptation technique proposed by daumé ( 2007 ) , we propose a simple domain adaptation method , which can be applied to neural networks trained with a cross - entropy loss . on captioning datasets , we show performance improvements over other domain adaptation methods .
RANK = 10; score = 0.19090909090909092; correct = False; id = 46bbf96215b8e7e3df652b564c000d7132e6fcde
using n - gram and word network features for native language identification we report on the performance of two different feature sets in the native language identification shared task ( tetreault et al . , 2013 ) . our feature sets were inspired by existing literature on native language identification and word networks . experiments show that word networks have competitive performance against the baseline feature set , which is a promising result . we also present a discussion of feature analysis based on information gain , and an overview on the performance of different word network features in the native language identification task .
RANK = 11; score = 0.19008264462809918; correct = False; id = 5893320b326a64758005ca41869030307c1a8eec
controlled and balanced dataset for japanese lexical simplification we propose a new dataset for evaluating a japanese lexical simplification method . previous datasets have several deficiencies . all of them substitute only a single target word , and some of them extract sentences only from newswire corpus . in addition , most of these datasets do not allow ties and integrate simplification ranking from all the annotators without considering the quality . in contrast , our dataset has the following advantages : ( 1 ) it is the first controlled and balanced dataset for japanese lexical simplification with high correlation with human judgment and ( 2 ) the consistency of the simplification ranking is improved by allowing candidates to have ties and by considering the reliability of annotators .
RANK = 12; score = 0.1889763779527559; correct = False; id = bf1c5d2b47657906df421aeb19e0991825882a5f
a feature - enriched tree kernel for relation extraction tree kernel is an effective technique for relation extraction . however , the traditional syntactic tree representation is often too coarse or ambiguous to accurately capture the semantic relation information between two entities . in this paper , we propose a new tree kernel , called feature - enriched tree kernel ( ftk ) , which can enhance the traditional tree kernel by : 1 ) refining the syntactic tree representation by annotating each tree node with a set of discriminant features ; and 2 ) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration . experimental results show that our method can achieve a 5.4 % f - measure improvement over the traditional convolution tree kernel .
RANK = 13; score = 0.1885245901639344; correct = False; id = c3ae2989d1d3e235cb5fc078de04ea9cbeccfb6c
tunable domain - independent event extraction in the mira framework we describe the system of the pikb team for bionlp’09 shared task 1 , which targets tunable domain - independent event extraction . our approach is based on a three - stage classification : ( 1 ) trigger word tagging , ( 2 ) simple event extraction , and ( 3 ) complex event extraction . we use the mira framework for all three stages , which allows us to trade precision for increased recall by appropriately changing the loss function during training . we report results for three systems focusing on recall ( r = 28.88 % ) , precision ( p = 65.58 % ) , and f1-measure ( f1 = 33.57 % ) , respectively .
RANK = 14; score = 0.18487394957983194; correct = False; id = 8a3d207dcae6b9b8db652b04c89b9014e4ec276e
investigating unsupervised learning for text categorization bootstrapping we propose a generalized bootstrapping algorithm in which categories are described by relevant seed features . our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme : ( i ) using latent semantic space to obtain a generalized similarity measure between instances and features , and ( ii ) the gaussian mixture algorithm , to obtain uniform classification probabilities for unlabeled examples . the algorithm was evaluated on two text categorization tasks and obtained state - of - theart performance using only the category names as initial seeds .
RANK = 15; score = 0.18487394957983194; correct = False; id = 87e79beb7976919a76b912c94ff797126981308c
deeppurple : lexical , string and affective feature fusion for sentence - level semantic similarity estimation this paper describes our submission for the * sem shared task of semantic textual similarity . we estimate the semantic similarity between two sentences using regression models with features : 1 ) n - gram hit rates ( lexical matches ) between sentences , 2 ) lexical semantic similarity between non - matching words , 3 ) string similarity metrics , 4 ) affective content similarity and 5 ) sentence length . domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47 .
RANK = 16; score = 0.18382352941176472; correct = False; id = 1192bbdccccf7ef4a2e0254b23798f316ff0be10
lig system for word level qe task at wmt14 this paper describes our word - level qe system for wmt 2014 shared task on spanish english pair . compared to wmt 2013 , this year ’s task is different due to the lack of smt setting information and additional resources . we report how we overcome this challenge to retain most of the important features which performed well last year in our system . novel features related to the availability of multiple systems output ( new point of this year ) are also proposed and experimented along with baseline set . the system is optimized by several ways : tuning the classification threshold , combining with wmt 2013 data , and refining using feature selection strategy on our development set , before dealing with the test set for submission .
RANK = 17; score = 0.1834862385321101; correct = False; id = 5a535b64c4a178fd065354c1fab8312c8ac36de5
hhu at semeval-2016 task 1 : multiple approaches to measuring semantic textual similarity this paper describes our participation in the semeval-2016 task 1 : semantic textual similarity ( sts ) . we developed three methods for the english subtask ( sts core ) . the first method is unsupervised and uses wordnet and word2vec to measure a token - based overlap . in our second approach , we train a neural network on two features . the third method uses word2vec and lda with regression splines .
RANK = 18; score = 0.183206106870229; correct = False; id = 1bc4ad28bf871190b6c38035fd1c4231cbfed6a6
usfd at semeval-2016 task 1 : putting different state - of - the - arts into a box in this paper we describe our participation in the sts core subtask which is the determination of the monolingual semantic similarity between pair of sentences . in our participation we adapted state - ofthe - art approaches from related work applied on previous sts core subtasks and run them on the 2016 data . we investigated the performance of single methods but also the combination of them . our results show that convolutional neural networks ( cnn ) are superior to both the monolingual word alignment and the word2vec approaches . the combination of all the three methods performs slightly better than using cnn only . our results also show that the performance of our systems varies between the datasets .
RANK = 19; score = 0.18253968253968253; correct = False; id = a7656644a724f788f8c1325ae9ef38ca97a2ea67
shef - nn : translation quality estimation with neural networks we describe our systems for tasks 1 and 2 of the wmt15 shared task on quality estimation . our submissions use ( i ) a continuous space language model to extract additional features for task 1 ( shefgp , shef - svm ) , ( ii ) a continuous bagof - words model to produce word embeddings as features for task 2 ( shef - w2v ) and ( iii ) a combination of features produced by quest++ and a feature produced with word embedding models ( shefquest++ ) . our systems outperform the baseline as well as many other submissions . the results are especially encouraging for task 2 , where our best performing system ( shef - w2v ) only uses features learned in an unsupervised fashion .
RANK = 20; score = 0.18181818181818182; correct = False; id = 1eb0bfef79bf1b9e66d7d1bd7a93c673c87576e0
a framework of feature selection methods for text categorization in text categorization , feature selection ( fs ) is a strategy that aims at making text classifiers more efficient and accurate . however , when dealing with a new task , it is still difficult to quickly select a suitable one from various fs methods provided by many previous studies . in this paper , we propose a theoretic framework of fs methods based on two basic measurements : frequency measurement and ratio measurement . then six popular fs methods are in detail discussed under this framework . moreover , with the guidance of our theoretical analysis , we propose a novel method called weighed frequency and odds ( wfo ) that combines the two measurements with trained weights . the experimental results on data sets from both topic - based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features .

RANKING 1965
QUERY
measuring topic coherence through optimal word buckets measuring topic quality is essential for scoring the learned topics and their subsequent use in information retrieval and text classification . to measure quality of latent dirichlet allocation ( lda ) based topics learned from text , we propose a novel approach based on grouping of topic words into buckets ( tbuckets ) . a single large bucket signifies a single coherent theme , in turn indicating high topic coherence . tbuckets uses word embeddings of topic words and employs singular value decomposition ( svd ) and integer linear programming based optimization to create coherent word buckets . tbuckets outperforms the state - of - the - art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper .
First cited at 137
TOP CITED PAPERS
RANK 137
optimizing semantic coherence in topic models latent variable models have the potential to add value to large document collections by discovering interpretable , low - dimensional subspaces . in order for people to use such models , however , they must trust them . unfortunately , typical dimensionality reduction methods for text , such as latent dirichlet allocation , often produce low - dimensional subspaces ( topics ) that are obviously flawed to human domain experts . the contributions of this paper are threefold : ( 1 ) an analysis of the ways in which topics can be flawed ; ( 2 ) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data ; ( 3 ) a novel statistical topic model based on this metric that significantly improves topic quality in a large - scale document collection from the national institutes of health ( nih ) .
RANK 362
automatic evaluation of topic coherence this paper introduces the novel task of topic coherence evaluation , whereby a set of words , as generated by a topic model , is rated for coherence or interpretability . we apply a range of topic scoring models to the evaluation task , drawing on wordnet , wikipedia and the google search engine , and existing research on lexical similarity / relatedness . in comparison with human scores for a set of learned topics over two distinct datasets , we show a simple co - occurrence measure based on pointwise mutual information over wikipedia data is able to achieve results for the task at or nearing the level of inter - annotator correlation , and that other wikipedia - based lexical relatedness methods also achieve strong results . google produces strong , if less consistent , results , while our results over wordnet are patchy at best .
RANK 878
multi - document summarization using sentence - based topic models most of the existing multi - document summarization methods decompose the documents into sentences and work directly in the sentence space using a term - sentence matrix . however , the knowledge on the document side , i.e. the topics embedded in the documents , can help the context understanding and guide the sentence selection in the summarization procedure . in this paper , we propose a new bayesian sentence - based topic model for summarization by making use of both the term - document and term - sentence associations . an efficient variational bayesian algorithm is derived for model parameter estimation . experimental results on benchmark data sets show the effectiveness of the proposed model for the multi - document summarization task .
TOP UNCITED PAPERS
RANK 1
graph - based unsupervised learning of word similarities using heterogeneous feature types in this work , we propose a graph - based approach to computing similarities between words in an unsupervised manner , and take advantage of heterogeneous feature types in the process . the approach is based on the creation of two separate graphs , one for words and one for features of different types ( alignmentbased , orthographic , etc . ) . the graphs are connected through edges that link nodes in the feature graph to nodes in the word graph , the edge weights representing the importance of a particular feature for a particular word . high quality graphs are learned during training , and the proposed method outperforms experimental baselines .
RANK 2
sprinkling topics for weakly supervised text classification supervised text classification algorithms require a large number of documents labeled by humans , that involve a laborintensive and time consuming process . in this paper , we propose a weakly supervised algorithm in which supervision comes in the form of labeling of latent dirichlet allocation ( lda ) topics . we then use this weak supervision to “ sprinkle ” artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations . we evaluate this approach to improve performance of text classification on three real world datasets .
RANK 3
identifying sentiment words using an optimization - based model without seed words sentiment word identification ( swi ) is a basic technique in many sentiment analysis applications . most existing researches exploit seed words , and lead to low robustness . in this paper , we propose a novel optimization - based model for swi . unlike previous approaches , our model exploits the sentiment labels of documents instead of seed words . several experiments on real datasets show that weed is effective and outperforms the state - of - the - art methods with seed words .
TOP 20
RANK = 1; score = 0.232; correct = False; id = 0c2df9ca8a83a5c24aff3d9090f56de046d0fe79
graph - based unsupervised learning of word similarities using heterogeneous feature types in this work , we propose a graph - based approach to computing similarities between words in an unsupervised manner , and take advantage of heterogeneous feature types in the process . the approach is based on the creation of two separate graphs , one for words and one for features of different types ( alignmentbased , orthographic , etc . ) . the graphs are connected through edges that link nodes in the feature graph to nodes in the word graph , the edge weights representing the importance of a particular feature for a particular word . high quality graphs are learned during training , and the proposed method outperforms experimental baselines .
RANK = 2; score = 0.23076923076923078; correct = False; id = deb3359b05ccec7f83b19487c337e7a634e31d6d
sprinkling topics for weakly supervised text classification supervised text classification algorithms require a large number of documents labeled by humans , that involve a laborintensive and time consuming process . in this paper , we propose a weakly supervised algorithm in which supervision comes in the form of labeling of latent dirichlet allocation ( lda ) topics . we then use this weak supervision to “ sprinkle ” artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations . we evaluate this approach to improve performance of text classification on three real world datasets .
RANK = 3; score = 0.22950819672131148; correct = False; id = c2d4d4b5c8166204ea8b281117c6b1effab45666
identifying sentiment words using an optimization - based model without seed words sentiment word identification ( swi ) is a basic technique in many sentiment analysis applications . most existing researches exploit seed words , and lead to low robustness . in this paper , we propose a novel optimization - based model for swi . unlike previous approaches , our model exploits the sentiment labels of documents instead of seed words . several experiments on real datasets show that weed is effective and outperforms the state - of - the - art methods with seed words .
RANK = 4; score = 0.2288135593220339; correct = False; id = f8cafbc760e90ccc4e9082395b8e30d0a85b9f7b
using latent dirichlet allocation for child narrative analysis child language narratives are used for language analysis , measurement of language development , and the detection of language impairment . in this paper , we explore the use of latent dirichlet allocation ( lda ) for detecting topics from narratives , and use the topics derived from lda in two classification tasks : automatic prediction of coherence and language impairment . our experiments show lda is useful for detecting the topics that correspond to the narrative structure . we also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by lda .
RANK = 5; score = 0.22764227642276422; correct = False; id = adfd48db819ce54375d9647b32b7596a3107d57e
improving twitter sentiment analysis with topic - based mixture modeling and semi - supervised training in this paper , we present multiple approaches to improve sentiment analysis on twitter data . we first establish a state - of - the - art baseline with a rich feature set . then we build a topic - based sentiment mixture model with topic - specific data in a semi - supervised training framework . the topic information is generated through topic modeling based on an efficient implementation of latent dirichlet allocation ( lda ) . the proposed sentiment model outperforms the top system in the task of sentiment analysis in twitter in semeval-2013 in terms of averaged f scores .
RANK = 6; score = 0.22764227642276422; correct = False; id = 5bf4897a199d51d9f5048e2f53a5a9edf8bfe8b7
a novel content enriching model for microblog using news corpus in this paper , we propose a novel model for enriching the content of microblogs by exploiting external knowledge , thus improving the data sparseness problem in short text classification . we assume that microblogs share the same topics with external knowledge . we first build an optimization model to infer the topics of microblogs by employing the topic - word distribution of the external knowledge . then the content of microblogs is further enriched by relevant words from external knowledge . experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods .
RANK = 7; score = 0.22137404580152673; correct = False; id = 3a9204c4c41f46954b17a9c682a18a0924917e8e
regularizing text categorization with clusters of words regularization is a critical step in supervised learning to not only address overfitting , but also to take into account any prior knowledge we may have on the features and their dependence . in this paper , we explore stateof - the - art structured regularizers and we propose novel ones based on clusters of words from lsi topics , word2vec embeddings and graph - of - words document representation . we show that our proposed regularizers are faster than the state - of - the - art ones and still improve text classification accuracy . code and data are available online1 .
RANK = 8; score = 0.22058823529411764; correct = False; id = 52d3d9c8c07eccb23dbd2c133176c07291560da1
detecting " smart " spammers on social network : a topic model approach spammer detection on social network is a challenging problem . the rigid anti - spam rules have resulted in emergence of " smart " spammers . they resemble legitimate users who are difficult to identify . in this paper , we present a novel spammer classification approach based on latent dirichlet allocation ( lda ) , a topic model . our approach extracts both the local and the global information of topic distribution patterns , which capture the essence of spamming . tested on one benchmark dataset and one self - collected dataset , our proposed method outperforms other stateof - the - art methods in terms of averaged f1score .
RANK = 9; score = 0.21951219512195122; correct = False; id = 83da3ad6b7577eecc6b51cba2b3626009ce36068
text segmentation with lda - based fisher kernel in this paper we propose a domainindependent text segmentation method , which consists of three components . latent dirichlet allocation ( lda ) is employed to compute words semantic distribution , and we measure semantic similarity by the fisher kernel . finally global best segmentation is achieved by dynamic programming . experiments on chinese data sets with the technique show it can be effective . introducing latent semantic information , our algorithm is robust on irregular - sized segments .
RANK = 10; score = 0.21428571428571427; correct = False; id = 3f37fb43f0cb22f03e51494b38a26ed46d43dcb0
extended topic model for word dependency topic model such as latent dirichlet allocation(lda ) makes assumption that topic assignment of different words are conditionally independent . in this paper , we propose a new model extended global topic random field ( egtrf ) to model non - linear dependencies between words . specifically , we parse sentences into dependency trees and represent them as a graph , and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words . word similarity information learned from large corpus is incorporated to enhance word topic assignment . parameters are estimated efficiently by variational inference and experimental results on two datasets show egtrf achieves lower perplexity and higher log predictive probability .
RANK = 11; score = 0.21374045801526717; correct = False; id = a492a571dcc51ff35f8a2dafdb86295d15fa2210
unsupervised text segmentation using semantic relatedness graphs segmenting text into semantically coherent fragments improves readability of text and facilitates tasks like text summarization and passage retrieval . in this paper , we present a novel unsupervised algorithm for linear text segmentation ( ts ) that exploits word embeddings and a measure of semantic relatedness of short texts to construct a semantic relatedness graph of the document . semantically coherent segments are then derived from maximal cliques of the relatedness graph . the algorithm performs competitively on a standard synthetic dataset and outperforms the best - performing method on a real - world ( i.e. , non - artificial ) dataset of political manifestos .
RANK = 12; score = 0.2125984251968504; correct = False; id = 776ae7a23aa8972752496b7af29a67125eaf6409
sentiment classification using semantic features extracted from wordnet - based resources in this paper , we concentrate on the 3 of the tracks proposed in the ntcir 8 moat , concerning the classification of sentences according to their opinionatedness , relevance and polarity . we propose a method for the detection of opinions , relevance , and polarity classification , based on isr - wn ( a resource for the multidimensional analysis with relevant semantic trees of sentences using different wordnet - based information sources ) . based on the results obtained , we can conclude that the resource and methods we propose are appropriate for the task , reaching the level of state - of - the - art approaches .
RANK = 13; score = 0.2125984251968504; correct = False; id = 58e50b1ee2630fc031a51ffe37be7f7d5df4705f
intrinsic plagiarism detection using n - gram classes when it is not possible to compare the suspicious document to the source document(s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself . in this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n - gram classes . the proposed method was evaluated on three publicly available standard corpora . the obtained results are comparable to the ones obtained by the best state - of - the - art methods .
RANK = 14; score = 0.21232876712328766; correct = False; id = 52ce4358343745cfcf031daa16bb7748b1bed9b7
unsupervised concept annotation using latent dirichlet allocation and segmental methods training efficient statistical approaches for natural language understanding generally requires data with segmental semantic annotations . unfortunately , building such resources is costly . in this paper , we propose an approach that produces annotations in an unsupervised way . the first step is an implementation of latent dirichlet allocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence . this knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models ( ibm models ) to produce the final semantic annotation . the relation between automaticallyderived topics and task - dependent concepts is evaluated on a spoken dialogue task with an available reference annotation .
RANK = 15; score = 0.21212121212121213; correct = False; id = e0019670534b3ae643df601bbd0037fec3248967
topicality - based indices for essay scoring in this paper , we address the problem of quantifying the overall extent to which a testtaker ’s essay deals with the topic it is assigned ( prompt ) . we experiment with a number of models for word topicality , and a number of approaches for aggregating word - level indices into text - level ones . all models are evaluated for their ability to predict the holistic quality of essays . we show that the best texttopicality model provides a significant improvement in a state - of - art essay scoring system . we also show that the findings of the relative merits of different models generalize well across three different datasets .
RANK = 16; score = 0.2112676056338028; correct = False; id = 8a646fcbfcd4398b702b9da0980cd71eedfe8dc5
vua - background : when to use background information to perform word sense disambiguation we present in this paper our submission to task 13 of semeval2015 , which makes use of background information and external resources ( dbpedia and wikipedia ) to automatically disambiguate texts . our approach follows two routes for disambiguation : one route is proposed by a state – of – the – art wsd system , and the other one by the predominant sense information extracted in an unsupervised way from an automatically built background corpus . we reached 4th position in terms of f1-score in task number 13 of semeval2015 : “ multilingual all - words sense disambiguation and entity linking ” ( moro and navigli , 2015 ) . all the software and code created for this approach are publicly available on github1 .
RANK = 17; score = 0.2112676056338028; correct = False; id = ca2323da42107cc6d5d0e494a14cecfa0875bc0d
learning bilingual sentiment word embeddings for cross - language sentiment classification the sentiment classification performance relies on high - quality sentiment resources . however , these resources are imbalanced in different languages . cross - language sentiment classification ( clsc ) can leverage the rich resources in one language ( source language ) for sentiment classification in a resource - scarce language ( target language ) . bilingual embeddings could eliminate the semantic gap between two languages for clsc , but ignore the sentiment information of text . this paper proposes an approach to learning bilingual sentiment word embeddings ( bswe ) for english - chinese clsc . the proposed bswe incorporate sentiment information of text into bilingual embeddings . furthermore , we can learn high - quality bswe by simply employing labeled corpora and their translations , without relying on largescale parallel corpora . experiments on nlp&cc 2013 clsc dataset show that our approach outperforms the state - of - theart systems .
RANK = 18; score = 0.21; correct = False; id = 9f00812ddac1cbe30e881f32e4ce1c12d05e64b1
measuring semantic relatedness using people and wordnet in this paper , we ( 1 ) propose a new dataset for testing the degree of relatedness between pairs of words ; ( 2 ) propose a new wordnet - based measure of relatedness , and evaluate it on the new dataset .
RANK = 19; score = 0.20967741935483872; correct = False; id = a1aaa7c75464e7ebe41cfe5c5258241ca34c6414
farasa : a fast and furious segmenter for arabic in this paper , we present farasa , a fast and accurate arabic segmenter . our approach is based on svm - rank using linear kernels . we measure the performance of the segmenter in terms of accuracy and efficiency , in two nlp tasks , namely machine translation ( mt ) and information retrieval ( ir ) . farasa outperforms or is at par with the stateof - the - art arabic segmenters ( stanford and madamira ) , while being more than one order of magnitude faster .
RANK = 20; score = 0.20930232558139536; correct = False; id = de7bdbf80c80831c9df8b8f82dd002fb91bb439d
unsupervised domain tuning to improve word sense disambiguation the topic of a document can prove to be useful information for word sense disambiguation ( wsd ) since certain meanings tend to be associated with particular topics . this paper presents an lda - based approach for wsd , which is trained using any available wsd system to establish a sense per ( latent dirichlet allocation based ) topic . the technique is tested using three unsupervised and one supervised wsd algorithms within the sport and finance domains giving a performance increase each time , suggesting that the technique may be useful to improve the performance of any available wsd system .

