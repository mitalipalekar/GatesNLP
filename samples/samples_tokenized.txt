RANKING 2280
QUERY
bamboozling certificate authorities with bgp the public key infrastructure ( pki ) protects users from malicious man - in - the - middle attacks by having trusted certificate authorities ( cas ) vouch for the domain names of servers on the internet through digitally signed certificates . ironically , the mechanism cas use to issue certificates is itself vulnerable to man - in - the - middle attacks by network - level adversaries . autonomous systems ( ases ) can exploit vulnerabilities in the border gateway protocol ( bgp ) to hijack traffic destined to a victim ’s domain . in this paper , we rigorously analyze attacks that an adversary can use to obtain a bogus certificate . we perform the first real - world demonstration of bgp attacks to obtain bogus certificates from top cas in an ethical manner . to assess the vulnerability of the pki , we collect a dataset of 1.8 million certificates and find that an adversary would be capable of gaining a bogus certificate for the vast majority of domains . finally , we propose and evaluate two countermeasures to secure the pki : 1 ) cas verifying domains from multiple vantage points to make it harder to launch a successful attack , and 2 ) a bgp monitoring system for cas to detect suspicious bgp routes and delay certificate issuance to give network operators time to react to bgp attacks .
First cited at 68
TOP CITED PAPERS
RANK 68
accurate real - time identification of ip prefix hijacking we present novel and practical techniques to accurately detect ip prefix hijacking attacks in real time to facilitate mitigation . attacks may hijack victim 's address space to disrupt network services or perpetrate malicious activities such as spamming and dos attacks without disclosing identity . we propose novel ways to significantly improve the detection accuracy by combining analysis of passively collected bgp routing updates with data plane fingerprints of suspicious prefixes . the key insight is to use data plane information in the form of edge network fingerprinting to disambiguate suspect ip hijacking incidences based on routing anomaly detection . conflicts in data plane fingerprints provide much more definitive evidence of successful ip prefix hijacking . utilizing multiple real - time bgp feeds , we demonstrate the ability of our system to distinguish between legitimate routing changes and actual attacks . strong correlation with addresses that originate spam emails from a spam honeypot confirms the accuracy of our techniques .
RANK 915
scion : scalability , control , and isolation on next - generation networks we present the first internet architecture designed to provide route control , failure isolation , and explicit trust information for end - to - end communications . scion separates ases into groups of independent routing sub - planes , called trust domains , which then interconnect to form complete routes . trust domains provide natural isolation of routing failures and human misconfiguration , give endpoints strong control for both inbound and outbound traffic , provide meaningful and enforceable trust , and enable scalable routing updates with high path freshness . as a result , our architecture provides strong resilience and security properties as an intrinsic consequence of good design principles , avoiding piecemeal add - on protocols as security patches . meanwhile , scion only assumes that a few top - tier isps in the trust domain are trusted for providing reliable end - to - end communications , thus achieving a small trusted computing base . both our security analysis and evaluation results show that scion naturally prevents numerous attacks and provides a high level of resilience , scalability , control , and isolation .
RANK 1918
raptor : routing attacks on privacy in tor the tor network is a widely used system for anonymous communication . however , tor is known to be vulnerable to attackers who can observe traffic at both ends of the communication path . in this paper , we show that prior attacks are just the tip of the iceberg . we present a suite of new attacks , called raptor , that can be launched by autonomous systems ( ases ) to compromise user anonymity . first , as - level adversaries can exploit the asymmetric nature of internet routing to increase the chance of observing at least one direction of user traffic at both ends of the communication . second , as - level adversaries can exploit natural churn in internet routing to lie on the bgp paths for more users over time . third , strategic adversaries can manipulate internet routing via bgp hijacks ( to discover the users using specific tor guard nodes ) and interceptions ( to perform traffic analysis ) . we demonstrate the feasibility of raptor attacks by analyzing historical bgp data and traceroute data as well as performing real - world attacks on the live tor network , while ensuring that we do not harm real users . in addition , we outline the design of two monitoring frameworks to counter these attacks : bgp monitoring to detect control - plane attacks , and traceroute monitoring to detect data - plane anomalies . overall , our work motivates the design of anonymity systems that are aware of the dynamics of internet routing .
TOP UNCITED PAPERS
RANK 1
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK 2
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK 3
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
TOP 20
RANK = 1; score = 0.762790858745575; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 2; score = 0.7329502701759338; correct = False; id = 66a6b8b5086454d2f511089ed3c157075239eb7d
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK = 3; score = 0.7278631925582886; correct = False; id = 8c56a28b6d287c93dc401470dadb6f74f240e29c
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
RANK = 4; score = 0.7268276214599609; correct = False; id = 142947116c7baec662984feb82b18c3812bc91f9
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
RANK = 5; score = 0.7243565320968628; correct = False; id = 4b7c753bb235b9aafd234e00300c942665e8e481
enf extraction from digital recordings using adaptive techniques and frequency tracking a novel forensic tool used for assessing the authenticity of digital audio recordings is known as the electric network frequency ( enf ) criterion . it involves extracting the embedded power line ( utility ) frequency from said recordings and matching it to a known database to verify the time the recording was made , and its authenticity . in this paper , a nonparametric , adaptive , and high resolution technique , known as the time - recursive iterative adaptive approach , is presented as a tool for the extraction of the enf from digital audio recordings . a comparison is made between this data dependent ( adaptive ) filter and the conventional short - time fourier transform ( stft ) . results show that the adaptive algorithm improves the enf estimation accuracy in the presence of interference from other signals . to further enhance the enf estimation accuracy , a frequency tracking method based on dynamic programming will be proposed . the algorithm uses the knowledge that the enf is varying slowly with time to estimate with high accuracy the frequency present in the recording .
RANK = 6; score = 0.7222617864608765; correct = False; id = 539265193da35286d4f46497755dc9cc6d51387c
digital single lens reflex camera identification from traces of sensor dust digital single lens reflex cameras suffer from a well - known sensor dust problem due to interchangeable lenses that they deploy . the dust particles that settle in front of the imaging sensor create a persistent pattern in all captured images . in this paper , we propose a novel source camera identification method based on detection and matching of these dust - spot characteristics . dust spots in the image are detected based on a ( gaussian ) intensity loss model and shape properties . to prevent false detections , lens parameter - dependent characteristics of dust spots are also taken into consideration . experimental results show that the proposed detection scheme can be used in identification of the source digital single lens reflex camera at low false positive rates , even under heavy compression and downsampling .
RANK = 7; score = 0.7220859527587891; correct = False; id = 31e4845a40cfa6a953aef78387b34ea3284cdff9
all your biases belong to us : breaking rc4 in wpa - tkip and tls we present new biases in rc4 , break the wi - fi protected access temporal key integrity protocol ( wpa - tkip ) , and design a practical plaintext recovery attack against the transport layer security ( tls ) protocol . to empirically find new biases in the rc4 keystream we use statistical hypothesis tests . this reveals many new biases in the initial keystream bytes , as well as several new longterm biases . our fixed - plaintext recovery algorithms are capable of using multiple types of biases , and return a list of plaintext candidates in decreasing likelihood . to break wpa - tkip we introduce a method to generate a large number of identical packets . this packet is decrypted by generating its plaintext candidate list , and using redundant packet structure to prune bad candidates . from the decrypted packet we derive the tkip mic key , which can be used to inject and decrypt packets . in practice the attack can be executed within an hour . we also attack tls as used by https , where we show how to decrypt a secure cookie with a success rate of 94 % using 9 · 227 ciphertexts . this is done by injecting known data around the cookie , abusing this using mantin ’s absab bias , and brute - forcing the cookie by traversing the plaintext candidates . using our traffic generation technique , we are able to execute the attack in merely 75 hours .
RANK = 8; score = 0.7220041155815125; correct = False; id = 06f16d9430d5f6213cf5399b167a3d989c3ff798
practical mitigations for timing - based side - channel attacks on modern x86 processors this paper studies and evaluates the extent to which automated compiler techniques can defend against timing - based side - channel attacks on modern x86 processors . we study how modern x86 processors can leak timing information through side - channels that relate to control flow and data flow . to eliminate key - dependent control flow and key - dependent timing behavior related to control flow , we propose the use of if - conversion in a compiler backend , and evaluate a proof - of - concept prototype implementation . furthermore , we demonstrate two ways in which programs that lack key - dependent control flow and key- dependent cache behavior can still leak timing information on modern x86 implementations such as the intel core 2 duo , and propose defense mechanisms against them .
RANK = 9; score = 0.7218882441520691; correct = False; id = 0e873a513c525fe556dd5660630bd330bf1d0be8
eon : modeling and analyzing dynamic access control systems with logic programs we present eon , a logic - programming language and tool that can be used to model and analyze dynamic access control systems . our language extends datalog with some carefully designed constructs that allow the introduction and transformation of new relations . for example , these constructs can model the creation of processes and objects , and the modification of their security labels at runtime . the information - flow properties of such systems can be analyzed by asking queries in this language . we show that query evaluation in eon can be reduced to decidable query satisfiability in a fragment of datalog , and further , under some restrictions , to efficient query evaluation in datalog . we implement these reductions in our tool , and demonstrate its scope through several case studies . in particular , we study in detail the dynamic access control models of the windows vista and asbestos operating systems . we also automatically prove the security of a webserver running on asbestos .
RANK = 10; score = 0.7211960554122925; correct = False; id = 605ed83a6d1f4eaf995e85830f373923b11d6c13
cover your acks : pitfalls of covert channel censorship circumvention in response to increasingly sophisticated methods of blocking access to censorship circumvention schemes such as tor , recently proposed systems such as skypemorph , freewave , and censorspoofer have used voice and video conferencing protocols as " cover channels " to hide proxy connections . we demonstrate that even with perfect emulation of the cover channel , these systems can be vulnerable to attacks that detect or disrupt the covert communications while having no effect on legitimate cover traffic . our attacks stem from differences in the channel requirements for the cover protocols , which are peer - to - peer and loss tolerant , and the covert traffic , which is client - proxy and loss intolerant . these differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumvention schemes .
RANK = 11; score = 0.7211700081825256; correct = False; id = d0a3852f95e51a6b730df75e153d8446d6e8a90a
easeandroid : automatic policy analysis and refinement for security enhanced android via large - scale semi - supervised learning mandatory protection systems such as selinux and seandroid harden operating system integrity . unfortunately , policy development is error prone and requires lengthy refinement using audit logs from deployed systems . while prior work has studied selinux policy in detail , seandroid is relatively new and has received little attention . seandroid policy engineering differs significantly from selinux : android fundamentally differs from traditional linux ; the same policy is used on millions of devices for which new audit logs are continually available ; and audit logs contain a mix of benign and malicious accesses . in this paper , we propose easeandroid , the first seandroid analytic platform for automatic policy analysis and refinement . our key insight is that the policy refinement process can be modeled and automated using semi - supervised learning . given an existing policy and a small set of known access patterns , easeandroid continually expands the knowledge base as new audit logs become available , producing suggestions for policy refinement . we evaluate easeandroid on 1.3 million audit logs from real - world devices . easeandroid successfully learns 2,518 new access patterns and generates 331 new policy rules . during this process , easeandroid discovers eight categories of attack access patterns in real devices , two of which are new attacks directly against the seandroid mac mechanism .
RANK = 12; score = 0.7205185294151306; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 13; score = 0.7199642062187195; correct = False; id = 3f770cc7662340485f8fb328b3f2c95403a08e8d
alice in warningland : a large - scale field study of browser security warning effectiveness we empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature . we used mozilla firefox and google chrome ’s in - browser telemetry to observe over 25 million warning impressions in situ . during our field study , users continued through a tenth of mozilla firefox ’s malware and phishing warnings , a quarter of google chrome ’s malware and phishing warnings , and a third of mozilla firefox ’s ssl warnings . this demonstrates that security warnings can be effective in practice ; security experts and system architects should not dismiss the goal of communicating security information to end users . we also find that user behavior varies across warnings . in contrast to the other warnings , users continued through 70.2 % of google chrome ’s ssl warnings . this indicates that the user experience of a warning can have a significant impact on user behavior . based on our findings , we make recommendations for warning designers and researchers .
RANK = 14; score = 0.7199363708496094; correct = False; id = 8db2f1e1986c1185c674f34b74df126c6c6bb4dc
the seaview security model 
RANK = 15; score = 0.7190232276916504; correct = False; id = 3b5c6faa99a454499e33d87cfaef9dfcb0d7b796
tapdance : end - to - middle anticensorship without flow blocking in response to increasingly sophisticated state - sponsored internet censorship , recent work has proposed a new approach to censorship resistance : end - to - middle proxying . this concept , developed in systems such as telex , decoy routing , and cirripede , moves anticensorship technology into the core of the network , at large isps outside the censoring country . in this paper , we focus on two technical obstacles to the deployment of certain end - to - middle schemes : the need to selectively block flows and the need to observe both directions of a connection . we propose a new construction , tapdance , that removes these requirements . tapdance employs a novel tcp - level technique that allows the anticensorship station at an isp to function as a passive network tap , without an inline blocking component . we also apply a novel steganographic encoding to embed control messages in tls ciphertext , allowing us to operate on https connections even under asymmetric routing . we implement and evaluate a tapdance prototype that demonstrates how the system could function with minimal impact on an isp ’s network operations .
RANK = 16; score = 0.7186965942382812; correct = False; id = 22652399c7fb219a093344b9b47028b5d0069711
the cracked cookie jar : http cookie hijacking and the exposure of private information the widespread demand for online privacy , also fueled by widely - publicized demonstrations of session hijacking attacks against popular websites , has spearheaded the increasing deployment of https . however , many websites still avoid ubiquitous encryption due to performance or compatibility issues . the prevailing approach in these cases is to force critical functionality and sensitive data access over encrypted connections , while allowing more innocuous functionality to be accessed over http . in practice , this approach is prone to flaws that can expose sensitive information or functionality to third parties . in this paper , we conduct an in - depth assessment of a diverse set of major websites and explore what functionality and information is exposed to attackers that have hijacked a user 's http cookies . we identify a recurring pattern across websites with partially deployed https , service personalization inadvertently results in the exposure of private information . the separation of functionality across multiple cookies with different scopes and inter - dependencies further complicates matters , as imprecise access control renders restricted account functionality accessible to non - session cookies . our cookie hijacking study reveals a number of severe flaws , attackers can obtain the user 's home and work address and visited websites from google , bing and baidu expose the user 's complete search history , and yahoo allows attackers to extract the contact list and send emails from the user 's account . furthermore , e - commerce vendors such as amazon and ebay expose the user 's purchase history ( partial and full respectively ) , and almost every website exposes the user 's name and email address . ad networks like doubleclick can also reveal pages the user has visited . to fully evaluate the practicality and extent of cookie hijacking , we explore multiple aspects of the online ecosystem , including mobile apps , browser security mechanisms , extensions and search bars . to estimate the extent of the threat , we run irb - approved measurements on a subset of our university 's public wireless network for 30 days , and detect over 282 k accounts exposing the cookies required for our hijacking attacks . we also explore how users can protect themselves and find that , while mechanisms such as the eff 's https everywhere extension can reduce the attack surface , http cookies are still regularly exposed . the privacy implications of these attacks become even more alarming when considering how they can be used to deanonymize tor users . our measurements suggest that a significant portion of tor users may currently be vulnerable to cookie hijacking .
RANK = 17; score = 0.7186759114265442; correct = False; id = 568c44678d2bba4ae9d735b555e847437a7e6f15
tor : the second - generation onion router we present tor , a circuit - based low - latency anonymous communication service . this second - generation onion routing system addresses limitations in the original design . tor adds perfect forward secrecy , congestion control , directory servers , integrity checking , configurable exit policies , and a practical design for rendezvous points . tor works on the real - world internet , requires no special privileges or kernel modifications , requires little synchronization or coordination between nodes , and provides a reasonable tradeoff between anonymity , usability , and efficiency . we briefly describe our experiences with an international network of more than a dozen hosts . we close with a list of open problems in anonymous communication .
RANK = 18; score = 0.7182919383049011; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 19; score = 0.7176091074943542; correct = False; id = a026179217a2a578a9cfaa4730f8ae57de190920
catch me if you can : evaluating android anti - malware against transformation attacks mobile malware threats ( e.g. , on android ) have recently become a real concern . in this paper , we evaluate the state - of - the - art commercial mobile anti - malware products for android and test how resistant they are against various common obfuscation techniques ( even with known malware ) . such an evaluation is important for not only measuring the available defense against mobile malware threats , but also proposing effective , next - generation solutions . we developed droidchameleon , a systematic framework with various transformation techniques , and used it for our study . our results on 10 popular commercial anti - malware applications for android are worrisome : none of these tools is resistant against common malware transformation techniques . in addition , a majority of them can be trivially defeated by applying slight transformation over known malware with little effort for malware authors . finally , in light of our results , we propose possible remedies for improving the current state of malware detection on mobile devices .
RANK = 20; score = 0.717374861240387; correct = False; id = 56ecb04f003d76317ff2f9a2b614dd9fba317317
an analysis of covert timing channels 

RANKING 1258
QUERY
how much attention do you need ? a granular analysis of neural machine translation architectures acl 2018 • baseline : 6 layer transformer , 512 hidden units • data sets : iwslt'16 , wmt'17 • metrics : bleu ( and meteor in the paper ) • 3 runs , reporting mean and standard deviation encoder / decoder combinations • architecture definition language • allows easy experimentation of neural machine translation ( nmt ) architecture variations • detailed analysis of nmt architecture combinations • multiple source attention layers and residual feedforward layer are key • self - attention more important on the source side introduction
First cited at 61
TOP CITED PAPERS
RANK 61
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK 3205
better hypothesis testing for statistical machine translation : controlling for optimizer instability in statistical machine translation , a researcher seeks to determine whether some innovation ( e.g. , a new feature , model , or inference algorithm ) improves translation quality in comparison to a baseline system . to answer this question , he runs an experiment to evaluate the behavior of the two systems on held - out data . in this paper , we consider how to make such experiments more statistically reliable . we provide a systematic analysis of the effects of optimizer instability — an extraneous variable that is seldom controlled for — on experimental outcomes , and make recommendations for reporting results more accurately .
RANK 5123
effective approaches to attention - based neural machine translation an attentional mechanism has lately been used to improve neural machine translation ( nmt ) by selectively focusing on parts of the source sentence during translation . however , there has been little work exploring useful architectures for attention - based nmt . this paper examines two simple and effective classes of attentional mechanism : a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . we demonstrate the effectiveness of both approaches on the wmt translation tasks between english and german in both directions . with local attention , we achieve a significant gain of 5.0 bleu points over non - attentional systems that already incorporate known techniques such as dropout . our ensemble model using different attention architectures yields a new state - of - the - art result in the wmt’15 english to german translation task with 25.9 bleu points , an improvement of 1.0 bleu points over the existing best system backed by nmt and an n - gram reranker.1
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8178408145904541; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8141793012619019; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.812674343585968; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.7883370518684387; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.7850621938705444; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 6; score = 0.7767152190208435; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.7702308297157288; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.7669839859008789; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 9; score = 0.7662442326545715; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 10; score = 0.763027548789978; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.7603669166564941; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 12; score = 0.7535810470581055; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.7526240348815918; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 14; score = 0.7487097978591919; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 15; score = 0.7477499842643738; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 16; score = 0.741867184638977; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 17; score = 0.7382540702819824; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 18; score = 0.7377657294273376; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 19; score = 0.736987829208374; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 20; score = 0.7332904934883118; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .

RANKING 1135
QUERY
measuring frame instance relatedness frame semantics is a well - established framework to represent the meaning of natural language in computational terms . in this work , we aim to propose a quantitative measure of relatedness between pairs of frame instances . we test our method on a dataset of sentence pairs , highlighting the correlation between our metric and human judgments of semantic similarity . furthermore , we propose an application of our measure for clustering frame instances to extract prototypical knowledge from natural language .
First cited at 1
TOP CITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 183
the berkeley framenet project framenet is a three - year nsf - supported project in corpus - based computational lexicography , now in its second year ( nsf iri-9618838 , " tools for lexicon building " ) . the project 's key features are ( a ) a commitment to corpus evidence for semantic and syntactic generalizations , and ( b ) the representation of the valences of its target words ( mostly nouns , adjectives , and verbs ) in which the semantic portion makes use of frame semantics . the resulting database will contain ( a ) descriptions of the semantic frames underlying the meanings of the words described , and ( b ) the valence representation ( semantic and syntactic ) of several thousand words and phrases , each accompanied by ( c ) a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between " frame elements " and their syntactic realizations ( e.g. grammatical function , phrase type , and other syntactic traits ) . this report will present the project 's goals and workflow , and information about the computational tools that have been adapted or created in - house for this work . 1 i n t r o d u c t i o n the berkeley framenet project 1 is producing frame - semantic descriptions of several thousand english lexical items and backing up these descriptions with semantically annotated attestations from contemporary english corpora 2 . 1the project is based at the international computer science institute ( 1947 center street , berkeley , ca ) . a fuller bibliography may be found in ( lowe et ai . , 1997 ) 2our main corpus is the british national corpus . we have access to it through the courtesy of oxford university press ; the pos - tagged and lemmatized version we use was prepared by the institut flit maschinelle sprachverarbeitung of the university of stuttgart ) . the these descriptions are based on hand - tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists . the primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine - readable form . the intuition of the lexicographers is guided by and constrained by the results of corpus - based research using highperformance software tools . the semantic domains to be covered are " health care , chance , perception , communication , transaction , time , space , body ( parts and functions of the body ) , motion , life stages , social context , emotion and cognition . 1.1 scope of t h e p r o j e c t the results of the project are ( a ) a lexical resource , called the framenet database 3 , and ( b ) associated software tools . the database has three major components ( described in more detail below : • lexicon containing entries which are composed of : ( a ) some conventional dictionary - type data , mainly for the sake of human readers ; ( b ) formulas which capture the morphosyntactic ways in which elements of the semantic frame can be realized within the phrases or sentences built up around the word ; ( c ) links to semantically annotated exameuropean collaborators whose participation has made this possible are sue atkins , oxford university press , and ulrich held , ims - stuttgart . sthe database will ultimately contain at least 5,000 lexical entries together with a parallel annotated corpus , these in formats suitable for integration into applications which use other lexical resources such as wordnet and comlex . the final design of the database will be selected in consultation with colleagues at princeton ( wordnet ) , icsi , and ims , and with other members of the nlp community .
RANK 1650
measuring frame relatedness in this paper we introduce the notion of “ frame relatedness ” , i.e. relatedness among prototypical situations as represented in the framenet database . we first demonstrate the cognitive plausibility of that notion through an annotation experiment , and then propose different types of computational measures to automatically assess relatedness . results show that our measures provide good performance on the task of ranking pairs of frames .
TOP UNCITED PAPERS
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK 4
textrank : bringing order into text 
TOP 20
RANK = 1; score = 0.8544687032699585; correct = True; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.8513326644897461; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8499246835708618; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8290081024169922; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.8276333212852478; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 6; score = 0.8245775699615479; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.8119039535522461; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 8; score = 0.8112455010414124; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 9; score = 0.8095216155052185; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 10; score = 0.805801510810852; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.8032128214836121; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 12; score = 0.7981664538383484; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 13; score = 0.7963486909866333; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 14; score = 0.7961606383323669; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 15; score = 0.7942492365837097; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 16; score = 0.7921517491340637; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 17; score = 0.7899205684661865; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 18; score = 0.7871841788291931; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 19; score = 0.7857394814491272; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 20; score = 0.785094678401947; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .

RANKING 2190
QUERY
exploiting contextual information via dynamic memory network for event detection the task of event detection involves identifying and categorizing event triggers . contextual information has been shown effective on the task . however , existing methods which utilize contextual information only process the context once . we argue that the context can be better exploited by processing the context multiple times , allowing the model to perform complex reasoning and to generate better context representation , thus improving the overall performance . meanwhile , dynamic memory network ( dmn ) has demonstrated promising capability in capturing contextual information and has been applied successfully to various tasks . in light of the multi - hop mechanism of the dmn to model the context , we propose the trigger detection dynamic memory network ( td - dmn ) to tackle the event detection problem . we performed a five - fold crossvalidation on the ace-2005 dataset and experimental results show that the multi - hop mechanism does improve the performance and the proposed model achieves best f1 score compared to the state - of - the - art methods .
First cited at 11
TOP CITED PAPERS
RANK 11
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK 98
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
RANK 311
joint event extraction via recurrent neural networks event extraction is a particularly challenging problem in information extraction . the stateof - the - art models for this problem have either applied convolutional neural networks in a pipelined framework ( chen et al . , 2015 ) or followed the joint architecture via structured prediction with rich local and global features ( li et al . , 2013 ) . the former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words . the latter , on the other hand , is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter - dependencies between event triggers and argument roles via discrete structures . in this work , we propose to do event extraction in a joint framework with bidirectional recurrent neural networks , thereby benefiting from the advantages of the two models as well as addressing issues inherent in the existing approaches . we systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state - of - the - art performance on the ace 2005 dataset .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8662797808647156; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.863558828830719; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8628298044204712; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8474360704421997; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.8413380980491638; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 6; score = 0.8370920419692993; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 7; score = 0.836235761642456; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 8; score = 0.8351165652275085; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 9; score = 0.8339731097221375; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 10; score = 0.8289920091629028; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.8288734555244446; correct = True; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 12; score = 0.827835202217102; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 13; score = 0.8255929350852966; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 14; score = 0.8238535523414612; correct = False; id = 06d22d068996a25df4e463410c2f36deded36512
semeval-2015 task 4 : timeline : cross - document event ordering this paper describes the outcomes of the timeline task ( cross - document event ordering ) , that was organised within the time and space track of semeval-2015 . given a set of documents and a set of target entities , the task consisted of building a timeline for each entity , by detecting , anchoring in time and ordering the events involving that entity . the timeline task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents . four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs , the best of which obtained an f1-score of 7.85 in the main track ( timeline creation from raw text ) .
RANK = 15; score = 0.8219440579414368; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 16; score = 0.820863664150238; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 17; score = 0.8177708387374878; correct = False; id = 436a1de1c3fafa0e5500ebbaf70ca79df232b87c
semeval-2015 task 14 : analysis of clinical text we describe two tasks — named entity recognition ( task 1 ) and template slot filling ( task 2)—for clinical texts . the tasks leverage annotations from the share corpus , which consists of clinical notes with annotated mentions disorders , along with their normalization to a medical terminology and eight additional attributes . the purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling . task 2 consisted of two subtasks : template slot filling given gold - standard disorder spans ( task 2a ) and end - to - end disorder span identification together with template slot filling ( task 2b ) . for task 1 ( disorder span detection and normalization ) , 16 teams participated . the best system yielded a strict f1-score of 75.7 , with a precision of 78.3 and recall of 73.2 . for task 2a ( template slot filling given goldstandard disorder spans ) , six teams participated . the best system yielded a combined overall weighted accuracy for slot filling of 88.6 . for task 2b ( disorder recognition and template slot filling ) , nine teams participated . the best system yielded a combined relaxed f ( for span detection ) and overall weighted accuracy of 80.8 .
RANK = 18; score = 0.8171202540397644; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 19; score = 0.8163259625434875; correct = False; id = 4773dd8af1413146383f53894a888184b7c68938
semeval-2015 task 1 : paraphrase and semantic similarity in twitter ( pit ) in this shared task , we present evaluations on two related tasks paraphrase identification ( pi ) and semantic textual similarity ( ss ) systems for the twitter data . given a pair of sentences , participants are asked to produce a binary yes / no judgement or a graded score to measure their semantic equivalence . the task features a newly constructed twitter paraphrase corpus that contains 18,762 sentence pairs . a total of 19 teams participated , submitting 36 runs to the pi task and 26 runs to the ss task . the evaluation shows encouraging results and open challenges for future research . the best systems scored a f1-measure of 0.674 for the pi task and a pearson correlation of 0.619 for the ss task respectively , comparing to a strong baseline using logistic regression model of 0.589 f1 and 0.511 pearson ; while the best ss systems can often reach > 0.80 pearson on well - formed text . this shared task also provides insights into the relation between the pi and ss tasks and suggests the importance to bringing these two research areas together . we make all the data , baseline systems and evaluation scripts publicly available.1
RANK = 20; score = 0.815409243106842; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .

RANKING 1374
QUERY
a web - scale system for scientific knowledge exploration to enable efficient exploration of webscale scientific knowledge , it is necessary to organize scientific publications into a hierarchical concept structure . in this work , we present a large - scale system to ( 1 ) identify hundreds of thousands of scientific concepts , ( 2 ) tag these identified concepts to hundreds of millions of scientific publications by leveraging both text and graph structure , and ( 3 ) build a six - level concept hierarchy with a subsumption - based model . the system builds the most comprehensive crossdomain scientific concept ontology published to date , with more than 200 thousand concepts and over one million relationships .
First cited at 11534
TOP CITED PAPERS
RANK 11534
inferring missing entity type instances for knowledge base completion : new dataset and methods most of previous work in knowledge base ( kb ) completion has focused on the problem of relation extraction . in this work , we focus on the task of inferring missing entity type instances in a kb , a fundamental task for kb competition yet receives little attention . due to the novelty of this task , we construct a large - scale dataset and design an automatic evaluation methodology . our knowledge base completion method uses information within the existing kb and external information from wikipedia . we show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods . we also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.7837194204330444; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.7800443768501282; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.7790096402168274; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.7562804818153381; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.7516465187072754; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 6; score = 0.7476319670677185; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 7; score = 0.7441747784614563; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.7390894889831543; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 9; score = 0.734495222568512; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 10; score = 0.7315418720245361; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.7310366630554199; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 12; score = 0.7296886444091797; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.7241968512535095; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 14; score = 0.718989372253418; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 15; score = 0.7188170552253723; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 16; score = 0.7148392200469971; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 17; score = 0.7128320932388306; correct = False; id = 36918139c42c201557cef3dc69e26a06460155d8
impar : a deterministic algorithm for implicit semantic role labelling this paper presents a novel deterministic algorithm for implicit semantic role labeling . the system exploits a very simple but relevant discursive property , the argument coherence over different instances of a predicate . the algorithm solves the implicit arguments sequentially , exploiting not only explicit but also the implicit arguments previously solved . in addition , we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data .
RANK = 18; score = 0.7050062417984009; correct = False; id = de794d50713ea5f91a7c9da3d72041e2f5ef8452
the third pascal recognizing textual entailment challenge this paper presents the third pascal recognising textual entailment challenge ( rte-3 ) , providing an overview of the dataset creating methodology and the submitted systems . in creating this year ’s dataset , a number of longer texts were introduced to make the challenge more oriented to realistic scenarios . additionally , a pool of resources was offered so that the participants could share common tools . a pilot task was also set up , aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions . 26 participants submitted 44 runs , using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges . 1.1 the rte challenges the goal of the rte challenges has been to create a benchmark task dedicated to textual entailment – recognizing that the meaning of one text is entailed , i.e. can be inferred , by another . in the recent years , this task has raised great interest since applied semantic inference concerns many practical natural language processing ( nlp ) applications , such as question answering ( qa ) , information extraction ( ie ) , summarization , machine translation and paraphrasing , and certain types of queries in information retrieval ( ir ) . more specifically , the rte challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different nlp applications need to handle . for example , in addition to textual entailment , qa systems need to handle issues such as answer retrieval and question type recognition . by separating out the general problem of textual entailment from these task - specific problems , progress on semantic inference for many application areas can be promoted . hopefully , research on textual entailment will finally lead to the development of entailment “ engines ” , which can be used as a standard module in many applications ( similar to the role of part - of - speech taggers and syntactic parsers in current nlp applications ) . in the following sections , a detailed description of rte-3 is presented . after a quick review 1 the task was first defined by dagan and glickman ( 2004 ) .
RANK = 19; score = 0.7017683982849121; correct = False; id = 15ae0badc584a287fc51e5de46d1ef51495a2398
finding deceptive opinion spam by any stretch of the imagination consumers increasingly rate , review and research products online ( jansen , 2010 ; litvin et al . , 2008 ) . consequently , websites containing consumer reviews are becoming targets of opinion spam . while recent work has focused primarily on manually identifiable instances of opinion spam , in this work we study deceptive opinion spam — fictitious opinions that have been deliberately written to sound authentic . integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold - standard opinion spam dataset . based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing .
RANK = 20; score = 0.7013422846794128; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .

RANKING 841
QUERY
thu_ngn at semeval-2018 task 3 : tweet irony detection with densely connected lstm and multi - task learning detecting irony is an important task to mine fine - grained information from social web messages . therefore , the semeval-2018 task 3 is aimed to detect the ironic tweets ( subtask a ) and their irony types ( subtask b ) . in order to address this task , we propose a system based on a densely connected lstm network with multi - task learning strategy . in our dense lstm model , each layer will take all outputs from previous layers as input . the last lstm layer will output the hidden representations of texts , and they will be used in three classification task . in addition , we incorporate several types of features to improve the model performance . our model achieved an f - score of 70.54 ( ranked 2/43 ) in the subtask a and 49.47 ( ranked 3/29 ) in the subtask b. the experimental results validate the effectiveness of our system .
First cited at 166
TOP CITED PAPERS
RANK 166
fracking sarcasm using neural network precise semantic representation of a sentence and definitive information extraction are key steps in the accurate processing of sentence meaning , especially for figurative phenomena such as sarcasm , irony , and metaphor cause literal meanings to be discounted and secondary or extended meanings to be intentionally profiled . semantic modelling faces a new challenge in social media , because grammatical inaccuracy is commonplace yet many previous state - of - the - art methods exploit grammatical structure . for sarcasm detection over social media content , researchers so far have counted on bag - of - words(bow ) , n - grams etc . in this paper , we propose a neural network semantic model for the task of sarcasm detection . we also review semantic modelling using support vector machine ( svm ) that employs constituency parsetrees fed and labeled with syntactic and semantic information . the proposed neural network model composed of convolution neural network(cnn ) and followed by a long short term memory ( lstm ) network and finally a deep neural network(dnn ) . the proposed model outperforms state - of - the - art textbased methods for sarcasm detection , yielding an f - score of .92 .
RANK 665
improved part - of - speech tagging for online conversational text with word clusters we consider the problem of part - of - speech tagging for informal , online conversational text . we systematically evaluate the use of large - scale unsupervised word clustering and new lexical features to improve tagging accuracy . with these features , our system achieves state - of - the - art tagging results on both twitter and irc pos tagging tasks ; twitter tagging is improved from 90 % to 93 % accuracy ( more than 3 % absolute ) . qualitative analysis of these word clusters yields insights about nlp and linguistic phenomena in this genre . additionally , we contribute the first pos annotation guidelines for such text and release a new dataset of english language tweets annotated using these guidelines . tagging software , annotation guidelines , and large - scale word clusters are available at : http://www.ark.cs.cmu.edu/tweetnlp this paper describes release 0.3 of the “ cmu twitter part - of - speech tagger ” and annotated data . [ this paper is forthcoming in proceedings of naacl 2013 ; atlanta , ga , usa . ]
RANK 2535
modelling sarcasm in twitter , a novel approach automatic detection of figurative language is a challenging task in computational linguistics . recognising both literal and figurative meaning is not trivial for a machine and in some cases it is hard even for humans . for this reason novel and accurate systems able to recognise figurative languages are necessary . we present in this paper a novel computational model capable to detect sarcasm in the social network twitter ( a popular microblogging service which allows users to post short messages ) . our model is easy to implement and , unlike previous systems , it does not include patterns of words as features . our seven sets of lexical features aim to detect sarcasm by its inner structure ( for example unexpectedness , intensity of the terms or imbalance between registers ) , abstracting from the use of specific terms .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.843238890171051; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.842051088809967; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8418115973472595; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8256959915161133; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 5; score = 0.8230596780776978; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 6; score = 0.8179357051849365; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 7; score = 0.8169379234313965; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 8; score = 0.8159704804420471; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 9; score = 0.8094324469566345; correct = False; id = 06d22d068996a25df4e463410c2f36deded36512
semeval-2015 task 4 : timeline : cross - document event ordering this paper describes the outcomes of the timeline task ( cross - document event ordering ) , that was organised within the time and space track of semeval-2015 . given a set of documents and a set of target entities , the task consisted of building a timeline for each entity , by detecting , anchoring in time and ordering the events involving that entity . the timeline task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents . four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs , the best of which obtained an f1-score of 7.85 in the main track ( timeline creation from raw text ) .
RANK = 10; score = 0.8053655028343201; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .
RANK = 11; score = 0.8031387329101562; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 12; score = 0.8026525974273682; correct = False; id = 436a1de1c3fafa0e5500ebbaf70ca79df232b87c
semeval-2015 task 14 : analysis of clinical text we describe two tasks — named entity recognition ( task 1 ) and template slot filling ( task 2)—for clinical texts . the tasks leverage annotations from the share corpus , which consists of clinical notes with annotated mentions disorders , along with their normalization to a medical terminology and eight additional attributes . the purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling . task 2 consisted of two subtasks : template slot filling given gold - standard disorder spans ( task 2a ) and end - to - end disorder span identification together with template slot filling ( task 2b ) . for task 1 ( disorder span detection and normalization ) , 16 teams participated . the best system yielded a strict f1-score of 75.7 , with a precision of 78.3 and recall of 73.2 . for task 2a ( template slot filling given goldstandard disorder spans ) , six teams participated . the best system yielded a combined overall weighted accuracy for slot filling of 88.6 . for task 2b ( disorder recognition and template slot filling ) , nine teams participated . the best system yielded a combined relaxed f ( for span detection ) and overall weighted accuracy of 80.8 .
RANK = 13; score = 0.8012955784797668; correct = False; id = 3209e1d72cdb630a43c50bd86742b9f7c7c01470
semeval-2015 task 17 : taxonomy extraction evaluation ( texeval ) this paper describes the first shared task on taxonomy extraction evaluation organised as part of semeval-2015 . participants were asked to find hypernym - hyponym relations between given terms . for each of the four selected target domains the participants were provided with two lists of domainspecific terms : a wordnet collection of terms and a well - known terminology extracted from an online publicly available taxonomy . a total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures , the structural similarity with a gold standard taxonomy , and through manual quality assessment of sampled novel relations .
RANK = 14; score = 0.8003570437431335; correct = False; id = 4773dd8af1413146383f53894a888184b7c68938
semeval-2015 task 1 : paraphrase and semantic similarity in twitter ( pit ) in this shared task , we present evaluations on two related tasks paraphrase identification ( pi ) and semantic textual similarity ( ss ) systems for the twitter data . given a pair of sentences , participants are asked to produce a binary yes / no judgement or a graded score to measure their semantic equivalence . the task features a newly constructed twitter paraphrase corpus that contains 18,762 sentence pairs . a total of 19 teams participated , submitting 36 runs to the pi task and 26 runs to the ss task . the evaluation shows encouraging results and open challenges for future research . the best systems scored a f1-measure of 0.674 for the pi task and a pearson correlation of 0.619 for the ss task respectively , comparing to a strong baseline using logistic regression model of 0.589 f1 and 0.511 pearson ; while the best ss systems can often reach > 0.80 pearson on well - formed text . this shared task also provides insights into the relation between the pi and ss tasks and suggests the importance to bringing these two research areas together . we make all the data , baseline systems and evaluation scripts publicly available.1
RANK = 15; score = 0.7974399924278259; correct = False; id = 3ff04518e8d6a2720c009be2fc481fbb7cc5040b
semeval-2015 task 2 : semantic textual similarity , english , spanish and pilot on interpretability in semantic textual similarity ( sts ) , systems rate the degree of semantic equivalence between two text snippets . this year , the participants were challenged with new datasets in english and spanish . for the english subtask , we exposed the systems to a diversity of testing scenarios , by preparing additional pairs from headlines and image descriptions , as well as introducing new genres , including answer pairs from a tutorial dialogue system , answer pairs from q&a websites , and pairs from a committed belief dataset . for the spanish subtask , additional pairs from news and wikipedia articles were selected . the annotations for both subtasks leveraged crowdsourcing . the english subtask attracted 29 teams with 74 system runs , and the spanish subtask engaged 7 teams participating with 16 system runs . in addition , this year we ran a pilot task on interpretable sts , where the systems needed to add an explanatory layer , that is , they had to align the chunks in the sentence pair , explicitly annotating the kind of relation and the score for the chunk pair . the train and test data were manually annotated by an expert , and included headline and image sentence pairs from previous years . 7 teams participated with 29 runs .
RANK = 16; score = 0.7971674799919128; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 17; score = 0.7966289520263672; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 18; score = 0.7954933643341064; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 19; score = 0.7925954461097717; correct = False; id = 8208d5644984a8ef5d10c9bc2f898f539d189263
semeval-2015 task 10 : sentiment analysis in twitter in this paper , we describe the 2015 iteration of the semeval shared task on sentiment analysis in twitter . this was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years . this year ’s shared task competition consisted of five sentiment prediction subtasks . two were reruns from previous years : ( a ) sentiment expressed by a phrase in the context of a tweet , and ( b ) overall sentiment of a tweet . we further included three new subtasks asking to predict ( c ) the sentiment towards a topic in a single tweet , ( d ) the overall sentiment towards a topic in a set of tweets , and ( e ) the degree of prior polarity of a phrase .
RANK = 20; score = 0.7887994647026062; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .

RANKING 2321
QUERY
data - differential address trace analysis : finding address - based side - channels in binaries cryptographic implementations are a valuable target for address - based side - channel attacks and should , thus , be protected against them . countermeasures , however , are often incorrectly deployed or completely omitted in practice . moreover , existing tools that identify information leaks in programs either suffer from imprecise abstraction or only cover a subset of possible leaks . we systematically address these limitations and propose a new methodology to test software for information leaks . in this work , we present data , a differential address trace analysis framework that detects address - based sidechannel leaks in program binaries . this accounts for attacks exploiting caches , dram , branch prediction , controlled channels , and likewise . data works in three phases . first , the program under test is executed to record several address traces . these traces are analyzed using a novel algorithm that dynamically re - aligns traces to increase detection accuracy . second , a generic leakage test filters differences caused by statistically independent program behavior , e.g. , randomization , and reveals true information leaks . the third phase classifies these leaks according to the information that can be obtained from them . this provides further insight to security analysts about the risk they pose in practice . we use data to analyze openssl and pycrypto in a fully automated way . among several expected leaks in symmetric ciphers , data also reveals known and previously unknown leaks in asymmetric primitives ( rsa , dsa , ecdsa ) , and data identifies erroneous bug fixes of supposedly fixed constant - time vulnerabilities .
First cited at 18
TOP CITED PAPERS
RANK 18
practical mitigations for timing - based side - channel attacks on modern x86 processors this paper studies and evaluates the extent to which automated compiler techniques can defend against timing - based side - channel attacks on modern x86 processors . we study how modern x86 processors can leak timing information through side - channels that relate to control flow and data flow . to eliminate key - dependent control flow and key - dependent timing behavior related to control flow , we propose the use of if - conversion in a compiler backend , and evaluate a proof - of - concept prototype implementation . furthermore , we demonstrate two ways in which programs that lack key - dependent control flow and key- dependent cache behavior can still leak timing information on modern x86 implementations such as the intel core 2 duo , and propose defense mechanisms against them .
RANK 648
differential slicing : identifying causal execution differences for security applications a security analyst often needs to understand two runs of the same program that exhibit a difference in program state or output . this is important , for example , for vulnerability analysis , as well as for analyzing a malware program that features different behaviors when run in different environments . in this paper we propose a differential slicing approach that automates the analysis of such execution differences . differential slicing outputs a causal difference graph that captures the input differences that triggered the observed difference and the causal path of differences that led from those input differences to the observed difference . the analyst uses the graph to quickly understand the observed difference . we implement differential slicing and evaluate it on the analysis of 11 real - world vulnerabilities and 2 malware samples with environment - dependent behaviors . we also evaluate it in an informal user study with two vulnerability analysts . our results show that differential slicing successfully identifies the input differences that caused the observed difference and that the causal difference graph significantly reduces the amount of time and effort required for an analyst to understand the observed difference .
RANK 894
controlled - channel attacks : deterministic side channels for untrusted operating systems the presence of large numbers of security vulnerabilities in popular feature - rich commodity operating systems has inspired a long line of work on excluding these operating systems from the trusted computing base of applications , while retaining many of their benefits . legacy applications continue to run on the untrusted operating system , while a small hyper visor or trusted hardware prevents the operating system from accessing the applications ' memory . in this paper , we introduce controlled - channel attacks , a new type of side - channel attack that allows an untrusted operating system to extract large amounts of sensitive information from protected applications on systems like overshadow , ink tag or haven . we implement the attacks on haven and ink tag and demonstrate their power by extracting complete text documents and outlines of jpeg images from widely deployed application libraries . given these attacks , it is unclear if over shadow 's vision of protecting unmodified legacy applications from legacy operating systems running on off - the - shelf hardware is still tenable .
TOP UNCITED PAPERS
RANK 1
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK 2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK 3
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
TOP 20
RANK = 1; score = 0.7675237059593201; correct = False; id = 2c5a5a2ab4f7b63523981ac790399c3ef2f08014
explicating sdks : uncovering assumptions underlying secure authentication and authorization module subject to dev guide concrete module with src or documentation black - box concrete module
RANK = 2; score = 0.7403915524482727; correct = False; id = 13dcd0c2c035417b7ab9c493b05f5294b27b6de2
an empirical study of textual key - fingerprint representations no peter gutman , 2011
RANK = 3; score = 0.7294653654098511; correct = False; id = 66a6b8b5086454d2f511089ed3c157075239eb7d
cross - origin pixel stealing : timing attacks using css filters timing attacks rely on systems taking varying amounts of time to process different input values . this is usually the result of either conditional branching in code or differences in input size . using css default filters , we have discovered a variety of timing attacks that work in multiple browsers and devices . the first attack exploits differences in time taken to render various dom trees . this knowledge can be used to determine boolean values such as whether or not a user has an account with a particular website . second , we introduce pixel stealing . pixel stealing attacks can be used to sniff user history and read text tokens .
RANK = 4; score = 0.7276402115821838; correct = False; id = 142947116c7baec662984feb82b18c3812bc91f9
20 years of covert channel modeling and analysis covert channels emerged in mystery and departed in
RANK = 5; score = 0.7257721424102783; correct = False; id = 22a78f31395e79cb6c99c3cedd248ecd6568b7f7
every second counts : quantifying the negative externalities of cybercrime via typosquatting while we have a good understanding of how cyber crime is perpetrated and the profits of the attackers , the harm experienced by humans is less well understood , and reducing this harm should be the ultimate goal of any security intervention . this paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference . intent inference allows us to define a new metric for quantifying harm to users , develop a new methodology for identifying typo squatting domain names , and quantify the harm caused by various typo squatting perpetrators . we find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page , and legitimate sites lose approximately 5 % of their mistyped traffic over the alternative of an unregistered typo . although on average perpetrators increase the time it takes a user to find their intended site , many typo squatters actually improve the latency between a typo and its correction , calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime .
RANK = 6; score = 0.7244628667831421; correct = False; id = 5c2bba059f15e8d3cda8b091ec02e180b81a6d3c
acing the ioc game : toward automatic discovery and analysis of open - source cyber threat intelligence to adapt to the rapidly evolving landscape of cyber threats , security professionals are actively exchanging indicators of compromise ( ioc ) ( e.g. , malware signatures , botnet ips ) through public sources ( e.g. blogs , forums , tweets , etc . ) . such information , often presented in articles , posts , white papers etc . , can be converted into a machine - readable openioc format for automatic analysis and quick deployment to various security mechanisms like an intrusion detection system . with hundreds of thousands of sources in the wild , the ioc data are produced at a high volume and velocity today , which becomes increasingly hard to manage by humans . efforts to automatically gather such information from unstructured text , however , is impeded by the limitations of today 's natural language processing ( nlp ) techniques , which can not meet the high standard ( in terms of accuracy and coverage ) expected from the iocs that could serve as direct input to a defense system . in this paper , we present iace , an innovation solution for fully automated ioc extraction . our approach is based upon the observation that the iocs in technical articles are often described in a predictable way : being connected to a set of context terms ( e.g. , " download " ) through stable grammatical relations . leveraging this observation , iace is designed to automatically locate a putative ioc token ( e.g. , a zip file ) and its context ( e.g. , " malware " , " download " ) within the sentences in a technical article , and further analyze their relations through a novel application of graph mining techniques . once the grammatical connection between the tokens is found to be in line with the way that the ioc is commonly presented , these tokens are extracted to generate an openioc item that describes not only the indicator ( e.g. , a malicious zip file ) but also its context ( e.g. , download from an external source ) . running on 71,000 articles collected from 45 leading technical blogs , this new approach demonstrates a remarkable performance : it generated 900 k openioc items with a precision of 95 % and a coverage over 90 % , which is way beyond what the state - of - the - art nlp technique and industry ioc tool can achieve , at a speed of thousands of articles per hour . further , by correlating the iocs mined from the articles published over a 13-year span , our study sheds new light on the links across hundreds of seemingly unrelated attack instances , particularly their shared infrastructure resources , as well as the impacts of such open - source threat intelligence on security protection and evolution of attack strategies .
RANK = 7; score = 0.7238361835479736; correct = False; id = 8db2f1e1986c1185c674f34b74df126c6c6bb4dc
the seaview security model 
RANK = 8; score = 0.7221512198448181; correct = False; id = 8c56a28b6d287c93dc401470dadb6f74f240e29c
digital image sharing by diverse image media conventional visual secret sharing ( vss ) schemes hide secret images in shares that are either printed on transparencies or are encoded and stored in a digital form . the shares can appear as noise - like pixels or as meaningful images ; but it will arouse suspicion and increase interception risk during transmission of the shares . hence , vss schemes suffer from a transmission risk problem for the secret itself and for the participants who are involved in the vss scheme . to address this problem , we proposed a natural - image - based vss scheme ( nvss scheme ) that shares secret images via various carrier media to protect the secret and the participants during the transmission phase . the proposed ( n , n)- nvss scheme can share one digital secret image over n-1 arbitrary selected natural images ( called natural shares ) and one noise - like share . the natural shares can be photos or hand - painted pictures in digital form or in printed form . the noise - like share is generated based on these natural shares and the secret image . the unaltered natural shares are diverse and innocuous , thus greatly reducing the transmission risk problem . we also propose possible ways to hide the noise - like share to reduce the transmission risk problem for the share . experimental results indicate that the proposed approach is an excellent solution for solving the transmission risk problem for the vss schemes .
RANK = 9; score = 0.7197716236114502; correct = False; id = 0e873a513c525fe556dd5660630bd330bf1d0be8
eon : modeling and analyzing dynamic access control systems with logic programs we present eon , a logic - programming language and tool that can be used to model and analyze dynamic access control systems . our language extends datalog with some carefully designed constructs that allow the introduction and transformation of new relations . for example , these constructs can model the creation of processes and objects , and the modification of their security labels at runtime . the information - flow properties of such systems can be analyzed by asking queries in this language . we show that query evaluation in eon can be reduced to decidable query satisfiability in a fragment of datalog , and further , under some restrictions , to efficient query evaluation in datalog . we implement these reductions in our tool , and demonstrate its scope through several case studies . in particular , we study in detail the dynamic access control models of the windows vista and asbestos operating systems . we also automatically prove the security of a webserver running on asbestos .
RANK = 10; score = 0.718899130821228; correct = False; id = 4b7c753bb235b9aafd234e00300c942665e8e481
enf extraction from digital recordings using adaptive techniques and frequency tracking a novel forensic tool used for assessing the authenticity of digital audio recordings is known as the electric network frequency ( enf ) criterion . it involves extracting the embedded power line ( utility ) frequency from said recordings and matching it to a known database to verify the time the recording was made , and its authenticity . in this paper , a nonparametric , adaptive , and high resolution technique , known as the time - recursive iterative adaptive approach , is presented as a tool for the extraction of the enf from digital audio recordings . a comparison is made between this data dependent ( adaptive ) filter and the conventional short - time fourier transform ( stft ) . results show that the adaptive algorithm improves the enf estimation accuracy in the presence of interference from other signals . to further enhance the enf estimation accuracy , a frequency tracking method based on dynamic programming will be proposed . the algorithm uses the knowledge that the enf is varying slowly with time to estimate with high accuracy the frequency present in the recording .
RANK = 11; score = 0.718694269657135; correct = False; id = 56ecb04f003d76317ff2f9a2b614dd9fba317317
an analysis of covert timing channels 
RANK = 12; score = 0.7166767120361328; correct = False; id = 539265193da35286d4f46497755dc9cc6d51387c
digital single lens reflex camera identification from traces of sensor dust digital single lens reflex cameras suffer from a well - known sensor dust problem due to interchangeable lenses that they deploy . the dust particles that settle in front of the imaging sensor create a persistent pattern in all captured images . in this paper , we propose a novel source camera identification method based on detection and matching of these dust - spot characteristics . dust spots in the image are detected based on a ( gaussian ) intensity loss model and shape properties . to prevent false detections , lens parameter - dependent characteristics of dust spots are also taken into consideration . experimental results show that the proposed detection scheme can be used in identification of the source digital single lens reflex camera at low false positive rates , even under heavy compression and downsampling .
RANK = 13; score = 0.7161873579025269; correct = False; id = 3f770cc7662340485f8fb328b3f2c95403a08e8d
alice in warningland : a large - scale field study of browser security warning effectiveness we empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature . we used mozilla firefox and google chrome ’s in - browser telemetry to observe over 25 million warning impressions in situ . during our field study , users continued through a tenth of mozilla firefox ’s malware and phishing warnings , a quarter of google chrome ’s malware and phishing warnings , and a third of mozilla firefox ’s ssl warnings . this demonstrates that security warnings can be effective in practice ; security experts and system architects should not dismiss the goal of communicating security information to end users . we also find that user behavior varies across warnings . in contrast to the other warnings , users continued through 70.2 % of google chrome ’s ssl warnings . this indicates that the user experience of a warning can have a significant impact on user behavior . based on our findings , we make recommendations for warning designers and researchers .
RANK = 14; score = 0.7160206437110901; correct = False; id = d0a3852f95e51a6b730df75e153d8446d6e8a90a
easeandroid : automatic policy analysis and refinement for security enhanced android via large - scale semi - supervised learning mandatory protection systems such as selinux and seandroid harden operating system integrity . unfortunately , policy development is error prone and requires lengthy refinement using audit logs from deployed systems . while prior work has studied selinux policy in detail , seandroid is relatively new and has received little attention . seandroid policy engineering differs significantly from selinux : android fundamentally differs from traditional linux ; the same policy is used on millions of devices for which new audit logs are continually available ; and audit logs contain a mix of benign and malicious accesses . in this paper , we propose easeandroid , the first seandroid analytic platform for automatic policy analysis and refinement . our key insight is that the policy refinement process can be modeled and automated using semi - supervised learning . given an existing policy and a small set of known access patterns , easeandroid continually expands the knowledge base as new audit logs become available , producing suggestions for policy refinement . we evaluate easeandroid on 1.3 million audit logs from real - world devices . easeandroid successfully learns 2,518 new access patterns and generates 331 new policy rules . during this process , easeandroid discovers eight categories of attack access patterns in real devices , two of which are new attacks directly against the seandroid mac mechanism .
RANK = 15; score = 0.7159317135810852; correct = False; id = 31e4845a40cfa6a953aef78387b34ea3284cdff9
all your biases belong to us : breaking rc4 in wpa - tkip and tls we present new biases in rc4 , break the wi - fi protected access temporal key integrity protocol ( wpa - tkip ) , and design a practical plaintext recovery attack against the transport layer security ( tls ) protocol . to empirically find new biases in the rc4 keystream we use statistical hypothesis tests . this reveals many new biases in the initial keystream bytes , as well as several new longterm biases . our fixed - plaintext recovery algorithms are capable of using multiple types of biases , and return a list of plaintext candidates in decreasing likelihood . to break wpa - tkip we introduce a method to generate a large number of identical packets . this packet is decrypted by generating its plaintext candidate list , and using redundant packet structure to prune bad candidates . from the decrypted packet we derive the tkip mic key , which can be used to inject and decrypt packets . in practice the attack can be executed within an hour . we also attack tls as used by https , where we show how to decrypt a secure cookie with a success rate of 94 % using 9 · 227 ciphertexts . this is done by injecting known data around the cookie , abusing this using mantin ’s absab bias , and brute - forcing the cookie by traversing the plaintext candidates . using our traffic generation technique , we are able to execute the attack in merely 75 hours .
RANK = 16; score = 0.7159095406532288; correct = False; id = 3b5c6faa99a454499e33d87cfaef9dfcb0d7b796
tapdance : end - to - middle anticensorship without flow blocking in response to increasingly sophisticated state - sponsored internet censorship , recent work has proposed a new approach to censorship resistance : end - to - middle proxying . this concept , developed in systems such as telex , decoy routing , and cirripede , moves anticensorship technology into the core of the network , at large isps outside the censoring country . in this paper , we focus on two technical obstacles to the deployment of certain end - to - middle schemes : the need to selectively block flows and the need to observe both directions of a connection . we propose a new construction , tapdance , that removes these requirements . tapdance employs a novel tcp - level technique that allows the anticensorship station at an isp to function as a passive network tap , without an inline blocking component . we also apply a novel steganographic encoding to embed control messages in tls ciphertext , allowing us to operate on https connections even under asymmetric routing . we implement and evaluate a tapdance prototype that demonstrates how the system could function with minimal impact on an isp ’s network operations .
RANK = 17; score = 0.7152328491210938; correct = False; id = 0ac40e64b2bf4a090ad76c6a5e54033f262ae4c2
random oracles are practical : a paradigm for designing efficient protocols we argue that the random oracle model — where all parties have access to a public random oracle — provides a bridge between cryptographic theory and cryptographic practice . in the paradigm we suggest , a practical protocol < italic > p</italic > is produced by first devising and proving correct a protocol < italic > p < supscrpt > r</supscrpt></italic > for the random oracle model , and then replacing oracle accesses by the computation of an “ appropriately chosen ” function < italic > h</italic>. this paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security . we illustrate these gains for problems including encryption , signatures , and zero - knowledge proofs .
RANK = 18; score = 0.7139666080474854; correct = True; id = 06f16d9430d5f6213cf5399b167a3d989c3ff798
practical mitigations for timing - based side - channel attacks on modern x86 processors this paper studies and evaluates the extent to which automated compiler techniques can defend against timing - based side - channel attacks on modern x86 processors . we study how modern x86 processors can leak timing information through side - channels that relate to control flow and data flow . to eliminate key - dependent control flow and key - dependent timing behavior related to control flow , we propose the use of if - conversion in a compiler backend , and evaluate a proof - of - concept prototype implementation . furthermore , we demonstrate two ways in which programs that lack key - dependent control flow and key- dependent cache behavior can still leak timing information on modern x86 implementations such as the intel core 2 duo , and propose defense mechanisms against them .
RANK = 19; score = 0.712963342666626; correct = False; id = 6470930ff36cde541c837bedcf17c20490fedbbc
@spam : the underground on 140 characters or less in this work we present a characterization of spam on twitter . we find that 8 % of 25 million urls posted to the site point to phishing , malware , and scams listed on popular blacklists . we analyze the accounts that send spam and find evidence that it originates from previously legitimate accounts that have been compromised and are now being puppeteered by spammers . using clickthrough data , we analyze spammers ' use of features unique to twitter and the degree that they affect the success of spam . we find that twitter is a highly successful platform for coercing users to visit spam pages , with a clickthrough rate of 0.13 % , compared to much lower rates previously reported for email spam . we group spam urls into campaigns and identify trends that uniquely distinguish phishing , malware , and spam , to gain an insight into the underlying techniques used to attract users . given the absence of spam filtering on twitter , we examine whether the use of url blacklists would help to significantly stem the spread of twitter spam . our results indicate that blacklists are too slow at identifying new threats , allowing more than 90 % of visitors to view a page before it becomes blacklisted . we also find that even if blacklist delays were reduced , the use by spammers of url shortening services for obfuscation negates the potential gains unless tools that use blacklists develop more sophisticated spam filtering .
RANK = 20; score = 0.7126163244247437; correct = False; id = 605ed83a6d1f4eaf995e85830f373923b11d6c13
cover your acks : pitfalls of covert channel censorship circumvention in response to increasingly sophisticated methods of blocking access to censorship circumvention schemes such as tor , recently proposed systems such as skypemorph , freewave , and censorspoofer have used voice and video conferencing protocols as " cover channels " to hide proxy connections . we demonstrate that even with perfect emulation of the cover channel , these systems can be vulnerable to attacks that detect or disrupt the covert communications while having no effect on legitimate cover traffic . our attacks stem from differences in the channel requirements for the cover protocols , which are peer - to - peer and loss tolerant , and the covert traffic , which is client - proxy and loss intolerant . these differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumvention schemes .

RANKING 1444
QUERY
decipherment of substitution ciphers with neural language models decipherment of homophonic substitution ciphers using language models ( lms ) is a wellstudied task in nlp . previous work in this topic scores short local spans of possible plaintext decipherments using n - gram lms . the most widely used technique is the use of beam search with n - gram lms proposed by nuhn et al . ( 2013 ) . we propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural lm . we augment beam search with a novel rest cost estimation that exploits the prediction power of a neural lm . we compare against the state of the art n - gram based methods on many different decipherment tasks . on challenging ciphers such as the beale cipher we provide significantly better error rates with much smaller beam sizes .
First cited at 65
TOP CITED PAPERS
RANK 65
attacking decipherment problems optimally with low - order n - gram models we introduce a method for solving substitution ciphers using low - order letter n - gram models . this method enforces global constraints using integer programming , and it guarantees that no decipherment key is overlooked . we carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n - gram order . we also make an empirical investigation of shannon ’s ( 1949 ) theory of uncertainty in decipherment .
RANK 118
bayesian inference for zodiac and other homophonic ciphers we introduce a novel bayesian approach for deciphering complex substitution ciphers . our method uses a decipherment model which combines information from letter n - gram language models as well as word dictionaries . bayesian inference is performed on our model using an efficient sampling technique . we evaluate the quality of the bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach , our method consistently produces almost 100 % accurate decipherments . the new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous zodiac-408 cipher in a fully automated fashion , which has never been done before .
RANK 350
unsupervised analysis for decipherment problems we study a number of natural language decipherment problems using unsupervised learning . these include letter substitution ciphers , character code conversion , phonetic decipherment , and word - based ciphers with relevance to machine translation . straightforward unsupervised learning techniques most often fail on the first try , so we describe techniques for understanding errors and significantly increasing performance .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8434784412384033; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.841202437877655; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8395501375198364; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8204824328422546; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.8184911608695984; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 6; score = 0.8161879181861877; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 7; score = 0.806745171546936; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.8047165274620056; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 9; score = 0.7999505400657654; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 10; score = 0.7992563843727112; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 11; score = 0.7980071902275085; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 12; score = 0.7948912978172302; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.7937282919883728; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 14; score = 0.7886601090431213; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 15; score = 0.7863409519195557; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 16; score = 0.7849004864692688; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 17; score = 0.7802011370658875; correct = False; id = 36918139c42c201557cef3dc69e26a06460155d8
impar : a deterministic algorithm for implicit semantic role labelling this paper presents a novel deterministic algorithm for implicit semantic role labeling . the system exploits a very simple but relevant discursive property , the argument coherence over different instances of a predicate . the algorithm solves the implicit arguments sequentially , exploiting not only explicit but also the implicit arguments previously solved . in addition , we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data .
RANK = 18; score = 0.7766847610473633; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 19; score = 0.7746270895004272; correct = False; id = de794d50713ea5f91a7c9da3d72041e2f5ef8452
the third pascal recognizing textual entailment challenge this paper presents the third pascal recognising textual entailment challenge ( rte-3 ) , providing an overview of the dataset creating methodology and the submitted systems . in creating this year ’s dataset , a number of longer texts were introduced to make the challenge more oriented to realistic scenarios . additionally , a pool of resources was offered so that the participants could share common tools . a pilot task was also set up , aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions . 26 participants submitted 44 runs , using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges . 1.1 the rte challenges the goal of the rte challenges has been to create a benchmark task dedicated to textual entailment – recognizing that the meaning of one text is entailed , i.e. can be inferred , by another . in the recent years , this task has raised great interest since applied semantic inference concerns many practical natural language processing ( nlp ) applications , such as question answering ( qa ) , information extraction ( ie ) , summarization , machine translation and paraphrasing , and certain types of queries in information retrieval ( ir ) . more specifically , the rte challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different nlp applications need to handle . for example , in addition to textual entailment , qa systems need to handle issues such as answer retrieval and question type recognition . by separating out the general problem of textual entailment from these task - specific problems , progress on semantic inference for many application areas can be promoted . hopefully , research on textual entailment will finally lead to the development of entailment “ engines ” , which can be used as a standard module in many applications ( similar to the role of part - of - speech taggers and syntactic parsers in current nlp applications ) . in the following sections , a detailed description of rte-3 is presented . after a quick review 1 the task was first defined by dagan and glickman ( 2004 ) .
RANK = 20; score = 0.7727134227752686; correct = False; id = d24fd1b77d139350031b6b3836b4ff70dae7c9b1
biased representation learning for domain adaptation representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains . we present a novel , formal statement of the representation learning task . we argue that because the task is computationally intractable in general , it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features . leveraging the posterior regularization framework , we develop an architecture for incorporating biases into representation learning . we investigate three types of biases , and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners , resulting in a relative reduction in error of more than 16 % for both tasks , with respect to existing state - of - the - art representation learning techniques .

RANKING 537
QUERY
linguistic cues to deception and perceived deception in interview dialogues we explore deception detection in interview dialogues . we analyze a set of linguistic features in both truthful and deceptive responses to interview questions . we also study the perception of deception , identifying characteristics of statements that are perceived as truthful or deceptive by interviewers . our analysis show significant differences between truthful and deceptive question responses , as well as variations in deception patterns across gender and native language . this analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech . our best classification performance is 72.74 f1-score ( about 27 % better than human performance ) , which is achieved using a combination of linguistic features and individual traits .
First cited at 30
TOP CITED PAPERS
RANK 30
finding deceptive opinion spam by any stretch of the imagination consumers increasingly rate , review and research products online ( jansen , 2010 ; litvin et al . , 2008 ) . consequently , websites containing consumer reviews are becoming targets of opinion spam . while recent work has focused primarily on manually identifiable instances of opinion spam , in this work we study deceptive opinion spam — fictitious opinions that have been deliberately written to sound authentic . integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold - standard opinion spam dataset . based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing .
RANK 105
syntactic stylometry for deception detection most previous studies in computerized deception detection have relied only on shallow lexico - syntactic patterns . this paper investigates syntactic stylometry for deception detection , adding a somewhat unconventional angle to prior literature . over four different datasets spanning from the product review to the essay domain , we demonstrate that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features . our results improve the best published result on the hotel review data ( ott et al . , 2011 ) reaching 91.2 % accuracy with 14 % error reduction .
RANK 151
the lie detector : explorations in the automatic recognition of deceptive language in this paper , we present initial experiments in the recognition of deceptive language . we introduce three data sets of true and lying texts collected for this purpose , and we show that automatic classification is a viable technique to distinguish between truth and falsehood as expressed in language . we also introduce a method for class - based feature analysis , which sheds some light on the features that are characteristic for deceptive text . you should not trust the devil , even if he tells the truth . – thomas of aquin ( medieval philosopher )
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK 3
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
TOP 20
RANK = 1; score = 0.8647229075431824; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.862263023853302; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 3; score = 0.8606698513031006; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 4; score = 0.8436260223388672; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 5; score = 0.8405433893203735; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 6; score = 0.8404297828674316; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.8270072340965271; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 8; score = 0.8260318040847778; correct = False; id = 9969c1b55d603654f0f7af0c6c771ba4a1e0396f
semeval-2015 task 15 : a cpa dictionary - entry - building task this paper describes the first semeval task to explore the use of natural language processing systems for building dictionary entries , in the framework of corpus pattern analysis . cpa is a corpus - driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used . task 15 draws on the pattern dictionary of english verbs ( www.pdev.org.uk ) , for the targeted lexical entries , and on the british national corpus for the input text . dictionary entry building is split into three subtasks which all start from the same concordance sample : 1 ) cpa parsing , where arguments and their syntactic and semantic categories have to be identified , 2 ) cpa clustering , in which sentences with similar patterns have to be clustered and 3 ) cpa automatic lexicography where the structure of patterns have to be constructed automatically . subtask 1 attracted 3 teams , though none could beat the baseline ( rule - based system ) . subtask 2 attracted 2 teams , one of which beat the baseline ( majority - class classifier ) . subtask 3 did not attract any participant . the task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences , and which is freely accessible .
RANK = 9; score = 0.825928807258606; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 10; score = 0.8217548131942749; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 11; score = 0.8206837773323059; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 12; score = 0.8142057657241821; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 13; score = 0.813421368598938; correct = False; id = 543f0a19638f45fc913baac86b70fc18dce03059
semeval-2013 task 7 : the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte - style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .
RANK = 14; score = 0.8098244667053223; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 15; score = 0.8097055554389954; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 16; score = 0.8088983297348022; correct = False; id = 110599f48c30251aba60f68b8484a7b0307bcb87
semeval-2015 task 11 : sentiment analysis of figurative language in twitter this report summarizes the objectives and evaluation of the semeval 2015 task on the sentiment analysis of figurative language on twitter ( task 11 ) . this is the first sentiment analysis task wholly dedicated to analyzing figurative language on twitter . specifically , three broad classes of figurative language are considered : irony , sarcasm and metaphor . gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform crowdflower . participating systems were required to provide a fine - grained sentiment score on an 11-point scale ( -5 to + 5 , including 0 for neutral intent ) for each tweet , and systems were evaluated against the gold standard using both a cosinesimilarity and a mean - squared - error measure .
RANK = 17; score = 0.8063452243804932; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 18; score = 0.8057736158370972; correct = False; id = 9300d37a6526fe9ffc4465608e86e1cd89f73add
semeval-2015 task 8 : spaceeval human languages exhibit a variety of strategies for communicating spatial information , including toponyms , spatial nominals , locations that are described in relation to other locations , and movements along paths . spaceeval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information . in this paper , we describe the spaceeval task , annotation schema , and corpora , and evaluate the performance of several supervised and semi - supervised machine learning systems developed with the goal of automating this task .
RANK = 19; score = 0.802788257598877; correct = False; id = 5467698bc7037a4733182d296c8c47ebddbe0341
semeval-2013 task 3 : spatial role labeling this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling . the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators . in addition to these major components , the links between them and the general - type of spatial relationships including region , direction and distance are targeted . the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark . we have one participant system with two runs . the participant ’s runs are compared to the system in ( kordjamshidi et al . , 2011c ) which is provided by task organizers .
RANK = 20; score = 0.7997024059295654; correct = False; id = 6ccf0c65e0350588285be4f34c46dc5f2e5a9ebf
semeval-2015 task 9 : clipeval implicit polarity of events sentiment analysis tends to focus on the polarity of words , combining their values to detect which portion of a text is opinionated . clipeval wants to promote a more holistic approach , looking at psychological researches that frame the connotations of words as the emotional values activated by them . the implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge .

RANKING 2622
QUERY
$ \alpha$ -trimmed weber representation and cross section asymmetrical coding for human identification using finger images in this paper , a novel method that utilizes feature - level fusion of finger vein ( fv ) and finger dorsal texture ( fdt ) images is proposed for human identification . motivated by weber ’s law , we present < inline - formula > < tex - math notation="latex">$\alpha $ < /tex - math></inline - formula>-trimmed weber representation ( < inline - formula > < tex - math notation="latex">$\alpha $ < /tex - math></inline - formula>-twr ) to enhance the foreground lines ( fls ) , i.e. , vessels underneath skin and line - like texture on skin . the proposed < inline - formula > < tex - math notation="latex">$\alpha $ < /tex - math></inline - formula>-twr is robust to illumination variation , as validated by a basic reflective and transmitted imaging model . cross section asymmetrical coding ( csac ) is performed to extract features for each pixel . the coding value contains discriminative information on the orientation and internal point location of the fls . the csac values of fv and fdt in each point are abreast in terms of binary representation . local density weighted matching is developed to obtain the matching score between two feature maps . we experimentally show that the proposed method outperforms other unimodal and multimodal identification methods in terms of equal - error - rate .
First cited at 18572
TOP CITED PAPERS
RANK 18572
importance of being unique from finger dorsal patterns : exploring minor finger knuckle patterns in verifying human identities automated biometrics identification using finger knuckle images has increasingly generated interest among researchers with emerging applications in human forensics and biometrics . prior efforts in the biometrics literature have only investigated the major finger knuckle patterns that are formed on the finger surface joining proximal phalanx and middle phalanx bones . this paper investigates the possible use of minor finger knuckle patterns , which are formed on the finger surface joining distal phalanx and middle phalanx bones . the minor finger knuckle patterns can either be used as independent biometric patterns or employed to improve the performance from the major finger knuckle patterns . a completely automated approach for the minor finger knuckle identification is developed with key steps for region of interest segmentation , image normalization , enhancement , and robust matching to accommodate image variations . this paper also introduces a new or first publicly available database for minor ( also major ) finger knuckle images from 503 different subjects . the efforts to develop an automated minor finger knuckle pattern matching scheme achieve promising results and illustrate its simultaneous use to significantly improve the performance over the conventional finger knuckle identification . several open questions on the stability and uniqueness of finger knuckle patterns should be addressed before knuckle pattern / image evidence can be admissible as supportive evidence in a court of law . therefore , this paper also presents a study on the stability of finger knuckle patterns from images acquired with an interval of 4 - 7 years . the experimental results and the images presented in this paper provide new insights on the finger knuckle pattern and identify the need for further work to exploit finger knuckle patterns in forensics and biometrics applications .
RANK 19093
personal authentication using finger knuckle surface this paper investigates a new approach for personal authentication using fingerback surface imaging . the texture pattern produced by the finger knuckle bending is highly unique and makes the surface a distinctive biometric identifier . the finger geometry features can be simultaneously acquired from the same image at the same time and integrated to further improve the user - identification accuracy of such a system . the fingerback surface images from each user are normalized to minimize the scale , translation , and rotational variations in the knuckle images . this paper details the development of such an approach using peg - free imaging . the experimental results from the proposed approach are promising and confirm the usefulness of such an approach for personal authentication .
TOP UNCITED PAPERS
RANK 1
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK 2
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK 3
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
TOP 20
RANK = 1; score = 0.7177287340164185; correct = False; id = 0825788b9b5a18e3dfea5b0af123b5e939a4f564
glove : global vectors for word representation recent methods for learning vector space representations of words have succeeded in capturing fine - grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . we analyze and make explicit the model properties needed for such regularities to emerge in word vectors . the result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . our model efficiently leverages statistical information by training only on the nonzero elements in a word - word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . the model produces a vector space with meaningful substructure , as evidenced by its performance of 75 % on a recent word analogy task . it also outperforms related models on similarity tasks and named entity recognition .
RANK = 2; score = 0.7112640738487244; correct = False; id = 8c36007c3e8217caa0bbb9ad1066dab51bb3eb22
semeval-2015 task 6 : clinical tempeval clinical tempeval 2015 brought the temporal information extraction tasks of past tempeval campaigns to the clinical domain . nine sub - tasks were included , covering problems in time expression identification , event expression identification and temporal relation identification . participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the mayo clinic , annotated with an extension of timeml for the clinical domain . three teams submitted a total of 13 system runs , with the best systems achieving near - human performance on identifying events and times , but with a large performance gap still remaining for temporal relations .
RANK = 3; score = 0.7108976244926453; correct = False; id = 9a7b75c6f1cbe0ac184cf5d598ef85070d52c327
semeval-2015 task 3 : answer selection in community question answering community question answering ( cqa ) provides new interesting research directions to the traditional question answering ( qa ) field , e.g. , the exploitation of the interaction between users and the structure of related posts . in this context , we organized semeval2015 task 3 on answer selection in cqa , which included two subtasks : ( a ) classifying answers as good , bad , or potentially relevant with respect to the question , and ( b ) answering a yes / no question with yes , no , or unsure , based on the list of all answers . we set subtask a for arabic and english on two relatively different cqa domains , i.e. , the qatar living website for english , and a quran - related website for arabic . we used crowdsourcing on amazon mechanical turk to label a large english training dataset , which we released to the research community . thirteen teams participated in the challenge with a total of 61 submissions : 24 primary and 37 contrastive . the best systems achieved an official score ( macro - averaged f1 ) of 57.19 and 63.7 for the english subtasks a and b , and 78.55 for the arabic subtask a.
RANK = 4; score = 0.7098265290260315; correct = False; id = 02df3d50dbd1d15c38db62ff58a5601ebf815d59
nltk : the natural language toolkit the natural language toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . nltk is written in python and distributed under the gpl open source license . over the past three years , nltk has become popular in teaching and research . we describe the toolkit and report on its current state of development .
RANK = 5; score = 0.7082282900810242; correct = False; id = e85a71c8cae795a1b2052a697d5e8182cc8c0655
the stanford corenlp natural language processing toolkit we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .
RANK = 6; score = 0.7049732208251953; correct = False; id = 967972821567b8a34dc058c9fbf60c4054dc3b69
a framework and graphical development environment for robust nlp tools and applications 
RANK = 7; score = 0.6995684504508972; correct = False; id = 11ea11aac43e91072b5b3aa1d3ed9db802199973
textrank : bringing order into text 
RANK = 8; score = 0.6977903842926025; correct = False; id = c4a3af54ae1c2c390cbd59c4efb2399328bdbe9d
smatch : an evaluation metric for semantic feature structures the evaluation of whole - sentence semantic structures plays an important role in semantic parsing and large - scale semantic structure annotation . however , there is no widely - used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter - annotator agreement study .
RANK = 9; score = 0.6976191997528076; correct = False; id = b050631bf7cef135805a32b5c09eef4038bd24f7
clpsych 2015 shared task : depression and ptsd on twitter this paper presents a summary of the computational linguistics and clinical psychology ( clpsych ) 2015 shared and unshared tasks . these tasks aimed to provide apples - to - apples comparisons of various approaches to modeling language relevant to mental health from social media . the data used for these tasks is from twitter users who state a diagnosis of depression or post traumatic stress disorder ( ptsd ) and demographically - matched community controls . the unshared task was a hackathon held at johns hopkins university in november 2014 to explore the data , and the shared task was conducted remotely , with each participating team submitted scores for a held - back test set of users . the shared task consisted of three binary classification experiments : ( 1 ) depression versus control , ( 2 ) ptsd versus control , and ( 3 ) depression versus ptsd . classifiers were compared primarily via their average precision , though a number of other metrics are used along with this to allow a more nuanced interpretation of the performance measures .
RANK = 10; score = 0.6955893039703369; correct = False; id = 15ae0badc584a287fc51e5de46d1ef51495a2398
finding deceptive opinion spam by any stretch of the imagination consumers increasingly rate , review and research products online ( jansen , 2010 ; litvin et al . , 2008 ) . consequently , websites containing consumer reviews are becoming targets of opinion spam . while recent work has focused primarily on manually identifiable instances of opinion spam , in this work we study deceptive opinion spam — fictitious opinions that have been deliberately written to sound authentic . integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold - standard opinion spam dataset . based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing .
RANK = 11; score = 0.6951403021812439; correct = False; id = 7bef4d04939a8553e4d424d98899153fe8786022
abstract meaning representation for sembanking meaning representation for sembanking
RANK = 12; score = 0.682050347328186; correct = False; id = 9ae62a9f18d2e29d2ad9b08e3418d6fd083d27d5
ppdb : the paraphrase database we present the 1.0 release of our paraphrase database , ppdb . its english portion , ppdb : eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning - preserving syntactic transformations . the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words . we also release ppdb : spa , a collection of 196 million spanish paraphrases . each paraphrase pair in ppdb contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n - grams and the annotated gigaword corpus . our release includes pruning tools that allow users to determine their own precision / recall tradeoff .
RANK = 13; score = 0.6809817552566528; correct = False; id = 36918139c42c201557cef3dc69e26a06460155d8
impar : a deterministic algorithm for implicit semantic role labelling this paper presents a novel deterministic algorithm for implicit semantic role labeling . the system exploits a very simple but relevant discursive property , the argument coherence over different instances of a predicate . the algorithm solves the implicit arguments sequentially , exploiting not only explicit but also the implicit arguments previously solved . in addition , we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data .
RANK = 14; score = 0.6729565262794495; correct = False; id = 68930f06c3444d00731fd01b6eb5f5c0b9cc6f1f
on the feasibility of internet - scale author identification we study techniques for identifying an anonymous author via linguistic stylometry , i.e. , comparing the writing style against a corpus of texts of known authorship . we experimentally demonstrate the effectiveness of our techniques with as many as 100,000 candidate authors . given the increasing availability of writing samples online , our result has serious implications for anonymity and free speech - an anonymous blogger or whistleblower may be unmasked unless they take steps to obfuscate their writing style . while there is a huge body of literature on authorship recognition based on writing style , almost none of it has studied corpora of more than a few hundred authors . the problem becomes qualitatively different at a large scale , as we show , and techniques from prior work fail to scale , both in terms of accuracy and performance . we study a variety of classifiers , both " lazy " and " eager , " and show how to handle the huge number of classes . we also develop novel techniques for confidence estimation of classifier outputs . finally , we demonstrate stylometric authorship recognition on texts written in different contexts . in over 20 % of cases , our classifiers can correctly identify an anonymous author given a corpus of texts from 100,000 authors ; in about 35 % of cases the correct author is one of the top 20 guesses . if we allow the classifier the option of not making a guess , via confidence estimation we are able to increase the precision of the top guess from 20 % to over 80 % with only a halving of recall .
RANK = 15; score = 0.6724919676780701; correct = False; id = 0d9d8be5ee0c1cda47beafea0ef0b14722cbd908
semeval-2013 task 1 : tempeval-3 : evaluating time expressions , events , and temporal relations within the semeval-2013 evaluation exercise , the tempeval-3 shared task aims to advance research on temporal information processing . it follows on from tempeval-1 and -2 , with : a three - part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems – in each task and in general . in this paper , we describe the participants’ approaches , results , and the observations from the results , which may guide future research in this area .
RANK = 16; score = 0.6712281703948975; correct = False; id = 283dedcdfa3e065146cb8649a7dd8a9ac6ab581d
instance weighting for domain adaptation in nlp domain adaptation is an important problem in natural language processing ( nlp ) due to the lack of labeled data in novel domains . in this paper , we study the domain adaptation problem from the instance weighting perspective . we formally analyze and characterize the domain adaptation problem from a distributional view , and show that there are two distinct needs for adaptation , corresponding to the different distributions of instances and classification functions in the source and the target domains . we then propose a general instance weighting framework for domain adaptation . our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
RANK = 17; score = 0.670100748538971; correct = False; id = 648c0a6d5023374c0c93fafb571b782da1dfbeed
the enemy in your own camp : how well can we detect statistically - generated fake reviews - an adversarial study . online reviews are a growing market , but it is struggling with fake reviews . they undermine both the value of reviews to the user , and their trust in the review sites . however , fake positive reviews can boost a business , and so a small industry producing fake reviews has developed . the two sides are facing an arms race that involves more and more natural language processing ( nlp ) . so far , nlp has been used mostly for detection , and works well on human - generated reviews . but what happens if nlp techniques are used to generate fake reviews as well ? we investigate the question in an adversarial setup , by assessing the detectability of different fake - review generation strategies . we use generative models to produce reviews based on meta - information , and evaluate their effectiveness against deceptiondetection models and human judges . we find that meta - information helps detection , but that nlp - generated reviews conditioned on such information are also much harder to detect than conventional ones .
RANK = 18; score = 0.6699925065040588; correct = False; id = 0e74425f7f95103127f691c73195a5bf2d47085e
semeval-2015 task 5 : qa tempeval - evaluating temporal information understanding with question answering qa tempeval shifts the goal of previous tempevals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering . this evaluation requires systems to capture temporal information relevant to perform an end - user task , as opposed to corpus - based evaluation where all temporal information is equally important . evaluation results show that the best automated timeml annotations reach over 30 % recall on questions with ‘ yes’ answer and about 50 % on easier questions with ‘ no’ answers . features that helped achieve better results are event coreference and a time expression reasoner .
RANK = 19; score = 0.6661046743392944; correct = False; id = de794d50713ea5f91a7c9da3d72041e2f5ef8452
the third pascal recognizing textual entailment challenge this paper presents the third pascal recognising textual entailment challenge ( rte-3 ) , providing an overview of the dataset creating methodology and the submitted systems . in creating this year ’s dataset , a number of longer texts were introduced to make the challenge more oriented to realistic scenarios . additionally , a pool of resources was offered so that the participants could share common tools . a pilot task was also set up , aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions . 26 participants submitted 44 runs , using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges . 1.1 the rte challenges the goal of the rte challenges has been to create a benchmark task dedicated to textual entailment – recognizing that the meaning of one text is entailed , i.e. can be inferred , by another . in the recent years , this task has raised great interest since applied semantic inference concerns many practical natural language processing ( nlp ) applications , such as question answering ( qa ) , information extraction ( ie ) , summarization , machine translation and paraphrasing , and certain types of queries in information retrieval ( ir ) . more specifically , the rte challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different nlp applications need to handle . for example , in addition to textual entailment , qa systems need to handle issues such as answer retrieval and question type recognition . by separating out the general problem of textual entailment from these task - specific problems , progress on semantic inference for many application areas can be promoted . hopefully , research on textual entailment will finally lead to the development of entailment “ engines ” , which can be used as a standard module in many applications ( similar to the role of part - of - speech taggers and syntactic parsers in current nlp applications ) . in the following sections , a detailed description of rte-3 is presented . after a quick review 1 the task was first defined by dagan and glickman ( 2004 ) .
RANK = 20; score = 0.6636393070220947; correct = False; id = 0eeba9d2cb597f4128f18f1a1354a56c1b9207c5
polarity sensitivity and evaluation order in type - logical grammar we present a novel , type - logical analysis of polarity sensitivity : how negative polarity items ( like any and ever ) or positive ones ( like some ) are licensed or prohibited . it takes not just scopal relations but also linear order into account , using the programming - language notions of delimited continuations and evaluation order , respectively . it thus achieves greater empirical coverage than previous proposals .

