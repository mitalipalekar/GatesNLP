RANKING 2408
QUERY
replication study : a cross - country field observation study of real world pin usage at atms and in various electronic payment scenarios in this paper , we describe the study we carried out to replicate and extend the field observation study of real world atm use carried out by de luca et al . , published at the soups conference in 2010 [ 10 ] . replicating de luca et al . ’s study , we observed pin shielding rates at atms in germany . we then extended their research by conducting a similar field observation study in sweden and the united kingdom . moreover , in addition to observing atm users ( withdrawing ) , we also observed electronic payment scenarios requiring pin entry . altogether , we gathered data related to 930 observations . similar to de luca et al . , we conducted follow - up interviews , the better to interpret our findings . we were able to confirm de luca et al . ’s findings with respect to low pin shielding incidence during atm cash withdrawals , with no significant differences between shielding rates across the three countries . pin shielding incidence during electronic payment scenarios was significantly lower than incidence during atm withdrawal scenarios in both the united kingdom and sweden . shielding levels in germany were similar during both withdrawal and payment scenarios . we conclude the paper by suggesting a number of explanations for the differences in shielding that our study revealed .
First cited at 4469
TOP CITED PAPERS
RANK 4469
users really do plug in usb drives they find we investigate the anecdotal belief that end users will pick up and plug in usb flash drives they find by completing a controlled experiment in which we drop 297 flash drives on a large university campus . we find that the attack is effective with an estimated success rate of 45 -- 98 % and expeditious with the first drive connected in less than six minutes . we analyze the types of drives users connected and survey those users to understand their motivation and security profile . we find that a drive 's appearance does not increase attack success . instead , users connect the drive with the altruistic intention of finding the owner . these individuals are not technically incompetent , but are rather typical community members who appear to take more recreational risks then their peers . we conclude with lessons learned and discussion on how social engineering attacks -- while less technical -- continue to be an effective attack vector that our community has yet to successfully address .
TOP UNCITED PAPERS
RANK 1
electronic payment technologies 
RANK 2
security notions and advanced method for human shoulder - surfing resistant pin - entry the personal identification number ( pin ) is a well - known authentication method used in various devices , such as atms , mobile devices , and electronic door locks . unfortunately , the conventional pin - entry method is vulnerable to shoulder - surfing attacks . consequently , various shoulder - surfing resistant methods have been proposed . however , the security analyses used to justify these proposed methods are not based on rigorous quantitative analysis , but instead on the results of experiments involving a limited number of human attackers . in this paper , we propose new theoretical and experimental techniques for quantitative security analysis of pin - entry methods . we first present new security notions and guidelines for secure pin - entry methods by analyzing the existing methods under the new framework . on the basis of these guidelines , we develop a new pin - entry method that effectively obviates human shoulder - surfing attacks by significantly increasing the amount of short - term memory required in an attack .
RANK 3
identification of pressed keys from mechanical vibrations this paper describes an attack that identifies the sequence of keystrokes analyzing mechanical vibrations generated by the act of pressing keys . we use accelerometers as vibration sensors . the apparatus necessary for this attack is inexpensive and can be unobtrusively embedded within the target equipment . we tested the proposed attack on an atm keypad and a pin - pad . we achieved the key recognition rates of 98.4 % in atm keypad , 76.7 % in pin - pad resting on a hard surface , and 82.1 % in pin - pad hold in hand .
TOP 20
RANK = 1; score = 0.22572421295299624; correct = False; id = 7e8e47e3529a570b25a200ee42ee14ad9fac3a87
electronic payment technologies 
RANK = 2; score = 0.2167217747573834; correct = False; id = d4bccfb26755d20e259758d31d319f85158c819c
security notions and advanced method for human shoulder - surfing resistant pin - entry the personal identification number ( pin ) is a well - known authentication method used in various devices , such as atms , mobile devices , and electronic door locks . unfortunately , the conventional pin - entry method is vulnerable to shoulder - surfing attacks . consequently , various shoulder - surfing resistant methods have been proposed . however , the security analyses used to justify these proposed methods are not based on rigorous quantitative analysis , but instead on the results of experiments involving a limited number of human attackers . in this paper , we propose new theoretical and experimental techniques for quantitative security analysis of pin - entry methods . we first present new security notions and guidelines for secure pin - entry methods by analyzing the existing methods under the new framework . on the basis of these guidelines , we develop a new pin - entry method that effectively obviates human shoulder - surfing attacks by significantly increasing the amount of short - term memory required in an attack .
RANK = 3; score = 0.21348040082893435; correct = False; id = 33bb3190c85785853c93f438a1480e2c6a77e248
identification of pressed keys from mechanical vibrations this paper describes an attack that identifies the sequence of keystrokes analyzing mechanical vibrations generated by the act of pressing keys . we use accelerometers as vibration sensors . the apparatus necessary for this attack is inexpensive and can be unobtrusively embedded within the target equipment . we tested the proposed attack on an atm keypad and a pin - pad . we achieved the key recognition rates of 98.4 % in atm keypad , 76.7 % in pin - pad resting on a hard surface , and 82.1 % in pin - pad hold in hand .
RANK = 4; score = 0.20491076472322528; correct = False; id = 54e2fbcd2c458091b2c227fc7e5ffeb48586bd61
chip and pin is broken emv is the dominant protocol used for smart card payments worldwide , with over 730 million cards in circulation . known to bank customers as “ chip and pin ” , it is used in europe ; it is being introduced in canada ; and there is pressure from banks to introduce it in the usa too . emv secures credit and debit card transactions by authenticating both the card and the customer presenting it through a combination of cryptographic authentication codes , digital signatures , and the entry of a pin . in this paper we describe and demonstrate a protocol flaw which allows criminals to use a genuine card to make a payment without knowing the card ’s pin , and to remain undetected even when the merchant has an online connection to the banking network . the fraudster performs a man - in - the - middle attack to trick the terminal into believing the pin verified correctly , while telling the card that no pin was entered at all . the paper considers how the flaws arose , why they remained unknown despite emv ’s wide deployment for the best part of a decade , and how they might be fixed . because we have found and validated a practical attack against the core functionality of emv , we conclude that the protocol is broken . this failure is significant in the field of protocol design , and also has important public policy implications , in light of growing reports of fraud on stolen emv cards . frequently , banks deny such fraud victims a refund , asserting that a card can not be used without the correct pin , and concluding that the customer must be grossly negligent or lying . our attack can explain a number of these cases , and exposes the need for further research to bridge the gap between the theoretical and practical security of bank payment systems . it also demonstrates the need for the next version of emv to be engineered properly .
RANK = 5; score = 0.20463858181151015; correct = False; id = ed6e02e85ce9d410467fb795abdb2ebbe837ef1f
a pin - entry method resilient against shoulder surfing magnetic stripe cards are in common use for electronic payments and cash withdrawal . reported incidents document that criminals easily pickpocket cards or skim them by swiping them through additional card readers . personal identification numbers ( pins ) are obtained by shoulder surfing , through the use of mirrors or concealed miniature cameras . both elements , the pin and the card , are generally sufficient to give the criminal full access to the victim 's account . in this paper , we present alternative pin entry methods to which we refer as cognitive trapdoor games . these methods make it significantly harder for a criminal to obtain pins even if he fully observes the entire input and output of a pin entry procedure . we also introduce the idea of probabilistic cognitive trapdoor games , which offer resilience to shoulder surfing even if the criminal records a pin entry procedure with a camera . we studied the security as well as the usability of our methods , the results of which we also present in the paper .
RANK = 6; score = 0.20208443592060138; correct = False; id = 733dab41cebc37815286fecbab465688ee328fee
securing classical ip over atm networks this paper discusses some security issues of clas sical ip over atm networks after analyzing new threats to ip networks based on atm secu rity mechanisms to protect these networks are intro duced the integration of rewalls into atm net works requires additional considerations we con clude that careful con guration of atm switches and atm services can provide some level of pro tection against spoo ng and denial of service at tacks our solutions are intended to be applied to current ip over atm networks and do not require any changes to these protocols or additions to cur rent switch capabilities
RANK = 7; score = 0.19634989733707567; correct = False; id = 071c7d7bf373d567b1314163f285915e80f6e482
securing atm networks in this paper we identify and address the challenges unique to providing a secure atm network . we analyse the network environment and consider the correct placement for a security mechanism , with particular to data transfer protection , in such an environment . we then introduce and describe a key - agile cryptographic device for atm networks . we present the technique to provide synchronisation , dynamic key change , dynamic initialisation vector change and message authentication code protection on atm data transfer . finally , we discuss the corresponding control functions for setting up of such a secure channel . a number of key exchange protocols are given as possible candidates for the establishment of the secure session and we describe the impact of these key exchange protocols on the design of an atm signalling protocol . an outline of our secure signalling and signalling security effort has also been presented .
RANK = 8; score = 0.19453947643947295; correct = False; id = 3121a5cfc495d4ee25ef70d367dbf09c42b6736b
design of a high - performance atm firewall a router - based packet - filtering firewall is an effective way of protecting an enterprise network from unauthorized access . however , it will not work efficiently in an atm network because it requires the termination of end - to - end atm connections at a packet - filtering router , which incurs huge overhead of sar ( segmentation and reassembly ) . very few approaches to this problem have been proposed in the literature , and none is completely satisfactory . in this paper we present the hardware design of a high - speed atm firewall that does not require the termination of an end - to - end connection in the middle . we propose a novel firewall design philosophy , called quality of firewalling ( qof ) , that applies security measures of different strength to traffic with different risk levels and show how it can be implemented in our firewall . compared with the traditional firewalls , this atm firewall performs exactly the same packet - level filtering without compromising the performance and has the same " look and feel " by sitting at the chokepoint between the trusted atm lan and untrusted atm wan . it is also easy to manage and flexible to use .
RANK = 9; score = 0.17958776318644465; correct = False; id = 482fd6d390161cf9fd31be5557b7ac916fece5c9
analysis and improvement of a pin - entry method resilient to shoulder - surfing and recording attacks devising a user authentication scheme based on personal identification numbers ( pins ) that is both secure and practically usable is a challenging problem . the greatest difficulty lies with the susceptibility of the pin entry process to direct observational attacks , such as human shoulder - surfing and camera - based recording . this paper starts with an examination of a previous attempt at solving the pin entry problem , which was based on an elegant adaptive black - and - white coloring of the 10-digit keypad in the standard layout . even though the method required uncomfortably many user inputs , it had the merit of being easy to understand and use . our analysis that takes both the experimental and theoretical approaches reveals multiple serious shortcomings of the previous method , including round redundancy , unbalanced key presses , highly frequent system errors , and insufficient resilience to recording attacks . the lessons learned through our analysis are then used to improve the black - and - white pin entry scheme . the new scheme has the remarkable property of resisting camera - based recording attacks over an unlimited number of authentication sessions without leaking any of the pin digits .
RANK = 10; score = 0.1784280551877307; correct = False; id = 7a288e8d507ce38d0c191dac32a30b03dcc27424
research on symmetric key - based mobile payment protocol security this paper presents a payment model in which the mobile businessman is seen as trusted third party . then we propose a new symmetric key - based mobile payment protocol and introduce both according key agreement protocol and payment protocol . in key agreement protocol , we introduce a hash function with key based on rpc to guarantee payment authorization security . the payment protocol achieves security properties of payment anonymity , transaction privacy , incontestability and so on .
RANK = 11; score = 0.15535521319949933; correct = False; id = 81765663b49323914effacf4f358d186a3e04b57
an efficient fair payment system many proposed payment systems allow the payer to remain anonymous during a transaction . however , this unconditional privacy protection could be misused by criminals , e.g. for blackmailing or money laundering . with a fair payment system , anonymous payments are still possible , but the anonymity can be removed with the help of a trusted party which need not be involved in the transaction itself . in this paper , we present an efficient fair payment system and we discuss its security .
RANK = 12; score = 0.155019530745032; correct = False; id = 08f9a62cdbe43fca7199147123a7d957892480af
chip and skim : cloning emv cards with the pre - play attack emv , also known as " chip and pin " , is the leading system for card payments worldwide . it is used throughout europe and much of asia , and is starting to be introduced in north america too . payment cards contain a chip so they can execute an authentication protocol . this protocol requires point - of - sale ( pos ) terminals or atms to generate a nonce , called the unpredictable number , for each transaction to ensure it is fresh . we have discovered two serious problems : a widespread implementation flaw and a deeper , more difficult to fix flaw with the emv protocol itself . the first flaw is that some emv implementers have merely used counters , timestamps or home - grown algorithms to supply this nonce . this exposes them to a " pre - play " attack which is indistinguishable from card cloning from the standpoint of the logs available to the card - issuing bank , and can be carried out even if it is impossible to clone a card physically . card cloning is the very type of fraud that emv was supposed to prevent . we describe how we detected the vulnerability , a survey methodology we developed to chart the scope of the weakness , evidence from atm and terminal experiments in the field , and our implementation of proof - of - concept attacks . we found flaws in widely - used atms from the largest manufacturers . we can now explain at least some of the increasing number of frauds in which victims are refused refunds by banks which claim that emv cards can not be cloned and that a customer involved in a dispute must therefore be mistaken or complicit . the second problem was exposed by the above work . independent of the random number quality , there is a protocol failure : the actual random number generated by the terminal can simply be replaced by one the attacker used earlier when capturing an authentication code from the card . this variant of the pre - play attack may be carried out by malware in an atm or pos terminal , or by a man - in - the - middle between the terminal and the acquirer . we explore the design and implementation mistakes that enabled these flaws to evade detection until now : shortcomings of the emv specification , of the emv kernel certification process , of implementation testing , formal analysis , and monitoring customer complaints . finally we discuss countermeasures . more than a year after our initial responsible disclosure of these flaws to the banks , action has only been taken to mitigate the first of them , while we have seen a likely case of the second in the wild , and the spread of atm and pos malware is making it ever more of a threat .
RANK = 13; score = 0.1512997795891569; correct = False; id = 9948598869473abdb3633fe17bd7965ac87479fd
seed - based de - anonymizability quantification of social networks in this paper , we implement the first comprehensive quantification of the perfect de - anonymizability and partial de - anonymizability of real - world social networks with seed information under general scenarios , which provides the theoretical foundation for the existing structure - based de - anonymization attacks and closes the gap between de - anonymization practice and theory . based on our quantification , we conduct a large - scale evaluation of the de - anonymizability of 24 real - world social networks by quantitatively showing the conditions for perfectly and partially de - anonymizing a social network , how de - anonymizable a social network is , and how many users of a social network can be successfully de - anonymized . furthermore , we show that both theoretically and experimentally , the overall structural information - based de - anonymization attack can be more powerful than the seed - based de - anonymization attack , and even without any seed information , a social network can be perfectly or partially de - anonymized . finally , we discuss the implications of this paper . our findings are expected to shed on research questions in the areas of structural data anonymization and de - anonymization and to help data owners evaluate their structural data vulnerability before data sharing and publishing .
RANK = 14; score = 0.15020566375379268; correct = False; id = c555abfe8649665ff0ebf249623dd4f1494c5d36
poster : wiping : wi - fi signal - based pin guessing attack this paper presents a new type of online password guessing attack called " wiping " ( wi - fi signal - based pin guessing attack ) to guess a victim 's pin ( personal identification number ) within a small number of unlock attempts . wiping uses wireless signal patterns identified from observing sequential finger movements involved in typing a pin to unlock a mobile device . a list of possible pin candidates is generated from the wireless signal patterns , and is used to improve performance of pin guessing attacks . we implemented a proof - of - concept attack to demonstrate the feasibility of wiping . our results showed that wiping could be practically effective : while pure guessing attacks failed to guess all 20 pins , wiping successfully guessed two pins .
RANK = 15; score = 0.12990098090036653; correct = False; id = 7c627f89c33dd6f55e07bc87cc3cb0004f16c9da
they can help : using crowdsourcing to improve the evaluation of grammatical error detection systems despite the rising interest in developing grammatical error detection systems for non - native speakers of english , progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers . in this paper we address these problems by presenting two evaluation methodologies , both based on a novel use of crowdsourcing . 1 motivation and contributions one of the fastest growing areas in need of nlp tools is the field of grammatical error detection for learners of english as a second language ( esl ) . according to guo and beckett ( 2007 ) , “ over a billion people speak english as their second or foreign language . ” this high demand has resulted in many nlp research papers on the topic , a synthesis series book ( leacock et al . , 2010 ) and a recurring workshop ( tetreault et al . , 2010a ) , all in the last five years . in this year ’s acl conference , there are four long papers devoted to this topic . despite the growing interest , two major factors encumber the growth of this subfield . first , the lack of consistent and appropriate score reporting is an issue . most work reports results in the form of precision and recall as measured against the judgment of a single human rater . this is problematic because most usage errors ( such as those in article and preposition usage ) are a matter of degree rather than simple rule violations such as number agreement . as a consequence , it is common for two native speakers to have different judgments of usage . therefore , an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner . second , systems are hardly ever compared to each other . in fact , to our knowledge , no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task — both commonly found in other nlp areas such as machine translation.1 for example , tetreault and chodorow ( 2008 ) , gamon et al . ( 2008 ) and felice and pulman ( 2008 ) developed preposition error detection systems , but evaluated on three different corpora using different evaluation measures . the goal of this paper is to address the above issues by using crowdsourcing , which has been proven effective for collecting multiple , reliable judgments in other nlp tasks : machine translation ( callison - burch , 2009 ; zaidan and callisonburch , 2010 ) , speech recognition ( evanini et al . , 2010 ; novotney and callison - burch , 2010 ) , automated paraphrase generation ( madnani , 2010 ) , anaphora resolution ( chamberlain et al . , 2009 ) , word sense disambiguation ( akkaya et al . , 2010 ) , lexicon construction for less commonly taught languages ( irvine and klementiev , 2010 ) , fact mining ( wang and callison - burch , 2010 ) and named entity recognition ( finin et al . , 2010 ) among several others . in particular , we make a significant contribution to the field by showing how to leverage crowdsourcthere has been a recent proposal for a related shared task ( dale and kilgarriff , 2010 ) that shows promise .
RANK = 16; score = 0.12704952340250375; correct = False; id = 6fc4a0b9984285091f8a9ca7f1a03f7c962996bf
timing attacks on pin input devices keypads are commonly used to enter personal identification numbers ( pin ) which are intended to authenticate a user based on what they know . a number of those keypads such as atm inputs and door keypads provide an audio feedback to the user for each button pressed . such audio feedback are observable from a modest distance . we are looking at quantifying the information leaking from delays between acoustic feedback pulses . preliminary experiments suggest that by using a hidden markov model , it might be possible to substantially narrow the search space . a subsequent brute force search on the reduced search space could be possible with- out triggering alerts , lockouts or other mechanisms design to thwart plain brute force attempts .
RANK = 17; score = 0.12520537844546947; correct = False; id = 42debc5ec28e3e1709a3005962f1dbc3120355be
introduction to credit networks : security , privacy , and applications credit networks model transitive ioweyou ( iou ) credit between their users . with their flexible - yet - scalable design and robustness against intrusion , we are observing a rapid increase in their popularity as a backbone of real - world permission - less payment settlement networks ( e.g. , ripple and stellar ) as well as several other weak - identity systems requiring sybil - tolerant communication . in payment scenarios , due to their unique capability to unite emerging crypto - currencies and user - defined currencies with the traditional fiat currency and banking systems , several existing and new payment enterprises are entering in this space . nevertheless , this enthusiasm in the market significantly exceeds our understanding of security , privacy , and reliability of these inherently distributed systems . currently employed ad hoc strategies to fix apparent flaws have made those systems vulnerable to bigger problems once they become lucrative targets for malicious players . in this tutorial , we first define the concept of iou credit networks , and describe some of the important credit network applications . we then describe and analyze recent and ongoing projects to improve the credit - network security , privacy and reliability . we end our discussion with interesting open problems and systems challenges in the field . this introductory tutorial is accessible to the standard ccs audience with graduate - level security knowledge .
RANK = 18; score = 0.12206673736090591; correct = False; id = 9725a56f4578462f96cc3ec54c4577ba476f0aad
structural non - correspondence in translation kaplan et al ( 1989 ) present an approach to machine translation based on co - description . in this paper we show that the notation is not as natural and expressive as it appears . we first show that the most natural analysis proposed in kaplan et al ( 1989 ) can not in fact cover the range of data for the important translational phenomenon in question . this contribution extends the work reported on in sadler et al ( 1989 ) and sadler et al ( 1990 ) . we then go on to discuss alternatives which depart from or extend the formalism proposed in kaplan et al ( 1989 ) in various respects , pointing out some directions for further research . the strategies discussed have been implemented .
RANK = 19; score = 0.1203549862808079; correct = False; id = d26456b0df6f7677873791077b68f92d504c6d02
pairwise neural machine translation evaluation • model : however , in that work we used convolution kernels , which is computationally expensive and does not scale well to large datasets and complex structures such as graphs and enriched trees . this inefficiency arises both at training and testing time . thus , here we use neural embeddings and multilayer neural networks , which yields an efficient learning framework that works significantly better on the same datasets ( although we are not using exactly the same information for learning ) . to the best of our knowledge , the application of structured neural embeddings and a neural network learning architecture for mt evaluation is completely novel . this is despite the growing interest in recent years for deep neural nets ( nns ) and word embeddings with application to a myriad of nlp problems . for example , in smt we have observed an increased use of neural nets for language modeling ( bengio et al . , 2003 ; mikolov et al . , 2010 ) as well as for improving the translation mod l ( d vlin et al . , 2014 ; sutskever et al . , 2014 ) . deep learning has spread beyond language modeling . for example , recursive nns have been used for syntactic parsing ( socher et al . , 2013a ) and sentiment analysis ( socher et al . , 2013b ) . the increased use of nns by the nlp community is in part due to ( i ) the emergence of tools such as word2vec ( mikolov et al . , 2013a ) and glove ( pennington et al . , 2014 ) , which have enabled nlp researchers to learn word embeddings , and ( ii ) unified learning frameworks , e.g. , ( collobert et al . , 2011 ) , which cover a variety of nlp tasks such as part - of - speech tagging , chunking , named entity recognition , and semantic role labeling . while in this work we make use of widely available pre - computed structured embeddings , the novelty of our work goes beyond the type of information considered as input , and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about mt evaluation .
RANK = 20; score = 0.11842692014996324; correct = False; id = 86a0b0fb00b6f062595d21431a31ff8a07978963
incremental speech understanding in a multi - party virtual human dialogue system this demonstration highlights some emerging capabilities for incremental speech understanding and processing in virtual human dialogue systems . this work is part of an ongoing effort that aims to enable realistic spoken dialogue with virtual humans in multi - party negotiation scenarios ( plüss et al . , 2011 ; traum et al . , 2008b ) . these scenarios are designed to allow trainees to practice their negotiation skills by engaging in face - to - face spoken negotiation with one or more virtual humans . an important component in achieving naturalistic behavior in these negotiation scenarios , which ideally should have the virtual humans demonstrating fluid turn - taking , complex reasoning , and responding to factors like trust and emotions , is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech , as the users are speaking ( devault et al . , 2011b ) . these responses could range from relatively straightforward turn management behaviors , like having a virtual human recognize when it is being addressed by a user utterance , and possibly turn to look at the user who has started speaking , to more complex responses such as emotional reactions to the content of what users are saying . the current demonstration extends our previous demonstration of incremental processing ( sagae et al . , 2010 ) in several important respects . first , it includes additional indicators , as described in ( devault et al . , 2011a ) . second , it is applied to a new domain , an extension of that presented in ( plüss et al . , 2011 ) . finally , it is integrated with the dialogue figure 1 : saso negotiation in the saloon : utah ( left ) looking at harmony ( right ) .

RANKING 2510
QUERY
cloud - based approximate constrained shortest distance queries over encrypted graphs with privacy protection constrained shortest distance ( csd ) querying is one of the fundamental graph query primitives , which finds the shortest distance from an origin to a destination in a graph with a constraint that the total cost does not exceed a given threshold . csd querying has a wide range of applications , such as routing in telecommunications and transportation . with an increasing prevalence of cloud computing paradigm , graph owners desire to outsource their graphs to cloud servers . in order to protect sensitive information , these graphs are usually encrypted before being outsourced to the cloud . this , however , imposes a great challenge to csd querying over encrypted graphs . since performing constraint filtering is an intractable task , existing work mainly focuses on unconstrained shortest distance queries . csd querying over encrypted graphs remains an open research problem . in this paper , we propose connor , a novel graph encryption scheme that enables approximate csd querying . connor is built based on an efficient , tree - based ciphertext comparison protocol , and makes use of symmetric - key primitives and the somewhat homomorphic encryption , making it computationally efficient . using connor , a graph owner can first encrypt privacy - sensitive graphs and then outsource them to the cloud server , achieving the necessary privacy without losing the ability of querying . extensive experiments with real - world data sets demonstrate the effectiveness and efficiency of the proposed graph encryption scheme .
First cited at 1
TOP CITED PAPERS
RANK 1
grecs : graph encryption for approximate shortest distance queries we propose graph encryption schemes that efficiently support approximate shortest distance queries on large - scale encrypted graphs . shortest distance queries are one of the most fundamental graph operations and have a wide range of applications . using such graph encryption schemes , a client can outsource large - scale privacy - sensitive graphs to an untrusted server without losing the ability to query it . other applications include encrypted graph databases and controlled disclosure systems . we propose grecs ( stands for graph encryption for approximate shortest distance queries ) which includes three oracle encryption schemes that are provably secure against any semi - honest server . our first construction makes use of only symmetric - key operations , resulting in a computationally - efficient construction . our second scheme makes use of somewhat - homomorphic encryption and is less computationally - efficient but achieves optimal communication complexity ( i.e. uses a minimal amount of bandwidth ) . finally , our third scheme is both computationally - efficient and achieves optimal communication complexity at the cost of a small amount of additional leakage . we implemented and evaluated the efficiency of our constructions experimentally . the experiments demonstrate that our schemes are efficient and can be applied to graphs that scale up to 1.6 million nodes and 11 million edges .
RANK 42
inference attacks on property - preserving encrypted databases many encrypted database ( edb ) systems have been proposed in the last few years as cloud computing has grown in popularity and data breaches have increased . the state - of - the - art edb systems for relational databases can handle sql queries over encrypted data and are competitive with commercial database systems . these systems , most of which are based on the design of cryptdb ( sosp 2011 ) , achieve these properties by making use of property - preserving encryption schemes such as deterministic ( dte ) and order- preserving encryption ( ope ) . in this paper , we study the concrete security provided by such systems . we present a series of attacks that recover the plaintext from dte- and ope - encrypted database columns using only the encrypted column and publicly - available auxiliary information . we consider well - known attacks , including frequency analysis and sorting , as well as new attacks based on combinatorial optimization . we evaluate these attacks empirically in an electronic medical records ( emr ) scenario using real patient data from 200 u.s. hospitals . when the encrypted database is operating in a steady - state where enough encryption layers have been peeled to permit the application to run its queries , our experimental results show that an alarming amount of sensitive information can be recovered . in particular , our attacks correctly recovered certain ope - encrypted attributes ( e.g. , age and disease severity ) for more than 80 % of the patient records from 95 % of the hospitals ; and certain dte- encrypted attributes ( e.g. , sex , race , and mortality risk ) for more than 60 % of the patient records from more than 60 % of the hospitals .
RANK 121
dynamic searchable symmetric encryption searchable symmetric encryption ( sse ) allows a client to encrypt its data in such a way that this data can still be searched . the most immediate application of sse is to cloud storage , where it enables a client to securely outsource its data to an untrusted cloud provider without sacrificing the ability to search over it . sse has been the focus of active research and a multitude of schemes that achieve various levels of security and efficiency have been proposed . any practical sse scheme , however , should ( at a minimum ) satisfy the following properties : sublinear search time , security against adaptive chosen - keyword attacks , compact indexes and the ability to add and delete files efficiently . unfortunately , none of the previously - known sse constructions achieve all these properties at the same time . this severely limits the practical value of sse and decreases its chance of deployment in real - world cloud storage systems . to address this , we propose the first sse scheme to satisfy all the properties outlined above . our construction extends the inverted index approach ( curtmola et al . , ccs 2006 ) in several non - trivial ways and introduces new techniques for the design of sse . in addition , we implement our scheme and conduct a performance evaluation , showing that our approach is highly efficient and ready for deployment .
TOP UNCITED PAPERS
RANK 2
word graphs for statistical machine translation word graphs have various applications in the field of machine translation . therefore it is important for machine translation systems to produce compact word graphs of high quality . we will describe the generation of word graphs for state of the art phrase - based statistical machine translation . we will use these word graph to provide an analysis of the search process . we will evaluate the quality of the word graphs using the well - known graph word error rate . additionally , we introduce the two novel graph - to - string criteria : the position - independent graph word error rate and the graph bleu score . experimental results are presented for two chinese – english tasks : the small iwslt task and the nist large data track task . for both tasks , we achieve significant reductions of the graph error rate already with compact word graphs .
RANK 3
sedic : privacy - aware data intensive computing on hybrid clouds the emergence of cost - effective cloud services offers organizations great opportunity to reduce their cost and increase productivity . this development , however , is hampered by privacy concerns : a significant amount of organizational computing workload at least partially involves sensitive data and therefore can not be directly outsourced to the public cloud . the scale of these computing tasks also renders existing secure outsourcing techniques less applicable . a natural solution is to split a task , keeping the computation on the private data within an organization 's private cloud while moving the rest to the public commercial cloud . however , this hybrid cloud computing is not supported by today 's data - intensive computing frameworks , mapreduce in particular , which forces the users to manually split their computing tasks . in this paper , we present a suite of new techniques that make such privacy - aware data - intensive computing possible . our system , called sedic , leverages the special features of mapreduce to automatically partition a computing job according to the security levels of the data it works on , and arrange the computation across a hybrid cloud . specifically , we modified mapreduce 's distributed file system to strategically replicate data , moving sanitized data blocks to the public cloud . over this data placement , map tasks are carefully scheduled to outsource as much workload to the public cloud as possible , given sensitive data always stay on the private cloud . to minimize inter - cloud communication , our approach also automatically analyzes and transforms the reduction structure of a submitted job to aggregate the map outcomes within the public cloud before sending the result back to the private cloud for the final reduction . this also allows the users to interact with our system in the same way they work with mapreduce , and directly run their legacy code in our framework . we implemented sedic on hadoop and evaluated it using both real and synthesized computing jobs on a large - scale cloud test - bed . the study shows that our techniques effectively protect sensitive user data , offload a large amount of computation to the public cloud and also fully preserve the scalability of mapreduce .
RANK 4
hierarchical attribute - based encryption for fine - grained access control in cloud storage services cloud computing , as an emerging computing paradigm , enables users to remotely store their data into a cloud so as to enjoy scalable services on - demand . especially for small and medium - sized enterprises with limited budgets , they can achieve cost savings and productivity enhancements by using cloud - based services to manage projects , to make collaborations , and the like . however , allowing cloud service providers ( csps ) , which are not in the same trusted domains as enterprise users , to take care of confidential data , may raise potential security and privacy issues . to keep the sensitive user data confidential against untrusted csps , a natural way is to apply cryptographic approaches , by disclosing decryption keys only to authorized users . however , when enterprise users outsource confidential data for sharing on cloud servers , the adopted encryption system should not only support fine - grained access control , but also provide high performance , full delegation , and scalability , so as to best serve the needs of accessing data anytime and anywhere , delegating within enterprises , and achieving a dynamic set of users . in this paper , we propose a scheme to help enterprises to efficiently share confidential data on cloud servers . we achieve this goal by first combining the hierarchical identity - based encryption ( hibe ) system and the ciphertext - policy attribute - based encryption ( cp - abe ) system , and then making a performance - expressivity tradeoff , finally applying proxy re - encryption and lazy re - encryption to our scheme .
TOP 20
RANK = 1; score = 0.5237355039287438; correct = True; id = 1f6cbdee0cd99b74ab2a8ffb381265286a11ea90
grecs : graph encryption for approximate shortest distance queries we propose graph encryption schemes that efficiently support approximate shortest distance queries on large - scale encrypted graphs . shortest distance queries are one of the most fundamental graph operations and have a wide range of applications . using such graph encryption schemes , a client can outsource large - scale privacy - sensitive graphs to an untrusted server without losing the ability to query it . other applications include encrypted graph databases and controlled disclosure systems . we propose grecs ( stands for graph encryption for approximate shortest distance queries ) which includes three oracle encryption schemes that are provably secure against any semi - honest server . our first construction makes use of only symmetric - key operations , resulting in a computationally - efficient construction . our second scheme makes use of somewhat - homomorphic encryption and is less computationally - efficient but achieves optimal communication complexity ( i.e. uses a minimal amount of bandwidth ) . finally , our third scheme is both computationally - efficient and achieves optimal communication complexity at the cost of a small amount of additional leakage . we implemented and evaluated the efficiency of our constructions experimentally . the experiments demonstrate that our schemes are efficient and can be applied to graphs that scale up to 1.6 million nodes and 11 million edges .
RANK = 2; score = 0.3069041665869233; correct = False; id = 27b6985d455ae63ef1042fec2a7c8e79a75954d0
word graphs for statistical machine translation word graphs have various applications in the field of machine translation . therefore it is important for machine translation systems to produce compact word graphs of high quality . we will describe the generation of word graphs for state of the art phrase - based statistical machine translation . we will use these word graph to provide an analysis of the search process . we will evaluate the quality of the word graphs using the well - known graph word error rate . additionally , we introduce the two novel graph - to - string criteria : the position - independent graph word error rate and the graph bleu score . experimental results are presented for two chinese – english tasks : the small iwslt task and the nist large data track task . for both tasks , we achieve significant reductions of the graph error rate already with compact word graphs .
RANK = 3; score = 0.27081861440957516; correct = False; id = 0dcd19dc6ff699d183e3326d31b6e730ddf73219
sedic : privacy - aware data intensive computing on hybrid clouds the emergence of cost - effective cloud services offers organizations great opportunity to reduce their cost and increase productivity . this development , however , is hampered by privacy concerns : a significant amount of organizational computing workload at least partially involves sensitive data and therefore can not be directly outsourced to the public cloud . the scale of these computing tasks also renders existing secure outsourcing techniques less applicable . a natural solution is to split a task , keeping the computation on the private data within an organization 's private cloud while moving the rest to the public commercial cloud . however , this hybrid cloud computing is not supported by today 's data - intensive computing frameworks , mapreduce in particular , which forces the users to manually split their computing tasks . in this paper , we present a suite of new techniques that make such privacy - aware data - intensive computing possible . our system , called sedic , leverages the special features of mapreduce to automatically partition a computing job according to the security levels of the data it works on , and arrange the computation across a hybrid cloud . specifically , we modified mapreduce 's distributed file system to strategically replicate data , moving sanitized data blocks to the public cloud . over this data placement , map tasks are carefully scheduled to outsource as much workload to the public cloud as possible , given sensitive data always stay on the private cloud . to minimize inter - cloud communication , our approach also automatically analyzes and transforms the reduction structure of a submitted job to aggregate the map outcomes within the public cloud before sending the result back to the private cloud for the final reduction . this also allows the users to interact with our system in the same way they work with mapreduce , and directly run their legacy code in our framework . we implemented sedic on hadoop and evaluated it using both real and synthesized computing jobs on a large - scale cloud test - bed . the study shows that our techniques effectively protect sensitive user data , offload a large amount of computation to the public cloud and also fully preserve the scalability of mapreduce .
RANK = 4; score = 0.2605634538492105; correct = False; id = 5be9d57e78a77d8fe904168c0e95976f5fbc7761
hierarchical attribute - based encryption for fine - grained access control in cloud storage services cloud computing , as an emerging computing paradigm , enables users to remotely store their data into a cloud so as to enjoy scalable services on - demand . especially for small and medium - sized enterprises with limited budgets , they can achieve cost savings and productivity enhancements by using cloud - based services to manage projects , to make collaborations , and the like . however , allowing cloud service providers ( csps ) , which are not in the same trusted domains as enterprise users , to take care of confidential data , may raise potential security and privacy issues . to keep the sensitive user data confidential against untrusted csps , a natural way is to apply cryptographic approaches , by disclosing decryption keys only to authorized users . however , when enterprise users outsource confidential data for sharing on cloud servers , the adopted encryption system should not only support fine - grained access control , but also provide high performance , full delegation , and scalability , so as to best serve the needs of accessing data anytime and anywhere , delegating within enterprises , and achieving a dynamic set of users . in this paper , we propose a scheme to help enterprises to efficiently share confidential data on cloud servers . we achieve this goal by first combining the hierarchical identity - based encryption ( hibe ) system and the ciphertext - policy attribute - based encryption ( cp - abe ) system , and then making a performance - expressivity tradeoff , finally applying proxy re - encryption and lazy re - encryption to our scheme .
RANK = 5; score = 0.2578214395660786; correct = False; id = 0c2df9ca8a83a5c24aff3d9090f56de046d0fe79
graph - based unsupervised learning of word similarities using heterogeneous feature types in this work , we propose a graph - based approach to computing similarities between words in an unsupervised manner , and take advantage of heterogeneous feature types in the process . the approach is based on the creation of two separate graphs , one for words and one for features of different types ( alignmentbased , orthographic , etc . ) . the graphs are connected through edges that link nodes in the feature graph to nodes in the word graph , the edge weights representing the importance of a particular feature for a particular word . high quality graphs are learned during training , and the proposed method outperforms experimental baselines .
RANK = 6; score = 0.25206231688644365; correct = False; id = 4f25c3365b6db20f8d3a4aa210a74a65d8deea31
privacy - preserving and regular language search over encrypted cloud data using cloud - based storage service , users can remotely store their data to clouds but also enjoy the high quality data retrieval services , without the tedious and cumbersome local data storage and maintenance . however , the sole storage service can not satisfy all desirable requirements of users . over the last decade , privacy - preserving search over encrypted cloud data has been a meaningful and practical research topic for outsourced data security . the fact of remote cloud storage service that users can not have full physical possession of their data makes the privacy data search a formidable mission . a naive solution is to delegate a trusted party to access the stored data and fulfill a search task . this , nevertheless , does not scale well in practice as the fully data access may easily yield harm for user privacy . to securely introduce an effective solution , we should guarantee the privacy of search contents , i.e. , what a user wants to search , and return results , i.e. , what a server returns to the user . furthermore , we also need to guarantee privacy for the outsourced data , and bring no additional local search burden to user . in this paper , we design a novel privacy - preserving functional encryption - based search mechanism over encrypted cloud data . a major advantage of our new primitive compared with the existing public key based search systems is that it supports an extreme expressive search mode , regular language search . our security and performance analysis show that the proposed system is provably secure and more efficient than some searchable systems with high expressiveness .
RANK = 7; score = 0.2506659193338383; correct = False; id = 530415adb29efa65bf8bbe797cba3019ccaa67c6
geometric range search on encrypted spatial data geometric range search is a fundamental primitive for spatial data analysis in sql and nosql databases . it has extensive applications in location - based services , computer - aided design , and computational geometry . due to the dramatic increase in data size , it is necessary for companies and organizations to outsource their spatial data sets to third - party cloud services ( e.g. , amazon ) in order to reduce storage and query processing costs , but , meanwhile , with the promise of no privacy leakage to the third party . searchable encryption is a technique to perform meaningful queries on encrypted data without revealing privacy . however , geometric range search on spatial data has not been fully investigated nor supported by existing searchable encryption schemes . in this paper , we design a symmetric - key searchable encryption scheme that can support geometric range queries on encrypted spatial data . one of our major contributions is that our design is a general approach , which can support different types of geometric range queries . in other words , our design on encrypted data is independent from the shapes of geometric range queries . moreover , we further extend our scheme with the additional use of tree structures to achieve search complexity that is faster than linear . we formally define and prove the security of our scheme with indistinguishability under selective chosen - plaintext attacks , and demonstrate the performance of our scheme with experiments in a real cloud platform ( amazon ec2 ) .
RANK = 8; score = 0.24394875341363728; correct = False; id = 39c995e061304233e8441e76b7ba8f28e5080cca
achieving secure role - based access control on encrypted data in cloud storage with the rapid developments occurring in cloud computing and services , there has been a growing trend to use the cloud for large - scale data storage . this has raised the important security issue of how to control and prevent unauthorized access to data stored in the cloud . one well known access control model is the role - based access control ( rbac ) , which provides flexible controls and management by having two mappings , users to roles and roles to privileges on data objects . in this paper , we propose a role - based encryption ( rbe ) scheme that integrates the cryptographic techniques with rbac . our rbe scheme allows rbac policies to be enforced for the encrypted data stored in public clouds . based on the proposed scheme , we present a secure rbe - based hybrid cloud storage architecture that allows an organization to store data securely in a public cloud , while maintaining the sensitive information related to the organization 's structure in a private cloud . we describe a practical implementation of the proposed rbe - based architecture and discuss the performance results . we demonstrate that users only need to keep a single key for decryption , and system operations are efficient regardless of the complexity of the role hierarchy and user membership in the system .
RANK = 9; score = 0.2396840768167092; correct = False; id = ca1bdc37b64833344c59fd2fe69b423152f1a4c9
an efficient file hierarchy attribute - based encryption scheme in cloud computing ciphertext - policy attribute - based encryption ( cp - abe ) has been a preferred encryption technology to solve the challenging problem of secure data sharing in cloud computing . the shared data files generally have the characteristic of multilevel hierarchy , particularly in the area of healthcare and the military . however , the hierarchy structure of shared files has not been explored in cp - abe . in this paper , an efficient file hierarchy attribute - based encryption scheme is proposed in cloud computing . the layered access structures are integrated into a single access structure , and then , the hierarchical files are encrypted with the integrated access structure . the ciphertext components related to attributes could be shared by the files . therefore , both ciphertext storage and time cost of encryption are saved . moreover , the proposed scheme is proved to be secure under the standard assumption . experimental simulation shows that the proposed scheme is highly efficient in terms of encryption and decryption . with the number of the files increasing , the advantages of our scheme become more and more conspicuous .
RANK = 10; score = 0.23717404338418918; correct = False; id = ec2b17d6843730367746ae1c3196b5f629ead8aa
optimized search - and - compute circuits and their application to query evaluation on encrypted data private query processing on encrypted databases allows users to obtain data from encrypted databases in such a way that the users ' sensitive data will be protected from exposure . given an encrypted database , users typically submit queries similar to the following examples : 1 ) how many employees in an organization make over u.s. $ 100000 ? 2 ) what is the average age of factory workers suffering from leukemia ? answering the questions requires one to search and then compute over the relevant encrypted data sets in sequence . in this paper , we are interested in efficiently processing queries that require both operations to be performed on fully encrypted databases . one immediate solution is to use several special - purpose encryption schemes simultaneously ; however , this approach is associated with a high computational cost for maintaining multiple encryption contexts . another solution is to use a privacy homomorphic scheme . however , no secure solutions have been developed that satisfy the efficiency requirements . in this paper , we construct a unified framework to efficiently and privately process queries with search and compute operations . for this purpose , the first part of our work involves devising several underlying circuits as primitives for queries on encrypted data . second , we apply two optimization techniques to improve the efficiency of these circuit primitives . one technique involves exploiting single - instruction - multiple - data ( simd ) techniques to accelerate the basic circuit operations . unlike general simd approaches , our simd implementation can be applied even to a single basic operation . the other technique is to use a large integer ring ( e.g. , z2 t ) as a message space rather than a binary field . even for an integer of k bits with k > t , addition can be performed using degree 1 circuits with lazy carry operations . finally , we present various experiments performed by varying the considered parameters , such as the query type and the number of tuples .
RANK = 11; score = 0.23578366341309231; correct = False; id = 1f0324eec1c81061a3db587335f0039a115dec29
query encrypted databases practically the cloud database services are attractive for managing outsourced databases . however , the data security and privacy is a big concern hampering the acceptance of cloud database services . a straightforward way to address this concern is to encrypt the database , but an encrypted database can not be easily queried . in this demo paper , we demonstrate that aggregate sql queries with range conditions can be performed efficiently over encrypted databases , without decrypting the databases first , by using our new homomorphic encryption scheme . the techniques in this paper can be applied to existing database management systems ( dbmss ) . moreover , the techniques do not need to predetermine the maximum sum and number of data in one database table column . these features make our technologies suitable to manage long - standing and large encrypted databases .
RANK = 12; score = 0.23399147557983552; correct = False; id = 6696d6d15849ccfce6455fa0c27220664eca6fd5
sets : scalable and efficient tree search in dependency graphs we present a syntactic analysis query toolkit geared specifically towards massive dependency parsebanks and morphologically rich languages . the query language allows arbitrary tree queries , including negated branches , and is suitable for querying analyses with rich morphological annotation . treebanks of over a million words can be comfortably queried on a low - end netbook , and a parsebank with over 100 m words on a single consumer - grade server . we also introduce a web - based interface for interactive querying . all contributions are available under open licenses .
RANK = 13; score = 0.23369519503011366; correct = False; id = 5812e18fd4373c81551a2142476412ee3290b813
equivalence - based security for querying encrypted databases : theory and application to privacy policy audits to reduce costs , organizations may outsource data storage and data processing to third - party clouds . this raises confidentiality concerns , since the outsourced data may have sensitive information . although semantically secure encryption of the data prior to outsourcing alleviates these concerns , it also renders the outsourced data useless for any relational processing . motivated by this problem , we present two database encryption schemes that reveal just enough information about structured data to support a wide - range of relational queries . our main contribution is a definition and proof of security for the two schemes . this definition captures confidentiality offered by the schemes using a novel notion of equivalence of databases from the adversary 's perspective . as a specific application , we adapt an existing algorithm for finding violations of a rich class of privacy policies to run on logs encrypted under our schemes and observe low to moderate overheads .
RANK = 14; score = 0.22946658714884519; correct = False; id = 92f94084ca8a99303cfa3913a148e3ee053b4373
a privacy - preserving and copy - deterrence content - based image retrieval scheme in cloud computing with the increasing importance of images in people 's daily life , content - based image retrieval ( cbir ) has been widely studied . compared with text documents , images consume much more storage space . hence , its maintenance is considered to be a typical example for cloud storage outsourcing . for privacy - preserving purposes , sensitive images , such as medical and personal images , need to be encrypted before outsourcing , which makes the cbir technologies in plaintext domain to be unusable . in this paper , we propose a scheme that supports cbir over encrypted images without leaking the sensitive information to the cloud server . first , feature vectors are extracted to represent the corresponding images . after that , the pre - filter tables are constructed by locality - sensitive hashing to increase search efficiency . moreover , the feature vectors are protected by the secure knn algorithm , and image pixels are encrypted by a standard stream cipher . in addition , considering the case that the authorized query users may illegally copy and distribute the retrieved images to someone unauthorized , we propose a watermark - based protocol to deter such illegal distributions . in our watermark - based protocol , a unique watermark is directly embedded into the encrypted images by the cloud server before images are sent to the query user . hence , when image copy is found , the unlawful query user who distributed the image can be traced by the watermark extraction . the security analysis and the experiments show the security and efficiency of the proposed scheme .
RANK = 15; score = 0.22928510924891957; correct = False; id = 5aeea932b6ac9553eca0bb3bb13b6d46592d694b
efficient tree - based approximation for entailment graph learning learning entailment rules is fundamental in many semantic - inference applications and has been an active field of research in recent years . in this paper we address the problem of learning transitive graphs that describe entailment rules between predicates ( termed entailment graphs ) . we first identify that entailment graphs exhibit a “ tree - like ” property and are very similar to a novel type of graph termed forest - reducible graph . we utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges , where each iteration takes linear time . we compare our approximation algorithm to a recently - proposed state - of - the - art exact algorithm and show that it is more efficient and scalable both theoretically and empirically , while its output quality is close to that given by the optimal solution of the exact algorithm .
RANK = 16; score = 0.22432339181745678; correct = False; id = f5fd5e0e5da36b1a1e7f1a66d4292ba22e53429b
graph parsing with s - graph grammars a key problem in semantic parsing with graph - based semantic representations is graph parsing , i.e. computing all possible analyses of a given graph according to a grammar . this problem arises in training synchronous string - to - graph grammars , and when generating strings from them . we present two algorithms for graph parsing ( bottom - up and top - down ) with s - graph grammars . on the related problem of graph parsing with hyperedge replacement grammars , our implementations outperform the best previous system by several orders of magnitude .
RANK = 17; score = 0.22245477885856874; correct = False; id = 90ade0533830b426fbf91805fb59ff6ee0ad065b
nothing is for free : security in searching shared and encrypted data most existing symmetric searchable encryption schemes aim at allowing a user to outsource her encrypted data to a cloud server and delegate the latter to search on her behalf . these schemes do not qualify as a secure and scalable solution for the multiparty setting , where users outsource their encrypted data to a cloud server and selectively authorize each other to search . due to the possibility that the cloud server may collude with some malicious users , it is a challenge to have a secure and scalable multiparty searchable encryption ( mpse ) scheme . this is shown by our analysis on the popa - zeldovich scheme , which says that an honest user may leak all her search patterns even if she shares only one of her documents with another malicious user . based on our analysis , we present a new security model for mpse by considering the worst case and average - case scenarios , which capture different server - user collusion possibilities . we then propose a mpse scheme by employing the bilinear property of type-3 pairings and prove its security based on the bilinear diffie - hellman variant and symmetric external diffie - hellman assumptions in the random oracle model .
RANK = 18; score = 0.22010343838108432; correct = False; id = 068f7e719b394f5b098fd29f1ad190c8f65bc3ee
a scalable approach to attack graph generation attack graphs are important tools for analyzing security vulnerabilities in enterprise networks . previous work on attack graphs has not provided an account of the scalability of the graph generating process , and there is often a lack of logical formalism in the representation of attack graphs , which results in the attack graph being difficult to use and understand by human beings . pioneer work by sheyner , et al . is the first attack - graph tool based on formal logical techniques , namely model - checking . however , when applied to moderate - sized networks , sheyner 's tool encountered a significant exponential explosion problem . this paper describes a new approach to represent and generate attack graphs . we propose logical attack graphs , which directly illustrate logical dependencies among attack goals and configuration information . a logical attack graph always has size polynomial to the network being analyzed . our attack graph generation tool builds upon mulval , a network security analyzer based on logical programming . we demonstrate how to produce a derivation trace in the mulval logic - programming engine , and how to use the trace to generate a logical attack graph in quadratic time . we show experimental evidence that our logical attack graph generation algorithm is very efficient . we have generated logical attack graphs for fully connected networks of 1000 machines using a pentium 4 cpu with 1 gb of ram .
RANK = 19; score = 0.21876178886483688; correct = False; id = 66401f8fd4114dade998b90fb5234fd2a586e026
attribute - based data sharing scheme revisited in cloud computing ciphertext - policy attribute - based encryption ( cp - abe ) is a very promising encryption technique for secure data sharing in the context of cloud computing . data owner is allowed to fully control the access policy associated with his data which to be shared . however , cp - abe is limited to a potential security risk that is known as key escrow problem , whereby the secret keys of users have to be issued by a trusted key authority . besides , most of the existing cp - abe schemes can not support attribute with arbitrary state . in this paper , we revisit attribute - based data sharing scheme in order to solve the key escrow issue but also improve the expressiveness of attribute , so that the resulting scheme is more friendly to cloud computing applications . we propose an improved two - party key issuing protocol that can guarantee that neither key authority nor cloud service provider can compromise the whole secret key of a user individually . moreover , we introduce the concept of attribute with weight , being provided to enhance the expression of attribute , which can not only extend the expression from binary to arbitrary state , but also lighten the complexity of access policy . therefore , both storage cost and encryption complexity for a ciphertext are relieved . the performance analysis and the security proof show that the proposed scheme is able to achieve efficient and secure data sharing in cloud computing .
RANK = 20; score = 0.21849376168891033; correct = False; id = b7d98de11771dfc352de9bb7818321d392bfd7ed
fifth acm cloud computing security workshop ( ccsw 2013 ) the cloud computing security workshop ( ccsw ) focuses on the security challenges and opportunities raised by cloud computing . the `` cloud '' is a general term for aggregation of computing resources within an extensive , elastic environment typically marked by a high degree of resource virtualization and sharing among tenants . as a multi - faceted trend , cloud computing creates many and varied security and privacy requirements at the intersection of a broad range of disciplines . the goal of the workshop is to elucidate the security and privacy problems raised by cloud computing and foster understanding of the connection between research and practice in this vibrant and transformative area .

RANKING 2193
QUERY
saql : a stream - based query system for real - time abnormal system behavior detection recently , advanced cyber attacks , which consist of a sequence of steps that involve many vulnerabilities and hosts , compromise the security of many well - protected businesses . this has led to the solutions that ubiquitously monitor system activities in each host ( big data ) as a series of events , and search for anomalies ( abnormal behaviors ) for triaging risky events . since fighting against these attacks is a time - critical mission to prevent further damage , these solutions face challenges in incorporating expert knowledge to perform timely anomaly detection over the large - scale provenance data . to address these challenges , we propose a novel stream - based query system that takes as input , a realtime event feed aggregated from multiple hosts in an enterprise , and provides an anomaly query engine that queries the event feed to identify abnormal behaviors based on the specified anomalies . to facilitate the task of expressing anomalies based on expert knowledge , our system provides a domain - specific query language , saql , which allows analysts to express models for ( 1 ) rule - based anomalies , ( 2 ) time - series anomalies , ( 3 ) invariant - based anomalies , and ( 4 ) outlier - based anomalies . we deployed our system in nec labs america comprising 150 hosts and evaluated it using 1.1 tb of real system monitoring data ( containing 3.3 billion events ) . our evaluations on a broad set of attack behaviors and micro - benchmarks show that our system has a low detection latency ( < 2s ) and a high system throughput ( 110,000 events / s ; supporting ∼4000 hosts ) , and is more efficient in memory utilization than the existing stream - based complex event processing systems .
First cited at 22
TOP CITED PAPERS
RANK 22
high fidelity data reduction for big data security dependency analyses intrusive multi - step attacks , such as advanced persistent threat ( apt ) attacks , have plagued enterprises with significant financial losses and are the top reason for enterprises to increase their security budgets . since these attacks are sophisticated and stealthy , they can remain undetected for years if individual steps are buried in background " noise . " thus , enterprises are seeking solutions to " connect the suspicious dots " across multiple activities . this requires ubiquitous system auditing for long periods of time , which in turn causes overwhelmingly large amount of system audit events . given a limited system budget , how to efficiently handle ever - increasing system audit logs is a great challenge . this paper proposes a new approach that exploits the dependency among system events to reduce the number of log entries while still supporting high - quality forensic analysis . in particular , we first propose an aggregation algorithm that preserves the dependency of events during data reduction to ensure the high quality of forensic analysis . then we propose an aggressive reduction algorithm and exploit domain knowledge for further data reduction . to validate the efficacy of our proposed approach , we conduct a comprehensive evaluation on real - world auditing systems using log traces of more than one month . our evaluation results demonstrate that our approach can significantly reduce the size of system logs and improve the efficiency of forensic analysis without losing accuracy .
RANK 124
effective and efficient malware detection at the end host malware is one of the most serious security threats on the internet today . in fact , most internet problems such as spam e - mails and denial of service attacks have malware as their underlying cause . that is , computers that are compromised with malware are often networked together to form botnets , and many attacks are launched using these malicious , attacker - controlled networks . with the increasing significance of malware in internet attacks , much research has concentrated on developing techniques to collect , study , and mitigate malicious code . without doubt , it is important to collect and study malware found on the internet . however , it is even more important to develop mitigation and detection techniques based on the insights gained from the analysis work . unfortunately , current host - based detection approaches ( i.e. , anti - virus software ) suffer from ineffective detection models . these models concentrate on the features of a specific malware instance , and are often easily evadable by obfuscation or polymorphism . also , detectors that check for the presence of a sequence of system calls exhibited by a malware instance are often evadable by system call reordering . in order to address the shortcomings of ineffective models , several dynamic detection approaches have been proposed that aim to identify the behavior exhibited by a malware family . although promising , these approaches are unfortunately too slow to be used as real - time detectors on the end host , and they often require cumbersome virtual machine technology . in this paper , we propose a novel malware detection approach that is both effective and efficient , and thus , can be used to replace or complement traditional anti - virus software at the end host . our approach first analyzes a malware program in a controlled environment to build a model that characterizes its behavior . such models describe the information flows between the system calls essential to the malware ’s mission , and therefore , can not be easily evaded by simple obfuscation or polymorphic techniques . then , we extract the program slices responsible for such information flows . for detection , we execute these slices to match our models against the runtime behavior of an unknown program . our experiments show that our approach can effectively detect running malicious code on an end user ’s host with a small overhead .
RANK 341
trustworthy whole - system provenance for the linux kernel in a provenance - aware system , mechanisms gather and report metadata that describes the history of each object being processed on the system , allowing users to understand how data objects came to exist in their present state . however , while past work has demonstrated the usefulness of provenance , less attention has been given to securing provenance - aware systems . provenance itself is a ripe attack vector , and its authenticity and integrity must be guaranteed before it can be put to use . we present linux provenance modules ( lpm ) , the first general framework for the development of provenance - aware systems . we demonstrate that lpm creates a trusted provenance - aware execution environment , collecting complete whole - system provenance while imposing as little as 2.7 % performance overhead on normal system operation . lpm introduces new mechanisms for secure provenance layering and authenticated communication between provenance - aware hosts , and also interoperates with existing mechanisms to provide strong security assurances . to demonstrate the potential uses of lpm , we design a provenance - based data loss prevention ( pb - dlp ) system . we implement pbdlp as a file transfer application that blocks the transmission of files derived from sensitive ancestors while imposing just tens of milliseconds overhead . lpm is the first step towards widespread deployment of trustworthy provenance - aware applications .
TOP UNCITED PAPERS
RANK 1
data mining approaches for intrusion detection in this paper we discuss our research in developing general and systematic methods for intrusion detection . the key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior , and use the set of relevant system features to compute ( inductively learned ) classifiers that can recognize anomalies and known intrusions . using experiments on the sendmail system call data and the network tcpdump data , we demonstrate that we can construct concise and accurate classifiers to detect anomalies . we provide an overview on two general data mining algorithms that we have implemented : the association rules algorithm and the frequent episodes algorithm . these algorithms can be used to compute the intraand interaudit record patterns , which are essential in describing program or user behavior . the discovered patterns can guide the audit data gathering process and facilitate feature selection . to meet the challenges of both efficient learning ( mining ) and real - time detection , we propose an agent - based architecture for intrusion detection systems where the learning agents continuously compute and provide the updated ( detection ) models to the detection agents .
RANK 2
adaptive real - time anomaly detection using inductively generated sequential patterns this paper describes a time - based inductive learning approach to the problem of real - time anomaly detection . this approach uses sequential rules that characterize a user 's behavior over time . a rulebase is used to store pattems of user activities , and anomalies are reponed whenever a user 's activity deviates significantly from those specified in the rules . the rules in the rulebase characterize either the sequential relationships between security audit records , or the temporal properties of the records . the rules are created in two ways : they are either dynamically generated and modified by a time - based inductive engine in order to adapt to changes in a user 's behavior , or they are specified by the security management to implement a site security policy . this approach allows the correlation between adjacent security events to be exploited for the purpose of greater sensitivity in anomaly detection against seemingly intractable ( or " erratic " ) activities using statistical approaches . real - time detection of anomaly activities is possible .
RANK 3
centered hyperspherical and hyperellipsoidal one - class support vector machines for anomaly detection in sensor networks anomaly detection in wireless sensor networks is an important challenge for tasks such as intrusion detection and monitoring applications . this paper proposes two approaches to detecting anomalies from measurements from sensor networks . the first approach is a linear programming - based hyperellipsoidal formulation , which is called a centered hyperellipsoidal support vector machine ( cesvm ) . while this cesvm approach has advantages in terms of its flexibility in the selection of parameters and the computational complexity , it has limited scope for distributed implementation in sensor networks . in our second approach , we propose a distributed anomaly detection algorithm for sensor networks using a one - class quarter - sphere support vector machine ( qssvm ) . here a hypersphere is found that captures normal data vectors in a higher dimensional space for each sensor node . then summary information about the hyperspheres is communicated among the nodes to arrive at a global hypersphere , which is used by the sensors to identify any anomalies in their measurements . we show that the cesvm and qssvm formulations can both achieve high detection accuracies on a variety of real and synthetic data sets . our evaluation of the distributed algorithm using qssvm reveals that it detects anomalies with comparable accuracy and less communication overhead than a centralized approach .
TOP 20
RANK = 1; score = 0.2558049849569349; correct = False; id = 1a76f0539a9badf317b0b35ea92f734c62467138
data mining approaches for intrusion detection in this paper we discuss our research in developing general and systematic methods for intrusion detection . the key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior , and use the set of relevant system features to compute ( inductively learned ) classifiers that can recognize anomalies and known intrusions . using experiments on the sendmail system call data and the network tcpdump data , we demonstrate that we can construct concise and accurate classifiers to detect anomalies . we provide an overview on two general data mining algorithms that we have implemented : the association rules algorithm and the frequent episodes algorithm . these algorithms can be used to compute the intraand interaudit record patterns , which are essential in describing program or user behavior . the discovered patterns can guide the audit data gathering process and facilitate feature selection . to meet the challenges of both efficient learning ( mining ) and real - time detection , we propose an agent - based architecture for intrusion detection systems where the learning agents continuously compute and provide the updated ( detection ) models to the detection agents .
RANK = 2; score = 0.2361566335864122; correct = False; id = 8481ea338f616341b516d1e5cb85fc402e24a1ff
adaptive real - time anomaly detection using inductively generated sequential patterns this paper describes a time - based inductive learning approach to the problem of real - time anomaly detection . this approach uses sequential rules that characterize a user 's behavior over time . a rulebase is used to store pattems of user activities , and anomalies are reponed whenever a user 's activity deviates significantly from those specified in the rules . the rules in the rulebase characterize either the sequential relationships between security audit records , or the temporal properties of the records . the rules are created in two ways : they are either dynamically generated and modified by a time - based inductive engine in order to adapt to changes in a user 's behavior , or they are specified by the security management to implement a site security policy . this approach allows the correlation between adjacent security events to be exploited for the purpose of greater sensitivity in anomaly detection against seemingly intractable ( or " erratic " ) activities using statistical approaches . real - time detection of anomaly activities is possible .
RANK = 3; score = 0.2194057086340719; correct = False; id = aea23355420b2a679d2d6f0e6cc01f4a153710cb
centered hyperspherical and hyperellipsoidal one - class support vector machines for anomaly detection in sensor networks anomaly detection in wireless sensor networks is an important challenge for tasks such as intrusion detection and monitoring applications . this paper proposes two approaches to detecting anomalies from measurements from sensor networks . the first approach is a linear programming - based hyperellipsoidal formulation , which is called a centered hyperellipsoidal support vector machine ( cesvm ) . while this cesvm approach has advantages in terms of its flexibility in the selection of parameters and the computational complexity , it has limited scope for distributed implementation in sensor networks . in our second approach , we propose a distributed anomaly detection algorithm for sensor networks using a one - class quarter - sphere support vector machine ( qssvm ) . here a hypersphere is found that captures normal data vectors in a higher dimensional space for each sensor node . then summary information about the hyperspheres is communicated among the nodes to arrive at a global hypersphere , which is used by the sensors to identify any anomalies in their measurements . we show that the cesvm and qssvm formulations can both achieve high detection accuracies on a variety of real and synthetic data sets . our evaluation of the distributed algorithm using qssvm reveals that it detects anomalies with comparable accuracy and less communication overhead than a centralized approach .
RANK = 4; score = 0.2140990992071943; correct = False; id = 21b3247679905e6466b9e9dcf3d2c2f46cce27a7
an experimental analysis of power and delay signal - to - noise requirements for detecting trojans and methods for achieving the required detection sensitivities new validation methods are needed for ensuring integrated circuit ( ic ) trust , and in particular for detecting hardware trojans . in this paper , we investigate the signal - to - noise ratio ( snr ) requirements for detecting trojans by conducting ring oscillator ( ro ) experiments on a set of v2pro fpgas . the ros enable a high degree of control over the switching activity in the fpgas while simultaneously permitting subtle delay and transient power supply anomalies to be introduced through simple modifications to the ro logic structure . power and delay analyses are first carried out across a set of fpgas using ro configurations that emulate trojan - free conditions . these experiments are designed to determine the magnitude of process and environmental ( pe ) variations , and are used to establish statistical limits on the noise floor for the subsequent emulated trojan experiments . the emulated trojan experiments introduce anomalies in power and delay in subtle ways as additional loads and series inserted gates . the data from both experiments is used to determine the detection sensitivity of several statistical methods to the transient anomalies introduced by these types of design modifications . a calibration technique is proposed that improves sensitivity to small transient anomalies significantly . finally , we describe testing techniques that enable high resolution measurements of power and delay to support the proposed calibration and statistics - based detection methods .
RANK = 5; score = 0.20835313761733357; correct = False; id = 3f9f2845b7eff9e764915bb6ae6b491e3e7b158d
predictive network anomaly detection and visualization various approaches have been developed for quantifying and displaying network traffic information for determining network status and in detecting anomalies . although many of these methods are effective , they rely on the collection of long - term network statistics . here , we present an approach that uses short - term observations of network features and their respective time averaged entropies . acute changes are localized in network feature space using adaptive wiener filtering and auto - regressive moving average modeling . the color - enhanced datagram is designed to allow a network engineer to quickly capture and visually comprehend at a glance the statistical characteristics of a network anomaly . first , average entropy for each feature is calculated for every second of observation . then , the resultant short - term measurement is subjected to first- and second - order time averaging statistics . these measurements are the basis of a novel approach to anomaly estimation based on the well - known fisher linear discriminant ( fld ) . average port , high port , server ports , and peered ports are some of the network features used for stochastic clustering and filtering . we empirically determine that these network features obey gaussian - like distributions . the proposed algorithm is tested on real - time network traffic data from ohio university 's main internet connection . experimentation has shown that the presented fld - based scheme is accurate in identifying anomalies in network feature space , in localizing anomalies in network traffic flow , and in helping network engineers to prevent potential hazards . furthermore , its performance is highly effective in providing a colorized visualization chart to network analysts in the presence of bursty network traffic .
RANK = 6; score = 0.20785683256821913; correct = False; id = 565964e45055bcb402b469798f6d7ea1257f4ab2
real - time detection , tracking , and monitoring of automatically discovered events in social media we introduce redites , a system for realtime event detection , tracking , monitoring and visualisation . it is designed to assist information analysts in understanding and exploring complex events as they unfold in the world . events are automatically detected from the twitter stream . then those that are categorised as being security - relevant are tracked , geolocated , summarised and visualised for the end - user . furthermore , the system tracks changes in emotions over events , signalling possible flashpoints or abatement . we demonstrate the capabilities of redites using an extended use case from the september 2013 westgate shooting incident . through an evaluation of system latencies , we also show that enriched events are made available for users to explore within seconds of that event occurring .
RANK = 7; score = 0.20369123996682298; correct = False; id = 537693d586aeced05a50954b9615225fe62d8b34
detection of abnormal visual events via global optical flow orientation histogram the aim of this paper is to detect abnormal events in video streams , a challenging but important subject in video surveillance . we propose a novel algorithm to address this problem . the algorithm is based on an image descriptor and a nonlinear classification method . we introduce a histogram of optical flow orientation as a descriptor encoding the moving information of each video frame . the nonlinear one - class support vector machine classification algorithm , following a learning period characterizing the normal behavior of training frames , detects abnormal events in the current frame . further , a fast version of the detection algorithm is designed by fusing the optical flow computation with a background subtraction step . we finally apply the method to detect abnormal events on several benchmark data sets , and show promising results .
RANK = 8; score = 0.19620857284111679; correct = False; id = 51c37c5e1ea925c93e4e03d1007f117582cd2844
anomaly detection of web - based attacks web - based vulnerabilities represent a substantial portion of the security exposures of computer networks . in order to detect known web - based attacks , misuse detection systems are equipped with a large number of signatures . unfortunately , it is difficult to keep up with the daily disclosure of web - related vulnerabilities , and , in addition , vulnerabilities may be introduced by installation - specific web - based applications . therefore , misuse detection systems should be complemented with anomaly detection systems . this paper presents an intrusion detection system that uses a number of different anomaly detection techniques to detect attacks against web servers and web - based applications . the system correlates the server - side programs referenced by client queries with the parameters contained in these queries . the application - specific characteristics of the parameters allow the system to perform focused analysis and produce a reduced number of false positives . the system derives automatically the parameter profiles associated with web applications ( e.g. , length and structure of parameters ) from the analyzed data . therefore , it can be deployed in very different application environments without having to perform time - consuming tuning and configuration .
RANK = 9; score = 0.19538988550340658; correct = False; id = c69e36594b5939fe52ac3de9cc9ad333decfe7e0
event sequence model for semantic analysis of time and location in dialogue system it is important for a natural language dialogue system to interpret relations among event concepts appearing in a dialogue . the more complex a dialog becomes , the more essential it becomes for a natural language dialogue system to perform this kind of interpretation . traditionally , many studies have focused on this problem . some dialogue systems supported such semantic analysis by using rules and/or models designed for particular scenes involving specific type of dialogue and/or specific problem solving . however , these frameworks require system developers to reconstruct those rules / models even if a slight change is added to the targeted scene . in many cases , their rules / models heavily depend on specific type of dialogue / problem solving , and they do not have high reusability and modularity . since those rules / models have scene - depending design , they can not be used to incrementally construct a bigger rule or model . in this research , we focus on a set of event concepts which are usually expected to occur sequentially . in a dialogue , a spoken event concept enables the listeners to guess a sequence of events . the sequence may sometimes be logically inferred , and it may be understood based on general common sense . we believe that a concept model of sequential events can be designed for each bigger event concept that consists of a series of smaller events . using the sequentiality of the events in the model , a dialogue system can analyze time and location of each event in a dialogue . in this paper , we design a structure of the event sequence model and propose a framework for analyzing time and location of event concepts appearing in a dialogue . we implemented this framework in a dialogue system , and designed some event sequence models . we confirmed that this system could analyze time and location of sequential events without scene - depending rules .
RANK = 10; score = 0.19326749579872637; correct = False; id = bdf67ee2a13931ca2d5eac458714ed98148d1b34
an intrusion - detection model a model of a real - time intrusion - detection expert system capable of detecting break - ins , penetrations , and other forms of computer abuse is described . the model is based on the hypothesis that security violations can be detected by monitoring a system 's audit records for abnormal patterns of system usage . the model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models , and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior . the model is independent of any particular system , application environment , system vulnerability , or type of intrusion , thereby providing a framework for a general - purpose intrusion - detection expert system .
RANK = 11; score = 0.18712404982214598; correct = False; id = 6df8cd4c69e75b286b1ba27417fd41a21d4982e1
event detection and co - reference with minimal supervision an important aspect of natural language understanding involves recognizing and categorizing events and the relations among them . however , these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task , resulting in supervised systems that attempt to learn complex models from small amounts of data , which they over - fit . this paper addresses this challenge by developing an event detection and co - reference system with minimal supervision , in the form of a few event examples . we view these tasks as semantic similarity problems between event mentions or event mentions and an ontology of types , thus facilitating the use of large amounts of out of domain text data . notably , our semantic relatedness function exploits the structure of the text by making use of a semantic - role - labeling based representation of an event . we show that our approach to event detection is competitive with the top supervised methods . more significantly , we outperform stateof - the - art supervised methods for event coreference on benchmark data sets , and support significantly better transfer across domains .
RANK = 12; score = 0.18586557662895783; correct = False; id = 741a4395d1ca156dd05cee087b35a17b81fc1e54
bootstrapping events and relations from text we describe a new approach to semi - supervised adaptive learning of event extraction from text . given a set of examples and an un - annotated text corpus , the bear system ( bootstrapping events and relations ) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text , such as events involving multiple entities and their roles . for example , given a series of descriptions of bombing and shooting incidents ( e.g. , in newswire ) the system will learn to extract , with a high degree of accuracy , other attack - type events mentioned elsewhere in text , irrespective of the form of description . a series of evaluations using the ace test data and event set show a significant performance improvement over previous approaches .
RANK = 13; score = 0.18557449428375727; correct = False; id = 98b763c35cbe0e8f31e9cbaeaf2a6fa27ee33888
identification of event mentions and their semantic class complex tasks like question answering need to be able to identify events in text and the relations among those events . we show that this event identification task and a related task , identifying the semantic class of these events , can both be formulated as classification problems in a word - chunking paradigm . we introduce a variety of linguistically motivated features for this task and then train a system that is able to identify events with a precision of 82 % and a recall of 71 % . we then show a variety of analyses of this model , and their implications for the event identification task .
RANK = 14; score = 0.1850535616772134; correct = False; id = 96e95c2e416cc9a4402c9a547758197cb204d85c
language - independent context aware query translation using wikipedia the exponential growth of the world - wide web has transformed it into an ecology of knowledge in which highly diverse information is linked in an extremely complex and arbitrary manner . at the same time , huge multilingual content is being added to it through various sources like blogs , news articles and several other forms . to facilitate the access of this enormous multilingual content , cross lingual information access systems ( clia ) are vital . clia systems take users’ information need and returns the required information to them . the information can be obtained from various sources and different languages , reducing the effort of user to search in different information sources . query translation is the most important task in building an effective clia system as the accuracy of the results are dependent on the accuracy of the query translation . the user input , usually few words , is considered as the query . given the differences in vocabulary across the languages , we have to capture the context of the query to increase the efficiency of the query translation . query translation forms the vital component of any clia system as it captures the users need and translating queries effectively is required . we devised a language independent approach that captures the users intent from his query by mapping query words to wikipedia topics . wikipedia , the free on - line encyclopedia , is a hypertext document collection with a rich link structure . unlike ordinary encyclopedias , wikipedia is based on the wiki concept , which enables community to create or improve articles . wikipedia articles cover many domain - specific and general topics , making it a rich knowledge resource . due to its unique concept , wikipedia is populated with articles in around 240 languages , making it a rich multi - lingual resource . we intend to leverage this multi lingual knowledge base as a resource to aid multi lingual query translation . in this work , we propose a query translation method that uses the rich link structure across the languages present in wikipedia . the query translation approach we have adopted is language independent and focuses on minimizing the use of language specific resources , making our approach scalable . this approach is specifically useful for under - resourced languages ( indian , african etc ) as these languages do not have sufficient resources and tools with good accuracies . for these under - resourced languages , a query translation approach is proposed using the resources mined from wikipedia substituting the language - specific resources for query translation . using the cross lingual links and the articles that are connected with these links , we build dictionaries that forms the resource for query translation . a language independent approach to identify parallel text is also proposed . a parallel text is a text placed alongside its translation or translations . parallel text forms the major resource for many areas of linguistic research and their availability helps to identify many retrieval problems between the pair of languages . the query translation approach we have adopted takes the context of the query into consideration to perform the translation . we use wikipedia concepts to mine the context of the user query and use them to form the translation . an evaluation method to evaluate query translation is also proposed using wikipedia , by mining statistics from the articles and the cross lingual links . a separate evaluation method is required in - line with our intent to develop the system without using any language resources due to their non - availability . the coverage of the dictionaries is 78.1 % while the accuracy of the query translation is 69.75 % . in this work , we develop a language - independent approach to query translation that also considers the context of the query . since no language resources were used , our approach is scalable and can be applied to any pair of languages present in wikipedia . the performance of the system is encouraging and condone our intention of substituting wikipedia for language resources . further the resources mined and approaches proposed can be leveraged in various areas of linguistic research . parallel sentence extraction approach can be used to solve the problem of cross lingual plagiarism detection . extraction of context from a sentence can be extended to support update summarization task . thus , in this work , we suggest a language independent query translation approach which can be a right direction towards building a clia system for less resourced languages .
RANK = 15; score = 0.18189252237037187; correct = False; id = 004d75ea6d3b8e0b84bd2a9392d679a634a10fb3
gray - box extraction of execution graphs for anomaly detection many host - based anomaly detection systems monitor a process by observing the system calls it makes , and comparing these calls to a model of behavior for the program that the process should be executing . in this paper we introduce a new model of system call behavior , called an < i > execution graph</i>. the execution graph is the first such model that both requires no static analysis of the program source or binary , and conforms to the control flow graph of the program . when used as the model in an anomaly detection system monitoring system calls , it offers two strong properties : ( i ) it accepts only system call sequences that are consistent with the control flow graph of the program ; ( ii ) it is maximal given a set of training data , meaning that any extensions to the execution graph could permit some intrusions to go undetected . in this paper , we formalize and prove these claims . we additionally evaluate the performance of our anomaly detection technique .
RANK = 16; score = 0.18066850107944524; correct = False; id = 6465642d08c82eaa4951435443f536df438b2ab9
a method for detecting abnormal program behavior on embedded devices a potential threat to embedded systems is the execution of unknown or malicious software capable of triggering harmful system behavior , aimed at theft of sensitive data or causing damage to the system . commercial off - the - shelf embedded devices , such as embedded medical equipment , are more vulnerable as these type of products can not be amended conventionally or have limited resources to implement protection mechanisms . in this paper , we present a self - organizing map ( som)-based approach to enhance embedded system security by detecting abnormal program behavior . the proposed method extracts features derived from processor 's program counter and cycles per instruction , and then utilises the features to identify abnormal behavior using the som . results achieved in our experiment show that the proposed method can identify unknown program behaviors not included in the training set with over 98.4 % accuracy .
RANK = 17; score = 0.180232697396447; correct = False; id = 642dbfed2d1971a54c7a39665077e95b02311d03
interdependent security risk analysis of hosts and flows detection of high risk hosts and flows continues to be a significant problem in security monitoring of high throughput networks . a comprehensive risk assessment method should consider the risk propagation among risky hosts and flows . in this paper , this is achieved by introducing two novel concepts . first , an interdependency relationship among the risk scores of a network flow and its source and destination hosts . on the one hand , the risk score of a host depends on risky flows initiated by or terminated at the host . on the other hand , the risk score of a flow depends on the risk scores of its source and destination hosts . second , which we call flow provenance , represents risk propagation among network flows which considers the likelihood that a particular flow is caused by the other flows . based on these two concepts , we develop an iterative algorithm for computing the risk score of hosts and network flows . we give a rigorous proof that our algorithm rapidly converges to unique risk estimates , and provide its extensive empirical evaluation using two real - world data sets . our evaluation shows that our method is effective in detecting high risk hosts and flows and is sufficiently efficient to be deployed in the high throughput networks .
RANK = 18; score = 0.17916135165587133; correct = False; id = 2ff9c10a0a8f43306f3a0492f8d6eca744d4e7c7
finding the linchpins of the dark web : a study on topologically dedicated hosts on malicious web infrastructures malicious web activities continue to be a major threat to the safety of online web users . despite the plethora forms of attacks and the diversity of their delivery channels , in the back end , they are all orchestrated through malicious web infrastructures , which enable miscreants to do business with each other and utilize others ' resources . identifying the linchpins of the dark infrastructures and distinguishing those valuable to the adversaries from those disposable are critical for gaining an upper hand in the battle against them . in this paper , using nearly 4 million malicious url paths crawled from different attack channels , we perform a large - scale study on the topological relations among hosts in the malicious web infrastructure . our study reveals the existence of a set of topologically dedicated malicious hosts that play orchestrating roles in malicious activities . they are well connected to other malicious hosts and do not receive traffic from legitimate sites . motivated by their distinctive features in topology , we develop a graph - based approach that relies on a small set of known malicious hosts as seeds to detect dedicate malicious hosts in a large scale . our method is general across the use of different types of seed data , and results in an expansion rate of over 12 times in detection with a low false detection rate of 2 % . many of the detected hosts operate as redirectors , in particular traffic distribution systems ( tdses ) that are long - lived and receive traffic from new attack campaigns over time . these tdses play critical roles in managing malicious traffic flows . detecting and taking down these dedicated malicious hosts can therefore have more impact on the malicious web infrastructures than aiming at short - lived doorways or exploit sites .
RANK = 19; score = 0.17843600629711526; correct = False; id = cf29e5f560d0428bb3f616865f1de1264cdd8360
the interpretation of tense and aspect in english an analysis of english tense and aspect is presented that specifies temporal precedence relations within a sentence . the relevant reference points for interpretation are taken to be the initial and terminal points of events in the world , as well as two " hypothetical " times : the perfect t ime ( when a sentence contains perfect aspect ) and the progressive or during time . a method for providing temporal interpretation for nontensed elements in the sentence is also described . 1 . i n t r o d u c t i o n the analysis of tense and aspect requires specifying what relations can or can not hold among times and events , given a sentence describing those events . 1 for example , a specification of the meaning of the past - tense sentence " john ate a cake " involves the fact that the time of the main event in this case , the cake - eating event precedes the time of utterance of the sentence . various proposals have also been made regarding the analysis of aspect which involve auxiliary times or events , whereby the proper relationship of these auxiliary times or events to " real " main events is specified . we provide an analysis of english tense and aspect that involves specifying relations among times rather than events . we also offer a means of interpreting tenseless elements like nouns and adjectives whose interpretation may be temporally dependent . for example , the noun phrase " the warm cakes " picks out different sets of cakes , depending on the time relative to which it'receives an interpretation . the analysis presented here has been implemented with the prolog data base query system 1the work presented here was suppor t ed by sp , i internat ional . i am grateful to phil cohen , bill croft , doug edwards , jerry hobbe , doug moran , and fernando perelm for helpful discussion and comments . chat ( pereira 1983 ) , and the representations are based on those used in that system . we shall show that an analysis of tense and aspect involving specification of relations among times rather than among events results in a clean analysis of various types of sentences . 2 . t i m e p o in t s harper and charniak ( 1986 ) [ henceforth h&c ] provide an interesting and revealing analysis of english tense and aspect involving relations between events . there are several kinds of events : the u ~ terance event , which is associated with the time of the utterance ; the main event , or the event being described by the main verb of the sentence ; the perfecg event ; and the progressivje event . the representation of every sentence involves the utterance event and the main event ; sentences with progressive or perfect aspect also involve progressive or perfect events . this treatment is quite different from the reichenbach ( 1947 ) conception of " reference time " , which is assumed to be relevant for all sentences . to translate between the two systems , the reference time may be thought of as being represented by the perfect event in perfect sentences and by the progressive event in progressive sentences . in the case of perfect progressives , one might consider that there are two reference events , while in simple tenses there is no reference event at all . alternatively , in a system like webber ( 1987 ) in which reference points for each sentence are used to construct an event structure , the tensed event ( what h&c call the " anchor event " ) is the relevant one : the perfect event for sentences with perfect aspect ; for sentences with progressive but no perfect aspect , the progressive event ; or the main event for simple tense sentences . 2 2 al though i n s t a ~ s ra ther t h a n events are u sed in the representa t ion described here , a similar s t ra tegy would be employable in buildin 5 up a webber - s tyle event s t ruc ture .
RANK = 20; score = 0.178052335243414; correct = False; id = bf1c3d6690398fe3d53845c811a94e11a07107cc
a memory - based learning approach to event extraction in biomedical texts in this paper we describe the memory - based machine learning system that we submitted to the bionlp shared task on event extraction . we modeled the event extraction task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding . the results obtained by our system ( 30.58 f - score in task 1 and 29.27 in task 2 ) suggest that the approach and the system need further adaptation to the complexity involved in extracting biomedical events .

RANKING 962
QUERY
on the strength of character language models for multilingual named entity recognition character - level patterns have been widely used as features in english named entity recognition ( ner ) systems . however , to date there has been no direct investigation of the inherent differences between name and nonname tokens in text , nor whether this property holds across multiple languages . this paper analyzes the capabilities of corpus - agnostic character - level language models ( clms ) in the binary task of distinguishing name tokens from non - name tokens . we demonstrate that clms provide a simple and powerful model for capturing these differences , identifying named entity tokens in a diverse set of languages at close to the performance of full ner systems . moreover , by adding very simple clm - based features we can significantly improve the performance of an off - the - shelf ner system for multiple languages.1
First cited at 33
TOP CITED PAPERS
RANK 33
neural architectures for named entity recognition state - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora that are available . in this paper , we introduce two new neural architectures — one based on bidirectional lstms and conditional random fields , and the other that constructs and labels segments using a transition - based approach inspired by shift - reduce parsers . our models rely on two sources of information about words : character - based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora . our models obtain state - of - the - art performance in ner in four languages without resorting to any language - specific knowledge or resources such as gazetteers . 1
RANK 60
language independent named entity recognition combining morphological and contextual evidence identifying and classifying personal , geographic , institutional or other names in a text is an important task for numerous applications . this paper describes and evaluates a language - independent bootstrapping algorithm based on iterative learning and re - estimation of contextual and morphological patterns captured in hierarchically smoothed trie models . the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language - specific information , tokenizers or tools . 1 i n t r o d u c t i o n the ability to determine the named entities in a text has been established as an important task for several natural language processing areas , including information retrieval , machine translation , information extraction and language understanding . for the 1995 message understanding conference ( muc-6 ) , a separate named entity recognition task was developed and the best systems achieved impressive accuracy ( with an f - measure approaching 95 % ) . what should be underlined here is that these systems were trained for a specific domain and a particular langnage ( english ) , typically making use of hand - coded rules , taggers , parsers and semantic lexicons . indeed , most named entity recognizers that have been published either use tagged text , perform syntactical and morphological analysis or use semantic information for contextual clues . even the systems that do not make use of extensive knowledge about a particular language , such as nominator ( choi et al . , 1997 ) , still typically use large data files containing lists of names , exceptions , personal and organizational identifiers . our aim has been to build a maximally langnageindependent system for both named - entity identification and classification , using minimal information about the source language . the applicability of ai - style algorithms and supervised methods is limited in the multilingual case because of the cost of knowledge databases and manually annotated corpora . therefore , a much more suitable approach is to consider an em - style bootstrapping algorithm . in terms of world knowledge , the simplest and most relevant resource for this task is a database of known names . for each entity class to be recognized and tagged , it is assumed that the user can provide a short list ( order of one hundred ) of unambiguous examples ( seeds ) . of course the more examples provided , the better the results , but what we try to prove is that even with minimal knowledge good results can be achieved . additionally some basic particularities of the language should be known : capitalization ( if it exists and is relevant some languages do not make use of capitalization ; in others , such as german , the capitalization is not of great help ) , allowable word separators ( if they exist ) , and a few frequent exceptions ( like the pronoun " / " in english ) . although such information can be utilised if present , it is not required , and no other assumptions are made in the general model . 1.1 wordin te rna l and contextual information the algorithm relies on both word internal and contextual clues as relatively independent evidence sources that drive the bootstrapping algorithm . the first category refers to the morphological structure of the word and makes use of the paradigm that for certain classes of entities some prefixes and suffixes are good indicators . for example , knowing that " maria " , " marinela " and " maricica " are feminine first names in romanian , the same classification may be a good guess for " mariana " , based on common prefix . suffixes are typically even more informative , for example " -escu " is an almost perfect indicator of a last name in romanian , the same applies to " -wski " in polish , " -ovic " and " -ivic " in serbocroatian , " -son " in english etc . such morphological information is automatically learned during bootstrapping . contextual patterns ( e.g. " mr. " , " in " and " mayor of " in left context ) are also clearly crucial to named entity identification and classification , especially for names that do not follow a typical morphological pattern for their word class , are of foreign origin or polysemous ( for example , many places or
RANK 2239
two discourse driven language models for semantics natural language understanding often requires deep semantic knowledge . expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction . we develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities . for each model , we investigate four implementations : a “ standard ” n - gram language model and three discriminatively trained “ neural ” language models that generate embeddings for semantic frames . the quality of the semantic language models ( semlm ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically – we show that our semlm helps improve performance on semantic natural language processing tasks such as co - reference resolution and discourse parsing .
TOP UNCITED PAPERS
RANK 1
named entity recognition in estonian the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names , organizations and locations . in this work , we address the problem of ner in estonian using supervised learning approach . we explore common issues related to building a ner system such as the usage of language - agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools . for system training and evaluation purposes , we create a gold standard ner corpus . on this corpus , our crf - based system achieves an overall f1-score of 87 % .
RANK 2
person name entity recognition for arabic named entity recognition ( ner ) is nowadays an important task , which is responsible for the identification of proper names in text and their classification as different types of named entity such as people , locations , and organizations . in this paper , we present our attempt at the recognition and extraction of the most important proper name entity , that is , the person name , for the arabic language . we developed the system , person name entity recognition for arabic ( pera ) , using a rule - based approach . the system consists of a lexicon , in the form of gazetteer name lists , and a grammar , in the form of regular expressions , which are responsible for recognizing person name entities . the pera system is evaluated using a corpus that is tagged in a semi - automated way . the system performance results achieved were satisfactory and confirm to the targets set forth for the precision , recall , and fmeasure .
RANK 3
improving multilingual named entity recognition with wikipedia entity type mapping the state - of - the - art named entity recognition ( ner ) systems are statistical machine learning models that have strong generalization capability ( i.e. , can recognize unseen entities that do not appear in training data ) based on lexical and contextual information . however , such a model could still make mistakes if its features favor a wrong entity type . in this paper , we utilize wikipedia as an open knowledge base to improve multilingual ner systems . central to our approach is the construction of high - accuracy , highcoverage multilingual wikipedia entity type mappings . these mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language - dependent knowledge involved . based on these mappings , we develop several approaches to improve an ner system . we evaluate the performance of the approaches via experiments on ner systems trained for 6 languages . experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities , especially when a system is applied to a new domain or it is trained with little training data ( up to 18.3 f1 score improvement ) .
TOP 20
RANK = 1; score = 0.3807900991444609; correct = False; id = 4723b9401da52ca0962a169a9c6260908fa6c478
named entity recognition in estonian the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names , organizations and locations . in this work , we address the problem of ner in estonian using supervised learning approach . we explore common issues related to building a ner system such as the usage of language - agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools . for system training and evaluation purposes , we create a gold standard ner corpus . on this corpus , our crf - based system achieves an overall f1-score of 87 % .
RANK = 2; score = 0.3459461729851157; correct = False; id = 6537f1fcf7f7549d9e9adc5f880a0a1c0da54838
person name entity recognition for arabic named entity recognition ( ner ) is nowadays an important task , which is responsible for the identification of proper names in text and their classification as different types of named entity such as people , locations , and organizations . in this paper , we present our attempt at the recognition and extraction of the most important proper name entity , that is , the person name , for the arabic language . we developed the system , person name entity recognition for arabic ( pera ) , using a rule - based approach . the system consists of a lexicon , in the form of gazetteer name lists , and a grammar , in the form of regular expressions , which are responsible for recognizing person name entities . the pera system is evaluated using a corpus that is tagged in a semi - automated way . the system performance results achieved were satisfactory and confirm to the targets set forth for the precision , recall , and fmeasure .
RANK = 3; score = 0.3452141445503752; correct = False; id = d7c9fe8e5c26be64854bf7d14a61f91158638f7f
improving multilingual named entity recognition with wikipedia entity type mapping the state - of - the - art named entity recognition ( ner ) systems are statistical machine learning models that have strong generalization capability ( i.e. , can recognize unseen entities that do not appear in training data ) based on lexical and contextual information . however , such a model could still make mistakes if its features favor a wrong entity type . in this paper , we utilize wikipedia as an open knowledge base to improve multilingual ner systems . central to our approach is the construction of high - accuracy , highcoverage multilingual wikipedia entity type mappings . these mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language - dependent knowledge involved . based on these mappings , we develop several approaches to improve an ner system . we evaluate the performance of the approaches via experiments on ner systems trained for 6 languages . experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities , especially when a system is applied to a new domain or it is trained with little training data ( up to 18.3 f1 score improvement ) .
RANK = 4; score = 0.3309429476744924; correct = False; id = 52b1f688ddb8f943fa65cc4b875181f6497ae14c
mining wiki resources for multilingual named entity recognition in this paper , we describe a system by which the multilingual characteristics of wikipedia can be utilized to annotate a large corpus of text with named entity recognition ( ner ) tags requiring minimal human intervention and no linguistic expertise . this process , though of value in languages for which resources exist , is particularly useful for less commonly taught languages . we show how the wikipedia format can be used to identify possible named entities and discuss in detail the process by which we use the category structure inherent to wikipedia to determine the named entity type of a proposed entity . we further describe the methods by which english language data can be used to bootstrap the ner process in other languages . we demonstrate the system by using the generated corpus as training sets for a variant of bbn 's identifinder in french , ukrainian , spanish , polish , russian , and portuguese , achieving overall f - scores as high as 84.7 % on independent , human - annotated corpora , comparable to a system trained on up to 40,000 words of human - annotated newswire .
RANK = 5; score = 0.3282370112990153; correct = False; id = b3e9742953dc84eeffe4c5eedd78467f5a7f5374
rule - based named entity recognition in urdu named entity recognition or extraction ( ner ) is an important task for automated text processing for industries and academia engaged in the field of language processing , intelligence gathering and bioinformatics . in this paper we discuss the general problem of named entity recognition , more specifically the challenges in ner in languages that do not have language resources e.g. large annotated corpora . we specifically address the challenges for urdu ner and differentiate it from other south asian ( indic ) languages . we discuss the differences between hindi and urdu and conclude that the ner computational models for hindi can not be applied to urdu . a rule - based urdu ner algorithm is presented that outperforms the models that use statistical learning .
RANK = 6; score = 0.324937895978736; correct = False; id = a2caf046aa395483d53b50ca51f9e21c5e9ad970
chinese word segmentation and named entity recognition based on conditional random fields models this paper mainly describes a chinese named entity recognition ( ner ) system ner@iscas , which integrates text , part - of - speech and a small - vocabularycharacter - lists feature for msra ner open track under the framework of conditional random fields ( crfs ) model . the techniques used for the close ner and word segmentation tracks are also presented .
RANK = 7; score = 0.32356628636286155; correct = False; id = 6115699ee0b8c3cf797be394382f84a17fa9c473
bootstrapped text - level named entity recognition for literature we present a named entity recognition ( ner ) system for tagging fiction : litner . relative to more traditional approaches , litner has two important properties : ( 1 ) it makes no use of handtagged data or gazetteers , instead it bootstraps a model from term clusters ; and ( 2 ) it leverages multiple instances of the same name in a text . our experiments show it to substantially outperform off - the - shelf supervised ner systems .
RANK = 8; score = 0.3195750973896184; correct = False; id = ba31d764068208e36bcbcc695702b12de78d1277
empirical study on the performance stability of named entity recognition model across domains when a machine learning - based named entity recognition system is employed in a new domain , its performance usually degrades . in this paper , we provide an empirical study on the impact of training data size and domain information on the performance stability of named entity recognition models . we present an informative sample selection method for building high quality and stable named entity recognition models across domains . experimental results show that the performance of the named entity recognition model is enhanced significantly after being trained with these informative samples .
RANK = 9; score = 0.3101972638304601; correct = False; id = 6938196e63ff09c25d1e1366aaec7135a6720216
boosting named entity recognition with neural character embeddings most state - of - the - art named entity recognition ( ner ) systems rely on handcrafted features and on the output of other nlp tasks such as part - of - speech ( pos ) tagging and text chunking . in this work we propose a language - independent ner system that uses automatically learned features only . our approach is based on the charwnn deep neural network , which uses word - level and character - level representations ( embeddings ) to perform sequential classification . we perform an extensive number of experiments using two annotated corpora in two different languages : harem i corpus , which contains texts in portuguese ; and the spa conll2002 corpus , which contains texts in spanish . our experimental results give evidence of the contribution of neural character embeddings for ner . moreover , we demonstrate that the same neural network which has been successfully applied to pos tagging can also achieve state - of - theart results for language - independet ner , using the same hyperparameters , and without any handcrafted features . for the harem i corpus , charwnn outperforms the state - of - the - art system by 7.9 points in the f1-score for the total scenario ( ten ne classes ) . for the spa conll-2002 corpus , charwnn outperforms the state - ofthe - art system by 0.8 point in the f1 .
RANK = 10; score = 0.30822481063001816; correct = False; id = d2e1a04836db23105aebc945521743233a63b8c2
incorporating speech recognition confidence into discriminative named entity recognition of speech data this paper proposes a named entity recognition ( ner ) method for speech recognition results that uses confidence on automatic speech recognition ( asr ) as a feature . the asr confidence feature indicates whether each word has been correctly recognized . the ner model is trained using asr results with named entity ( ne ) labels as well as the corresponding transcriptions with ne labels . in experiments using support vector machines ( svms ) and speech data from japanese newspaper articles , the proposed method outperformed a simple application of textbased ner to asr results in ner fmeasure by improving precision . these results show that the proposed method is effective in ner for noisy inputs .
RANK = 11; score = 0.30505558716836473; correct = False; id = 8d8155a1623fccd3eaeafd972a8b9a20080b81c8
using non - local features to improve named entity recognition recall named entity recognition ( ner ) is always limited by its lower recall resulting from the asymmetric data distribution where the none class dominates the entity classes . this paper presents an approach that exploits non - local information to improve the ner recall . several kinds of non - local features encoding entity token occurrence , entity boundary and entity class are explored under conditional random fields ( crfs ) framework . experiments on sighan 2006 msra ( cityu ) corpus indicate that non - local features can effectively enhance the recall of the state - of - the - art ner systems . incorporating the non - local features into the ner systems using local features alone , our best system achieves a 23.56 % ( 25.26 % ) relative error reduction on the recall and 17.10 % ( 11.36 % ) relative error reduction on the f1 score ; the improved f1 score 89.38 % ( 90.09 % ) is significantly superior to the best ner system with f1 of 86.51 % ( 89.03 % ) participated in the closed track .
RANK = 12; score = 0.3002583709234138; correct = False; id = 785eba2987f71cc34560ba1dcd3a5ffc133322e1
using n - best lists for named entity recognition from chinese speech we present the first known result for named entity recognition ( ner ) in realistic largevocabulary spoken chinese . we establish this result by applying a maximum entropy model , currently the single best known approach for textual chinese ner , to the recognition output of the bbn lvcsr system on chinese broadcast news utterances . our results support the claim that transferring ner approaches from text to spoken language is a significantly more difficult task for chinese than for english . we propose re - segmenting the asr hypotheses as well as applying postclassification to improve the performance . finally , we introduce a method of using n - best hypotheses that yields a small but nevertheless useful improvement ner accuracy . we use acoustic , phonetic , language model , ner and other scores as confidence measure . experimental results show an average of 6.7 % relative improvement in precision and 1.7 % relative improvement in f - measure .
RANK = 13; score = 0.2993722859003645; correct = False; id = 066502368061f8f002095c1fb8146e8b379cd595
exploiting morphology in turkish named entity recognition system turkish is an agglutinative language with complex morphological structures , therefore using only word forms is not enough for many computational tasks . in this paper we analyze the effect of morphology in a named entity recognition system for turkish . we start with the standard word - level representation and incrementally explore the effect of capturing syntactic and contextual properties of tokens . furthermore , we also explore a new representation in which roots and morphological features are represented as separate tokens instead of representing only words as tokens . using syntactic and contextual properties with the new representation provide an 7.6 % relative improvement over the baseline .
RANK = 14; score = 0.29662764980722456; correct = False; id = 22779b9189fda33abf84ed24d154bae69237ec0c
arabic named entity recognition : using features extracted from noisy data building an accurate named entity recognition ( ner ) system for languages with complex morphology is a challenging task . in this paper , we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate arabic ner system . we bootstrap noisy features by projection from an arabic - english parallel corpus that is automatically tagged with a baseline ner system . the feature space covers lexical , morphological , and syntactic features . the proposed approach yields an improvement of up to 1.64 f - measure ( absolute ) .
RANK = 15; score = 0.29567964607593444; correct = False; id = 3ad4fb6a4e1f438b98a770e2d49c2b8e51ae58aa
multi - language named - entity recognition system based on hmm we introduce a multi - language named - entity recognition system based on hmm . japanese , chinese , korean and english versions have already been implemented . in principle , it can analyze any other language if we have training data of the target language . this system has a common analytical engine and it can handle any language simply by changing the lexical analysis rules and statistical language model . in this paper , we describe the architecture and accuracy of the named - entity system , and report preliminary experiments on automatic bilingual named - entity dictionary construction using the japanese and english named - entity recognizer .
RANK = 16; score = 0.29541316925102845; correct = False; id = 0962f2a4452d7232fb031699ae933dccc77e5da0
named entity recognition for chinese social media with jointly trained embeddings we consider the task of named entity recognition for chinese social media . the long line of work in chinese ner has focused on formal domains , and ner for social media has been largely restricted to english . we present a new corpus of weibo messages annotated for both name and nominal mentions . additionally , we evaluate three types of neural embeddings for representing chinese text . finally , we propose a joint training objective for the embeddings that makes use of both ( ner ) labeled and unlabeled raw text . our methods yield a 9 % improvement over a stateof - the - art baseline .
RANK = 17; score = 0.29520868650404725; correct = False; id = 36ea0a9710b9310ce9c6ce199af63b6a00eea480
improving named entity recognition for chinese social media with word segmentation representation learning named entity recognition , and other information extraction tasks , frequently use linguistic features such as part of speech tags or chunkings . for languages where word boundaries are not readily identified in text , word segmentation is a key first step to generating features for an ner system . while using word boundary tags as features are helpful , the signals that aid in identifying these boundaries may provide richer information for an ner system . new state - of - the - art word segmentation systems use neural models to learn representations for predicting word boundaries . we show that these same representations , jointly trained with an ner system , yield significant improvements in ner for chinese social media . in our experiments , jointly training ner and word segmentation with an lstm - crf model yields nearly 5 % absolute improvement over previously published results .
RANK = 18; score = 0.280190940018932; correct = False; id = 303df576947198a7029ff384270750bdb8520521
language independent ner using a maximum entropy tagger named entity recognition ( ner ) systems need to integrate a wide variety of information for optimal performance . this paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy . the tagger uses features which can be obtained for a variety of languages and works effectively not only for english , but also for other languages such as german and dutch .
RANK = 19; score = 0.27978335764168716; correct = False; id = a0ed80f553f3a77a5b17542507bfc062eaf2c900
tree representations in probabilistic models for extended named entities detection in this paper we deal with named entity recognition ( ner ) on transcriptions of french broadcast data . two aspects make the task more difficult with respect to previous ner tasks : i ) named entities annotated used in this work have a tree structure , thus the task can not be tackled as a sequence labelling task ; ii ) the data used are more noisy than data used for previous ner tasks . we approach the task in two steps , involving conditional random fields and probabilistic context - free grammars , integrated in a single parsing algorithm . we analyse the effect of using several tree representations . our system outperforms the best system of the evaluation campaign by a significant margin .
RANK = 20; score = 0.2763592352853553; correct = False; id = 3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7
named entity recognition with bilingual constraints different languages contain complementary cues about entities , which can be used to improve named entity recognition ( ner ) systems . we propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple integer linear program , which encourages entity tags to agree via bilingual constraints . bilingual ner experiments on the large ontonotes 4.0 chinese - english corpus show that the proposed method can improve strong baselines for both chinese and english . in particular , chinese performance improves by over 5 % absolute f1 score . we can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method , and add it as uptraining data to the original monolingual ner training corpus . the chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % f1 score .

RANKING 2397
QUERY
unsupervised learning of style - sensitive word vectors correlation with human evaluation . a.k.a . word similarity task . unsupervised learning of style - sensitive word vectors reina akama ( reina.a@ecei.tohoku.ac.jp ) , kento watanabe , sho yokoi , sosuke kobayashi , kentaro inui 1graduate school of information sciences , tohoku university , 2national institute of advanced industrial science and technology ( aist ) , 3preferred networks , inc. , 4riken center for advanced intelligence project
First cited at 106
TOP CITED PAPERS
RANK 106
inducing lexical style properties for paraphrase and genre differentiation we present an intuitive and effective method for inducing style scores on words and phrases . we exploit signal in a phrase ’s rate of occurrence across stylistically contrasting corpora , making our method simple to implement and efficient to scale . we show strong results both intrinsically , by correlation with human judgements , and extrinsically , in applications to genre analysis and paraphrasing .
RANK 2579
a persona - based neural conversation model we present persona - based models for handling the issue of speaker consistency in neural response generation . a speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style . a dyadic speakeraddressee model captures properties of interactions between two interlocutors . our models yield qualitative performance improvements in both perplexity and bleu scores over baseline sequence - to - sequence models , with similar gains in speaker consistency as measured by human judges .
RANK 3139
simverb-3500 : a large - scale evaluation set of verb similarity verbs play a critical role in the meaning of sentences , but these ubiquitous words have received little attention in recent distributional semantics research . we introduce simverb-3500 , an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs . simverb-3500 covers all normed verb types from the usf free - association database , providing at least three examples for every verbnet class . this broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning . further , with significantly larger development and test sets than existing benchmarks , simverb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs . we hope that simverb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning .
TOP UNCITED PAPERS
RANK 1
problems with evaluation of word embeddings using word similarity tasks lacking standardized extrinsic evaluation methods for vector representations of words , the nlp community has relied heavily onword similaritytasks as a proxy for intrinsic evaluation of word vectors . word similarity evaluation , which correlates the distance between vectors and human judgments of “ semantic similarity ” is attractive , because it is computationally inexpensive and fast . in this paper we present several problems associated with the evaluation of word vectors on word similarity datasets , and summarize existing solutions . our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods .
RANK 2
proceedings of the 24th pacific asia conference on language , information and computation , paclic 24 , tohoku university , japan , 4 - 7 november 2010 
RANK 3
idi$@$ntnu at semeval-2016 task 6 : detecting stance in tweets using shallow features and glove vectors for word representation this paper describes an approach to automatically detect stance in tweets by building a supervised system combining shallow features and pre - trained word vectors as word representation . the word vectors were obtained from several collections of large corpora using glove , an unsupervised learning algorithm . we created feature vectors by selecting the word vectors relevant to the data and summing them for each unique word . combining multiple classifiers into a voting classifier , representing the best of both approaches , shows a significant improvement over the baseline system .
TOP 20
RANK = 1; score = 0.2244784744093376; correct = False; id = 24fc66e5c85b228d3ab23509788e9c34bea7ed2d
problems with evaluation of word embeddings using word similarity tasks lacking standardized extrinsic evaluation methods for vector representations of words , the nlp community has relied heavily onword similaritytasks as a proxy for intrinsic evaluation of word vectors . word similarity evaluation , which correlates the distance between vectors and human judgments of “ semantic similarity ” is attractive , because it is computationally inexpensive and fast . in this paper we present several problems associated with the evaluation of word vectors on word similarity datasets , and summarize existing solutions . our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods .
RANK = 2; score = 0.20345970901662494; correct = False; id = 33502d9a7505c81774c787c9f220d070c322d0e5
proceedings of the 24th pacific asia conference on language , information and computation , paclic 24 , tohoku university , japan , 4 - 7 november 2010 
RANK = 3; score = 0.1893252485805862; correct = False; id = 31bba5952e7043b6f249c5eab455e8056afd1cd7
idi$@$ntnu at semeval-2016 task 6 : detecting stance in tweets using shallow features and glove vectors for word representation this paper describes an approach to automatically detect stance in tweets by building a supervised system combining shallow features and pre - trained word vectors as word representation . the word vectors were obtained from several collections of large corpora using glove , an unsupervised learning algorithm . we created feature vectors by selecting the word vectors relevant to the data and summing them for each unique word . combining multiple classifiers into a voting classifier , representing the best of both approaches , shows a significant improvement over the baseline system .
RANK = 4; score = 0.1883381077330888; correct = False; id = dabffce543b1b01404921d49e36e26b58c920464
word vectors and two kinds of similarity this paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model . through two experiments , three methods for constructing word vectors , i.e. , lsa - based , cooccurrence - based and dictionary - based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , taxonomic similarity and associative similarity . the result of the comparison was that the dictionary - based word vectors better reflect taxonomic similarity , while the lsabased and the cooccurrence - based word vectors better reflect associative similarity .
RANK = 5; score = 0.17224005078129803; correct = False; id = 9a0e07601cd147cb973009a3929ad22028046afe
towards style transformation from written - style to audio - style in this paper , we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read . we study the differences between the written style and the audio style by consulting the linguistics and journalism literatures . guided by this study , we suggest a number of linguistic features to distinguish between the two styles . we show the correctness of our features and the impact of style transformation on the user experience through statistical analysis , a style classification task , and a user study .
RANK = 6; score = 0.1699927095626794; correct = False; id = adaeaf18fc9da2dd49695059755bc959342c2325
towards dynamic word sense discrimination with random indexing most distributional models of word similarity represent a word type by a single vector of contextual features , even though , words commonly have more than one sense . the multiple senses can be captured by employing several vectors per word in a multi - prototype distributional model , prototypes that can be obtained by first constructing all the context vectors for the word and then clustering similar vectors to create sense vectors . storing and clustering context vectors can be expensive though . as an alternative , we introduce multi - sense random indexing , which performs on - the - fly ( incremental ) clustering . to evaluate the method , a number of measures for word similarity are proposed , both contextual and non - contextual , including new measures based on optimal alignment of word senses . experimental results on the task of predicting semantic textual similarity do , however , not show a systematic difference between singleprototype and multi - prototype models .
RANK = 7; score = 0.15526790776630076; correct = False; id = 591b22bd49c7b7947018d4e08442c6ec4cdd07cc
non - distributional word vector representations data - driven representation learning for words is a technique of central importance in nlp . while indisputably useful as a source of features in downstream tasks , such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best . we present a method for constructing interpretable word vectors from hand - crafted linguistic resources like wordnet , framenet etc . these vectors are binary ( i.e , contain only 0 and 1 ) and are 99.9 % sparse . we analyze their performance on state - of - the - art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches .
RANK = 8; score = 0.15417978866770737; correct = False; id = beb12fc82cdc04169214c89059b3c849fc0655f1
intrinsic evaluation of word vectors fails to predict extrinsic performance the quality of word representations is frequently assessed using correlation with human judgements of word similarity . here , we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks . we study the correlation between results on ten word similarity benchmarks and tagger performance on three standard sequence labeling tasks using a variety of word vectors induced from an unannotated corpus of 3.8 billion words , and demonstrate that most intrinsic evaluations are poor predictors of downstream performance . we argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets . we make our evaluation tools openly available to facilitate further study .
RANK = 9; score = 0.14792104655681576; correct = False; id = 547fcbfe295e48ff8380eb9aeb79c4588bed5d42
learning text pair similarity with context - sensitive autoencoders we present a pairwise context - sensitive autoencoder for computing text pair similarity . our model encodes input text into context - sensitive representations and uses them to compute similarity between text pairs . our model outperforms the state - of - the - art models in two semantic retrieval tasks and a contextual word similarity task . for retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state - of - the - art supervised models and in some cases outperforms them .
RANK = 10; score = 0.14758369707119987; correct = False; id = 30c6ce423db5ca40969b57d96ba029089946c4e6
correlation - based intrinsic evaluation of word vector representations we introduce qvec - cca — an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources . we show that qveccca scores are an effective proxy for a range of extrinsic semantic and syntactic tasks . we also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks , compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity .
RANK = 11; score = 0.1469342209051295; correct = False; id = 7c09296d1e4f2956d0b86af83c41cabf5b0a4980
a two level model for context sensitive inference rules automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words , operating at the word space level . a recent line of work , which addresses context sensitivity of rules , represented contexts in a latent topic space and computed similarity over topic vectors . we propose a novel two - level model , which computes similarities between word - level vectors that are biased by topic - level context representations . evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word - level and topic - level models . we also release a first context - sensitive inference rule set .
RANK = 12; score = 0.14482824598539773; correct = False; id = 3f465d5be566a44a4ce64d5fe5fc5d745c2aa5af
a practical solution to the problem of automatic word sense induction recent studies in word sense induction are based on clustering global co - occurrence vectors , i.e. vectors that reflect the overall behavior of a word in a corpus . if a word is semantically ambiguous , this means that these vectors are mixtures of all its senses . inducing a word ’s senses therefore involves the difficult problem of recovering the sense vectors from the mixtures . in this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word . from human disambiguation performance we know that the context of a word is usually sufficient to determine its sense . based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts . the main difficulty with this approach , namely the problem of data sparseness , could be minimized by looking at only the three main dimensions of the context matrices .
RANK = 13; score = 0.13785534502252075; correct = False; id = 0cc3388cf91bb8737a328d0039fecd6c2a6c4824
nonlocal language modeling based on context co - occurrence vectors this paper presents a novel nonlocal language model which utilizes contextual information . a reduced vector space model calculated from co - occurrences of word pairs provides word co - occurrence vectors . the sum of word cooccurrence vectors represents the context of a document , and the cosine similarity between the context vector and the word co - occurrence vectors represents the long - distance lexical dependencies . experiments on the mainichi newspaper corpus show signi ca nt improvement in perplexity ( 5.0 % overall and 27.2 % on target vocabulary )
RANK = 14; score = 0.13731765323958542; correct = False; id = 22d185c7ba066468f9ff1df03f1910831076e943
learning better embeddings for rare words using distributional representations there are two main types of word representations : low - dimensional embeddings and high - dimensional distributional vectors , in which each dimension corresponds to a context word . in this paper , we initialize an embedding - learning model with distributional vectors . evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words .
RANK = 15; score = 0.13651782556308747; correct = False; id = 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
convolutional neural networks for sentence classification we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre - trained word vectors for sentence - level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task - specific vectors through fine - tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task - specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
RANK = 16; score = 0.13633792782546406; correct = False; id = 12c81bb90a11cfcbe7ec5af00fc50d7750c140c8
evaluation of word vector representations by subspace alignment unsupervisedly learned word vectors have proven to provide exceptionally effective features in many nlp tasks . most common intrinsic evaluations of vector quality measure correlation with similarity judgments . however , these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks . we present qvec — a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources — that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1
RANK = 17; score = 0.13173493628476146; correct = False; id = e0fb3ab6dfdf9c351e2c837e480850b4186aa961
dls$@$cu : sentence similarity from word alignment and semantic vector composition we describe a set of top - performing systems at the semeval 2015 english semantic textual similarity ( sts ) task . given two english sentences , each system outputs the degree of their semantic similarity . our unsupervised system , which is based on word alignments across the two input sentences , ranked 5th among 73 submitted system runs with a mean correlation of 79.19 % with human annotations . we also submitted two runs of a supervised system which uses word alignments and similarities between compositional sentence vectors as its features . our best supervised run ranked 1st with a mean correlation of 80.15 % .
RANK = 18; score = 0.1308279376534136; correct = False; id = e0f9e8fb44636ddd7f62d8d7c233ebb5b2189b59
using lexical expansion to learn inference rules from sparse data automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words . in this scheme , prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability . to improve the learning of such rules in an unsupervised way , we propose to lexically expand sparse argument word vectors with semantically similar words . our evaluation shows that lexical expansion significantly improves performance in comparison to state - of - the - art baselines .
RANK = 19; score = 0.12925866168810382; correct = False; id = 6a9b5bcafc5e88d731f644cf7ea59547df20495a
online learning of interpretable word embeddings word embeddings encode semantic meanings of words into low - dimension word vectors . in most word embeddings , one can not interpret the meanings of specific dimensions of those word vectors . nonnegative matrix factorization ( nmf ) has been proposed to learn interpretable word embeddings via non - negative constraints . however , nmf methods suffer from scale and memory issue because they have to maintain a global matrix for learning . to alleviate this challenge , we propose online learning of interpretable word embeddings from streaming text data . experiments show that our model consistently outperforms the state - of - the - art word embedding methods in both representation ability and interpretability . the source code of this paper can be obtained from http : //github.com / sktim / oiwe .
RANK = 20; score = 0.12523962898512195; correct = False; id = 8a7db2720d7711a4d65579030c4c7c2b899d08b4
evaluating unsupervised learning for natural language processing tasks the development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research . the primary advantage of these methods is that they do not require annotated data to learn a model . however , this advantage makes them difficult to evaluate against a manually labeled gold standard . using unsupervised part - of - speech tagging as our case study , we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods . instead , we argue that the rarely used in - context evaluation is more appropriate and more informative , as it takes into account the way these methods are likely to be applied . finally , bearing the issue of evaluation in mind , we propose directions for future work in unsupervised natural language processing .

RANKING 900
QUERY
personalized language model for query auto - completion query auto - completion is a search engine feature whereby the system suggests completed queries as the user types . recently , the use of a recurrent neural network language model was suggested as a method of generating query completions . we show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training . the personalized predictions are significantly better than a baseline that uses no user information .
First cited at 2
TOP CITED PAPERS
RANK 2
enriching cold start personalized language model using social network information personalized language models are useful in many applications , such as personalized search and personalized recommendation . nevertheless , it is challenging to build a personalized language model for cold start users , in which the size of the training corpus of those users is too small to create a reasonably accurate and representative model . we introduce a generalized framework to enrich the personalized language models for cold start users . the cold start problem is solved with content written by friends on social network services . our framework consists of a mixture language model , whose mixture weights are estimated with a factor graph . the factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights . the intrinsic and extrinsic experiments show significant improvement on cold start users .
RANK 3669
a persona - based neural conversation model we present persona - based models for handling the issue of speaker consistency in neural response generation . a speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style . a dyadic speakeraddressee model captures properties of interactions between two interlocutors . our models yield qualitative performance improvements in both perplexity and bleu scores over baseline sequence - to - sequence models , with similar gains in speaker consistency as measured by human judges .
TOP UNCITED PAPERS
RANK 1
an iterative implicit feedback approach to personalized search general information retrieval systems are designed to serve all users without considering individual needs . in this paper , we propose a novel approach to personalized search . it can , in a unified way , exploit and utilize implicit feedback information , such as query logs and immediately viewed documents . moreover , our approach can implement result re - ranking and query expansion simultaneously and collaboratively . based on this approach , we develop a client - side personalized web search agent pair ( personalized assistant for information retrieval ) , which supports both english and chinese . our experiments on trec and htrdp collections clearly show that the new approach is both effective and efficient .
RANK 3
personalized machine translation : predicting translational preferences machine translation ( mt ) has advanced in recent years to produce better translations for clients’ specific domains , and sophisticated tools allow professional translators to obtain translations according to their prior edits . we suggest that mt should be further personalized to the end - user level – the receiver or the author of the text – as done in other applications . as a step in that direction , we propose a method based on a recommender systems approach where the user ’s preferred translation is predicted based on preferences of similar users . in our experiments , this method outperforms a set of non - personalized methods , suggesting that user preference information can be employed to provide better - suited translations for each user .
RANK 4
query weighting for ranking model adaptation we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available , which is referred to as query weighting . query weighting is a key step in ranking model adaptation . as the learning object of ranking algorithms is divided by query instances , we argue that it ’s more reasonable to conduct importance weighting at query level than document level . we present two query weighting schemes . the first compresses the query into a query feature vector , which aggregates all document instances in the same query , and then conducts query weighting based on the query feature vector . this method can efficiently estimate query importance by compressing query data , but the potential risk is information loss resulted from the compression . the second measures the similarity between the source query and each target query , and then combines these fine - grained similarity values for its importance estimation . adaptation experiments on letor3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods .
TOP 20
RANK = 1; score = 0.31243121012605507; correct = False; id = 140f82838ef5f4df113cbc4e8c42be70a083013d
an iterative implicit feedback approach to personalized search general information retrieval systems are designed to serve all users without considering individual needs . in this paper , we propose a novel approach to personalized search . it can , in a unified way , exploit and utilize implicit feedback information , such as query logs and immediately viewed documents . moreover , our approach can implement result re - ranking and query expansion simultaneously and collaboratively . based on this approach , we develop a client - side personalized web search agent pair ( personalized assistant for information retrieval ) , which supports both english and chinese . our experiments on trec and htrdp collections clearly show that the new approach is both effective and efficient .
RANK = 2; score = 0.30476091566825675; correct = True; id = 4967dd5dd4639bbfbc221a29ffbc0d919ba6e6a9
enriching cold start personalized language model using social network information personalized language models are useful in many applications , such as personalized search and personalized recommendation . nevertheless , it is challenging to build a personalized language model for cold start users , in which the size of the training corpus of those users is too small to create a reasonably accurate and representative model . we introduce a generalized framework to enrich the personalized language models for cold start users . the cold start problem is solved with content written by friends on social network services . our framework consists of a mixture language model , whose mixture weights are estimated with a factor graph . the factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights . the intrinsic and extrinsic experiments show significant improvement on cold start users .
RANK = 3; score = 0.262918169302335; correct = False; id = a79149613f4c66802d00b31e9e952f53df44c8c3
personalized machine translation : predicting translational preferences machine translation ( mt ) has advanced in recent years to produce better translations for clients’ specific domains , and sophisticated tools allow professional translators to obtain translations according to their prior edits . we suggest that mt should be further personalized to the end - user level – the receiver or the author of the text – as done in other applications . as a step in that direction , we propose a method based on a recommender systems approach where the user ’s preferred translation is predicted based on preferences of similar users . in our experiments , this method outperforms a set of non - personalized methods , suggesting that user preference information can be employed to provide better - suited translations for each user .
RANK = 4; score = 0.2500332164914342; correct = False; id = 281c587dddbda1ad32f7566d44d18c5f771e5cb2
query weighting for ranking model adaptation we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available , which is referred to as query weighting . query weighting is a key step in ranking model adaptation . as the learning object of ranking algorithms is divided by query instances , we argue that it ’s more reasonable to conduct importance weighting at query level than document level . we present two query weighting schemes . the first compresses the query into a query feature vector , which aggregates all document instances in the same query , and then conducts query weighting based on the query feature vector . this method can efficiently estimate query importance by compressing query data , but the potential risk is information loss resulted from the compression . the second measures the similarity between the source query and each target query , and then combines these fine - grained similarity values for its importance estimation . adaptation experiments on letor3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods .
RANK = 5; score = 0.24829422003813414; correct = False; id = 7485578583dda7e2fcc4afb94e4b30245013d5fc
a flexible stand - off data model with query language for multi - level annotation we present an implemented xml data model and a new , simplified query language for multi - level annotated corpora . the new query language involves automatic conversion of queries into the underlying , more complicated mmaxql query language . it supports queries for sequential and hierarchical , but also associative ( e.g. coreferential ) relations . the simplified query language has been designed with non - expert users in mind .
RANK = 6; score = 0.2426866083883059; correct = False; id = 96e95c2e416cc9a4402c9a547758197cb204d85c
language - independent context aware query translation using wikipedia the exponential growth of the world - wide web has transformed it into an ecology of knowledge in which highly diverse information is linked in an extremely complex and arbitrary manner . at the same time , huge multilingual content is being added to it through various sources like blogs , news articles and several other forms . to facilitate the access of this enormous multilingual content , cross lingual information access systems ( clia ) are vital . clia systems take users’ information need and returns the required information to them . the information can be obtained from various sources and different languages , reducing the effort of user to search in different information sources . query translation is the most important task in building an effective clia system as the accuracy of the results are dependent on the accuracy of the query translation . the user input , usually few words , is considered as the query . given the differences in vocabulary across the languages , we have to capture the context of the query to increase the efficiency of the query translation . query translation forms the vital component of any clia system as it captures the users need and translating queries effectively is required . we devised a language independent approach that captures the users intent from his query by mapping query words to wikipedia topics . wikipedia , the free on - line encyclopedia , is a hypertext document collection with a rich link structure . unlike ordinary encyclopedias , wikipedia is based on the wiki concept , which enables community to create or improve articles . wikipedia articles cover many domain - specific and general topics , making it a rich knowledge resource . due to its unique concept , wikipedia is populated with articles in around 240 languages , making it a rich multi - lingual resource . we intend to leverage this multi lingual knowledge base as a resource to aid multi lingual query translation . in this work , we propose a query translation method that uses the rich link structure across the languages present in wikipedia . the query translation approach we have adopted is language independent and focuses on minimizing the use of language specific resources , making our approach scalable . this approach is specifically useful for under - resourced languages ( indian , african etc ) as these languages do not have sufficient resources and tools with good accuracies . for these under - resourced languages , a query translation approach is proposed using the resources mined from wikipedia substituting the language - specific resources for query translation . using the cross lingual links and the articles that are connected with these links , we build dictionaries that forms the resource for query translation . a language independent approach to identify parallel text is also proposed . a parallel text is a text placed alongside its translation or translations . parallel text forms the major resource for many areas of linguistic research and their availability helps to identify many retrieval problems between the pair of languages . the query translation approach we have adopted takes the context of the query into consideration to perform the translation . we use wikipedia concepts to mine the context of the user query and use them to form the translation . an evaluation method to evaluate query translation is also proposed using wikipedia , by mining statistics from the articles and the cross lingual links . a separate evaluation method is required in - line with our intent to develop the system without using any language resources due to their non - availability . the coverage of the dictionaries is 78.1 % while the accuracy of the query translation is 69.75 % . in this work , we develop a language - independent approach to query translation that also considers the context of the query . since no language resources were used , our approach is scalable and can be applied to any pair of languages present in wikipedia . the performance of the system is encouraging and condone our intention of substituting wikipedia for language resources . further the resources mined and approaches proposed can be leveraged in various areas of linguistic research . parallel sentence extraction approach can be used to solve the problem of cross lingual plagiarism detection . extraction of context from a sentence can be extended to support update summarization task . thus , in this work , we suggest a language independent query translation approach which can be a right direction towards building a clia system for less resourced languages .
RANK = 7; score = 0.23529958944318674; correct = False; id = 4886b3a08e3ff7ff5463aed1297bdbdf2e9eecbb
identifying web search query reformulation using concept based matching web search users frequently modify their queries in hope of receiving better results . this process is referred to as “ query reformulation ” . previous research has mainly focused on proposing query reformulations in the form of suggested queries for users . some research has studied the problem of predicting whether the current query is a reformulation of the previous query or not . however , this work has been limited to bag - of - words models where the main signals being used are word overlap , character level edit distance and word level edit distance . in this work , we show that relying solely on surface level text similarity results in many false positives where queries with different intents yet similar topics are mistakenly predicted as query reformulations . we propose a new representation for web search queries based on identifying the concepts in queries and show that we can significantly improve query reformulation performance using features of query concepts .
RANK = 8; score = 0.23350802965655362; correct = False; id = 249525d676573bc52ba00cbd559d4539733e8642
personalized recommendation of user comments via factor models in recent years , the amount of user - generated opinionated texts ( e.g. , reviews , user comments ) continues to grow at a rapid speed : featured news stories on a major event easily attract thousands of user comments on a popular online news service . how to consume subjective information of this volume becomes an interesting and important research question . in contrast to previous work on review analysis that tried to filter or summarize information for a generic average user , we explore a different direction of enabling personalized recommendation of such information . for each user , our task is to rank the comments associated with a given article according to personalized user preference ( i.e. , whether the user is likely to like or dislike the comment ) . to this end , we propose a factor model that incorporates rater - comment and rater - author interactions simultaneously in a principled way . our full model significantly outperforms strong baselines as well as related models that have been considered in previous work .
RANK = 9; score = 0.23253415960719578; correct = False; id = a28670b3a14f8bec873a381f67326dfa3ac5c011
language identification of search engine queries we consider the language identification problem for search engine queries . first , we propose a method to automatically generate a data set , which uses clickthrough logs of the yahoo ! search engine to derive the language of a query indirectly from the language of the documents clicked by the users . next , we use this data set to train two decision tree classifiers ; one that only uses linguistic features and is aimed for textual language identification , and one that additionally uses a non - linguistic feature , and is geared towards the identification of the language intended by the users of the search engine . our results show that our method produces a highly reliable data set very efficiently , and our decision tree classifier outperforms some of the best methods that have been proposed for the task of written language identification on the domain of search engine queries .
RANK = 10; score = 0.2246676155937672; correct = False; id = 2c69e02b319ea809725aa28462c2fbbde91cf20f
mixed language query disambiguation we propose a mixed language query disambiguation approach by using co - occurrence information from monolingual data only . a mixed language query consists of words in a primary language and a secondary language . our method translates the query into monolingual queries in either language . two novel features for disambiguation , namely contextual word voting and 1-best contextual word , are introduced and compared to a baseline feature , the nearest neighbor . average query translation accuracy for the two features are 81.37 % and 83.72 % , compared to the baseline accuracy
RANK = 11; score = 0.22267046846235786; correct = False; id = 8bd0df364a5a0e53ce34273ea3fae6598b3498be
an approach of generating personalized views from normalized electronic dictionaries : a practical experiment on arabic language electronic dictionaries covering all natural language levels are very relevant for the human use as well as for the automatic processing use , namely those constructed with respect to international standards . such dictionaries are characterized by a complex structure and an important access time when using a querying system . however , the need of a user is generally limited to a part of such a dictionary according to his domain and expertise level which corresponds to a specialized dictionary . given the importance of managing a unified dictionary and considering the personalized needs of users , we propose an approach for generating personalized views starting from a normalized dictionary with respect to lexical markup framework lmf - iso 24613 norm . this approach provides the re - use of already defined views for a community of users by managing their profiles information and promoting the materialization of the generated views . it is composed of four main steps : ( i ) the projection of data categories controlled by a set of constraints ( related to the user‟s profiles ) , ( ii ) the selection of values with consistency checking , ( iii ) the automatic generation of the query‟s model and finally , ( iv ) the refinement of the view . the proposed approach was consolidated by carrying out an experiment on an lmf normalized arabic dictionary .
RANK = 12; score = 0.2205160898091272; correct = False; id = 0d5857919bb29541c5a28a92821b6b0165c59837
automatic generation of personalized annotation tags for twitter users this paper introduces a system designed for automatically generating personalized annotation tags to label twitter user ’s interests and concerns . we applied tfidf ranking and textrank to extract keywords from twitter messages to tag the user . the user tagging precision we obtained is comparable to the precision of keyword extraction from web pages for content - targeted advertising .
RANK = 13; score = 0.21849405243636524; correct = False; id = 222cf1c2c0d838aa8e5099d434cb24ea021329df
query translation in chinese - english cross - language information retrieval this paper proposed a new query translation method based on the mutual information matrices of terms in the chinese and english corpora . instead of looking up a • bilingual phrase dictionary , the compositional phrase ( the translation of phrase can be derived from the translation of its components ) in the query can be indirectly translated via a general - purpose chinese - english dictionary look - up procedure . a novel selection method for translations of query terms is also presented in detail . our query translation method ultimately constructs an english query in which each query term has a weight . the evaluation results show that the retrieval performance achieved by our query translation method is about 73 % of monolingual information retrieval and is about 28 % higher than that of simple wordby - word translation way .
RANK = 14; score = 0.21544039066694157; correct = False; id = 9cfbd1641866f0693689ca6f87f7c6a0cb89f296
improving query spelling correction using web search results traditional research on spelling correction in natural language processing and information retrieval literature mostly relies on pre - defined lexicons to detect spelling errors . but this method does not work well for web query spelling correction , because there is no lexicon that can cover the vast amount of terms occurring across the web . recent work showed that using search query logs helps to solve this problem to some extent . however , such approaches can not deal with rarely - used query terms well due to the data sparseness problem . in this paper , a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top - ranked candidate . experiments are performed based on realworld queries randomly sampled from search engine ’s daily logs , and the results show that our new method can achieve 16.9 % relative f - measure improvement and 35.4 % overall error rate reduction in comparison with the baseline method .
RANK = 15; score = 0.2140058918531981; correct = False; id = 7f44ee25da7dd88d40a49c390dcf0dee7ec85499
japanese query alteration based on lexical semantic similarity we propose a unified approach to web search query alterations in japanese that is not limited to particular character types or orthographic similarity between a query and its alteration candidate . our model is based on previous work on english query correction , but makes some crucial improvements : ( 1 ) we augment the query - candidate list to include orthographically dissimilar but semantically similar pairs ; and ( 2 ) we use kernel - based lexical semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity . we also propose an efficient method for generating query - candidate pairs for model training and testing . we show that the proposed method achieves about 80 % accuracy on the query alteration task , improving over previously proposed methods that use semantic similarity .
RANK = 16; score = 0.2130317491869079; correct = False; id = 606f4dea3f2457f7a7fd237c025bb79142490d2a
syntactic parsing of web queries syntactic parsing of web queries is important for query understanding . however , web queries usually do not observe the grammar of a written language , and no labeled syntactic trees for web queries are available . in this paper , we focus on a query ’s clicked sentence , i.e. , a well - formed sentence that i ) contains all the tokens of the query , and ii ) appears in the query ’s top clicked web pages . we argue such sentences are semantically consistent with the query . we introduce algorithms to derive a query ’s syntactic structure from the dependency trees of its clicked sentences . this gives us a web query treebank without manual labeling . we then train a dependency parser on the treebank . our model achieves much better uas ( 0.86 ) and las ( 0.80 ) scores than state - of - the - art parsers on web queries .
RANK = 17; score = 0.2129933244162198; correct = False; id = 312b1cbf0ff2cf302596e734b071d2936c98776d
query ambiguity revisited : clickthrough measures for distinguishing informational and ambiguous queries introduction  query interpretation is the first crucial step in the operation of web search engines . • personalized search needs to understand which meaning of an ambiguous query is the preference of the user . • search engines can categorize retrieved results according to different aspects of an ambiguous query , providing more structured view to users .  ambiguous queries are difficult to detect because they share a similar clickthrough pattern with informational queries , in the view of query - url bipartite clickthrough graph .  our approach : distinguishing informational queries and ambiguous queries through user clickthrough pattern . intelligence information access lab
RANK = 18; score = 0.21036454776805455; correct = False; id = 44e5d13cfcb9ebd3117d4b457c08931d83602050
summarization - based query expansion in information retrieval we discuss a seml - interactive approach to information retrieval which consists of two tasks performed in a sequence . first , the system assists the searcher in building a comprehensive statement of information need , using automatically generated topical summaries of sample documents . second , the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system . in this paper , we investigate the role of automated document summarization in building effective search statements . we also discuss the results of latest evaluation of our system at the annual text retrieval conference ( tkec ) . i n f o r m a t i o n r e t ~ r i e v a l information retrieval ( ir ) is a task of selecting documents from a database in response to a user 's query , and ranking them according to relevance . this has been usually accomplished using statistical methods ( often coupled with manual encoding ) that ( a ) select terms ( words , phrases , and other units ) from documents that are deemed to best represent their content , and ( b ) create an inverted index file ( or files ) that provide an easy access to documents containing these terms . a subsequent search process at tempts to match preprocessed user queries against termbased representations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms . a search is successful if it can return as many as possible documents which are relevant to the query , with as few as possible non - relevant documents . in addition , the relevant documents should be ranked ahead of non - relevant ones . the quantitative tex~ representation methods , predominant in today 's leading information retrieval systems 1 limit ii ~ epresentations anchored on words , word or charthe system 's ability to generate a successful search because they rely more on the , form of a query than on its content in finding document matches . this problem is particularly acute in ad - hoc retrieval situations where the user has only a limited knowledge of database composition and needs to resort to generic or otherwise incomplete search statements . iri order to overcome this limitation , mariy ir systems allow varying degrees of user interaction that facilitates query optimization and calibration to closer match user 's information seeking goals . a popular technique here is relevance feedback , where the user or the system judges the relevance of a sample of resuits returned from an initial search , and the query is subsequently rebuilt to reflect this information . automatic relevance feedback techniques can lead to a very close mapping of known relevant documents , however , they also tend to overflt , which in turn reduces their ability of finding new documents on the same subject . therefore , a serious challenge for information retrieval is to devise methods for building better queries , or in assisting user to do so . b u i l d i n g e f f e c t i v e s e a r c h q u e r i e s we have been experimenting with manual and automatic natural language query ( or topic , in trec parlance ) building techniques . this differs from most query modification techniques used in ir in that our method is to reformulate the user 's state~ ment of information need rather than the search system 's internal representation of it , as relevance feedback does . our goal is to devise a method of fulltext expansion that would allow for creating exhaustive search topics such that : ( 1 ) the performance of any system using the expanded topics would be significantly better than when the system is run using the original topics , and ( 2 ) the method of topic acter sequences , or some surrogates of these , along with significance weights derived from their distribution in the database .
RANK = 19; score = 0.20966279078566966; correct = False; id = 32664255ad097b3616057bc559881a796b211fe0
web search intent induction via automatic query reformulation we present a computationally efficient method for automatic grouping of web search results based on reformulating the original query to alternative queries the user may have intended . the method requires no data other than query logs and the standard inverted indices used by most search engines . our method outperforms standard web search in the task of enabling users to quickly find relevant documents for informational queries .
RANK = 20; score = 0.20918262853689754; correct = False; id = f2c16b87ae4afd1d6ed0524e3d30e76b891000f7
selecting query term alternations for web search by exploiting query contexts query expansion by word alterations ( alternative forms of a word ) is often used in web search to replace word stemming . this allows users to specify particular word forms in a query . however , if many alterations are added , query traffic will be greatly increased . in this paper , we propose methods to select only a few useful word alterations for query expansion . the selection is made according to the appropriateness of the alteration to the query context ( using a bigram language model ) , or according to its expected impact on the retrieval effectiveness ( using a regression model ) . our experiments on two trec collections will show that both methods only select a few expansion terms , but the retrieval effectiveness can be improved significantly .

RANKING 1671
QUERY
self - attention with relative position representations relying entirely on an attention mechanism , the transformer introduced by vaswani et al . ( 2017 ) achieves state - of - the - art results for machine translation . in contrast to recurrent and convolutional neural networks , it does not explicitly model relative or absolute position information in its structure . instead , it requires adding representations of absolute positions to its inputs . in this work we present an alternative approach , extending the self - attention mechanism to efficiently consider representations of the relative positions , or distances between sequence elements . on the wmt 2014 english - to - german and english - to - french translation tasks , this approach yields improvements of 1.3 bleu and 0.3 bleu over absolute position representations , respectively . notably , we observe that combining relative and absolute position representations yields no further improvement in translation quality . we describe an efficient implementation of our method and cast it as an instance of relation - aware self - attention mechanisms that can generalize to arbitrary graphlabeled inputs .
First cited at 1
TOP CITED PAPERS
RANK 1
effective approaches to attention - based neural machine translation an attentional mechanism has lately been used to improve neural machine translation ( nmt ) by selectively focusing on parts of the source sentence during translation . however , there has been little work exploring useful architectures for attention - based nmt . this paper examines two simple and effective classes of attentional mechanism : a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . we demonstrate the effectiveness of both approaches on the wmt translation tasks between english and german in both directions . with local attention , we achieve a significant gain of 5.0 bleu points over non - attentional systems that already incorporate known techniques such as dropout . our ensemble model using different attention architectures yields a new state - of - the - art result in the wmt’15 english to german translation task with 25.9 bleu points , an improvement of 1.0 bleu points over the existing best system backed by nmt and an n - gram reranker.1
RANK 17
a decomposable attention model for natural language inference we propose a simple neural architecture for natural language inference . our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable . on the stanford natural language inference ( snli ) dataset , we obtain state - of - the - art results with almost an order of magnitude fewer parameters than previous work and without relying on any word - order information . adding intra - sentence attention that takes a minimum amount of order into account yields further improvements .
RANK 951
learning phrase representations using rnn encoder -- decoder for statistical machine translation in this paper , we propose a novel neural network model called rnn encoder– decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder – decoder as an additional feature in the existing log - linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .
TOP UNCITED PAPERS
RANK 2
a position encoding convolutional neural network based on dependency tree for relation classification . with the renaissance of neural network in recent years , relation classification has again become a research hotspot in natural language processing , and leveraging parse trees is a common and effective method of tackling this problem . in this work , we offer a new perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network ( pecnn ) based on dependency parse tree for relation classification . first , treebased position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations . then , based on a redefinition of “ context ” , we design two kinds of tree - based convolution kernels for capturing the semantic and structural information provided by dependency trees . finally , the features extracted by convolution module are fed to a classifier for labelling the semantic relations . experiments on the benchmark dataset show that pecnn outperforms state - of - the - art approaches . we also compare the effect of different position features and visualize the influence of treebased position feature by tracing back the convolution process .
RANK 3
inner attention based recurrent neural networks for answer selection attention based recurrent neural networks have shown advantages in representing natural language sentences ( hermann et al . , 2015 ; rocktäschel et al . , 2015 ; tan et al . , 2015 ) . based on recurrent neural networks ( rnn ) , external attention information was added to hidden representations to get an attentive sentence representation . despite the improvement over nonattentive models , the attention mechanism under rnn is not well studied . in this work , we analyze the deficiency of traditional attention based rnn models quantitatively and qualitatively . then we present three new rnn models that add attention information before rnn hidden representation , which shows advantage in representing sentence and achieves new stateof - art results in answer selection task .
RANK 4
learning continuous phrase representations for translation modeling this paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations . a pair of source and target phrases are projected into continuous - valued vector representations in a low - dimensional latent space , where their translation score is computed by the distance between the pair in this new space . the projection is performed by a neural network whose weights are learned on parallel training data . experimental evaluation has been performed on two wmt translation tasks . our best result improves the performance of a state - of - the - art phrase - based statistical machine translation system trained on wmt 2012 french - english data by up to 1.3 bleu points .
TOP 20
RANK = 1; score = 0.26556318089294917; correct = True; id = 1b1df9f75ee6f27433687dad302387f811dab64d
effective approaches to attention - based neural machine translation an attentional mechanism has lately been used to improve neural machine translation ( nmt ) by selectively focusing on parts of the source sentence during translation . however , there has been little work exploring useful architectures for attention - based nmt . this paper examines two simple and effective classes of attentional mechanism : a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . we demonstrate the effectiveness of both approaches on the wmt translation tasks between english and german in both directions . with local attention , we achieve a significant gain of 5.0 bleu points over non - attentional systems that already incorporate known techniques such as dropout . our ensemble model using different attention architectures yields a new state - of - the - art result in the wmt’15 english to german translation task with 25.9 bleu points , an improvement of 1.0 bleu points over the existing best system backed by nmt and an n - gram reranker.1
RANK = 2; score = 0.24696817885471137; correct = False; id = 661bee4278c82e7ad8ef5fbe73c9692d0d83a524
a position encoding convolutional neural network based on dependency tree for relation classification . with the renaissance of neural network in recent years , relation classification has again become a research hotspot in natural language processing , and leveraging parse trees is a common and effective method of tackling this problem . in this work , we offer a new perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network ( pecnn ) based on dependency parse tree for relation classification . first , treebased position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations . then , based on a redefinition of “ context ” , we design two kinds of tree - based convolution kernels for capturing the semantic and structural information provided by dependency trees . finally , the features extracted by convolution module are fed to a classifier for labelling the semantic relations . experiments on the benchmark dataset show that pecnn outperforms state - of - the - art approaches . we also compare the effect of different position features and visualize the influence of treebased position feature by tracing back the convolution process .
RANK = 3; score = 0.23382817542433684; correct = False; id = 52956422f86722aca6becb67ea4c3ad61f0c1aea
inner attention based recurrent neural networks for answer selection attention based recurrent neural networks have shown advantages in representing natural language sentences ( hermann et al . , 2015 ; rocktäschel et al . , 2015 ; tan et al . , 2015 ) . based on recurrent neural networks ( rnn ) , external attention information was added to hidden representations to get an attentive sentence representation . despite the improvement over nonattentive models , the attention mechanism under rnn is not well studied . in this work , we analyze the deficiency of traditional attention based rnn models quantitatively and qualitatively . then we present three new rnn models that add attention information before rnn hidden representation , which shows advantage in representing sentence and achieves new stateof - art results in answer selection task .
RANK = 4; score = 0.2231047598718887; correct = False; id = ab1961d086e404547ef34a4ffab113e828c24b25
learning continuous phrase representations for translation modeling this paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations . a pair of source and target phrases are projected into continuous - valued vector representations in a low - dimensional latent space , where their translation score is computed by the distance between the pair in this new space . the projection is performed by a neural network whose weights are learned on parallel training data . experimental evaluation has been performed on two wmt translation tasks . our best result improves the performance of a state - of - the - art phrase - based statistical machine translation system trained on wmt 2012 french - english data by up to 1.3 bleu points .
RANK = 5; score = 0.22067521550758876; correct = False; id = 4ab22be4350d6ded1dd5ed764a1739844c22dcee
determining the placement of german verbs in english - to - german smt when translating english to german , existing reordering models often can not model the long - range reorderings needed to generate german translations with verbs in the correct position . we reorder english as a preprocessing step for english - to - german smt . we use a sequence of hand - crafted reordering rules applied to english parse trees . the reordering rules place english verbal elements in the positions within the clause they will have in the german translation . this is a difficult problem , as german verbal elements can appear in different positions within a clause ( in contrast with english verbal elements , whose positions do not vary as much ) . we obtain a significant improvement in translation performance .
RANK = 6; score = 0.21943719751214227; correct = False; id = 1b0c597e2364381160e92e8b5ed4ac804a8965a8
a joint sequence translation model with integrated reordering we present a novel machine translation model which models translation by a linear sequence of operations . in contrast to the “ n - gram ” model , this sequence includes not only translation but also reordering operations . key ideas of our model are ( i ) a new reordering approach which better restricts the position to which a word or phrase can be moved , and is able to handle short and long distance reorderings in a unified way , and ( ii ) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase - based mt . we observe statistically significant improvements in bleu over moses for german - to - english and spanish - to - english tasks , and comparable results for a french - to - english task .
RANK = 7; score = 0.21669717144674197; correct = False; id = df8f3ff5a6f423ffbe33c27b0287329d814e6cab
representation based translation evaluation metrics precisely evaluating the quality of a translation against human references is a challenging task due to the flexible word ordering of a sentence and the existence of a large number of synonyms for words . this paper proposes to evaluate translations with distributed representations of words and sentences . we study several metrics based on word and sentence representations and their combination . experiments on the wmt metric task shows that the metric based on the combined representations achieves the best performance , outperforming the state - of - the - art translation metrics by a large margin . in particular , training the distributed representations only needs a reasonable amount of monolingual , unlabeled data that is not necessary drawn from the test domain .
RANK = 8; score = 0.2120501902607618; correct = False; id = 20a599c5f152c3f135b9b55a667e64c93ec8d477
multi - way , multilingual neural machine translation with a shared attention mechanism we propose multi - way , multilingual neural machine translation . the proposed approach enables a single neural translation model to translate between multiple languages , with a number of parameters that grows only linearly with the number of languages . this is made possible by having a single attention mechanism that is shared across all language pairs . we train the proposed multiway , multilingual model on ten language pairs from wmt’15 simultaneously and observe clear performance improvements over models trained on only one language pair . in particular , we observe that the proposed model significantly improves the translation quality of low - resource language pairs .
RANK = 9; score = 0.21043783777597952; correct = False; id = c45362793d10a8d56960b9cd43d1f27980ae5bd4
citation analysis with neural attention models automated citation analysis ( aca ) can be important for many applications including author ranking and literature based information retrieval , extraction , summarization and question answering . in this study , we developed a new compositional attention network ( can ) model to integrate local and global attention representations with a hierarchical attention mechanism . training on a new benchmark corpus we built , our evaluation shows that the can model performs consistently well on both citation classification and sentiment analysis tasks .
RANK = 10; score = 0.21036538923100753; correct = False; id = 4fef246fb4a26eb8f35c5d5645711d396a3a12c4
language to logical form with neural attention semantic parsing aims at mapping natural language to machine interpretable meaning representations . traditional approaches rely on high - quality lexicons , manually - built templates , and linguistic features which are either domainor representation - specific . in this paper we present a general method based on an attention - enhanced encoder - decoder model . we encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors . experimental results on four datasets show that our approach performs competitively without using hand - engineered features and is easy to adapt across domains and meaning representations .
RANK = 11; score = 0.20374226465567832; correct = False; id = 0ee236bf186190955e6e2a6b5e0a539b2abc9891
a joint dependency model of morphological and syntactic structure for statistical machine translation when translating between two languages that differ in their degree of morphological synthesis , syntactic structures in one language may be realized as morphological structures in the other , and smt models need a mechanism to learn such translations . prior work has used morpheme splitting with flat representations that do not encode the hierarchical structure between morphemes , but this structure is relevant for learning morphosyntactic constraints and selectional preferences . we propose to model syntactic and morphological structure jointly in a dependency translation model , allowing the system to generalize to the level of morphemes . we present a dependency representation of german compounds and particle verbs that results in improvements in translation quality of 1.4–1.8 bleu in the wmt english – german translation task .
RANK = 12; score = 0.2021759790789442; correct = False; id = 27f65deac09448557979fe2d5b1ab8964b1e91af
modeling with structures in statistical machine translation most statistical machine translation systems employ a word - based alignment model . in this paper we demonstrate that word - based alignment is a major cause of translation errors . we propose a new alignment model based on shallow phrase structures , and the structures can be automatically acquired from parallel corpus . this new model achieved over 10 % error reduction for our spoken language translation task . 1 i n t r o d u c t i o n most ( if not all ) statistical machine translation systems employ a word - based alignment model ( brown et al . , 1993 ; vogel , ney , and tillman , 1996 ; wang and waibel , 1997 ) , which treats words in a sentence as independent entities and ignores the structural relationship among them . while this independence assumption works well in speech recognition , it poses a major problem in our experiments with spoken language translation between a language pair with very different word orders . in this paper we propose a translation model that employs shallow phrase structures . it has the following advantages over word - based alignment : • since the translation model can directly depict phrase reordering in translation , it is more accurate for translation between languages with different word ( phrase ) orders . • the decoder of the translation system can use the phrase information and extend hypothesis by phrases ( multiple words ) , therefore it can speed up decoding . the paper is organized as follows . in section 2 , the problems of word - based alignment models are discussed . to alienate these problems , a new alignment model based on shallow phrase structures is introduced in section 3 . in section 4 , a grammar inference algorithm is presented that can automatically acquire the phrase structures used in the new model . translation performance is then evaluated in section 5 , and conclusions are presented in section 6 . 2 w o r d b a s e d a l i g n m e n t m o d e l in a word - based alignment translation model , the transformation from a sentence at the source end of a communication channel to a sentence at the target end can be described with the following random process : 1 . pick a length for the sentence at the target end . 2 . for each word position in the target sentence , align it with a source word . 3 . produce a word at each target word position according to the source word with which the target word position has been aligned . ibm alignment model 2 is a typical example of word - based alignment . assuming a sentence s = s l , . . . , s t at the source of a channel , the model picks a length m of the target sentence t according to the distribution p ( m i s ) = e , where e is a small , fixed number . then for each position i ( 0 < i _ < m ) in t , it finds its corresponding position ai in s according to an alignmen t distribution p(a i l i , a~ -1 , m , s ) = a(ai l i , re , l ) . finally , it generates a word ti at the position i of t from the source word s~ , at the aligned position ai , according to a translation z 1 m distribution p ( t i ] t~ , a 1 , s ) -t(ti i s~ , ) .
RANK = 13; score = 0.20151141346070744; correct = False; id = 029bf880f3774d9bf735cd447b032f681626adba
agreement on target - bidirectional neural machine translation neural machine translation ( nmt ) with recurrent neural networks , has proven to be an effective technique for end - to - end machine translation . however , in spite of its promising advances over traditional translation methods , it typically suffers from an issue of unbalanced outputs , that arise from both the nature of recurrent neural networks themselves , and the challenges inherent in machine translation . to overcome this issue , we propose an agreement model for neural machine translation and show its effectiveness on large - scale japaneseto - english and chinese - to - english translation tasks . our results show the model can achieve improvements of up to 1.4 bleu over the strongest baseline nmt system . with the help of an ensemble technique , this new end - to - end nmt approach finally outperformed phrasebased and hierarchical phrase - based moses baselines by up to 5.6 bleu points .
RANK = 14; score = 0.20076564891533563; correct = False; id = 660a046263bd12e80cb14987be117653b9d6d11c
learning the optimal use of dependency - parsing information for finding translations with comparable corpora using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries ( semi- ) automatically . the basic idea is based on the assumption that similar words have similar contexts across languages . the context of a word is often summarized by using the bag - of - words in the sentence , or by using the words which are in a certain dependency position , e.g. the predecessors and successors . these different context positions are then combined into one context vector and compared across languages . however , previous research makes the ( implicit ) assumption that these different context positions should be weighted as equally important . furthermore , only the same context positions are compared with each other , for example the successor position in spanish is compared with the successor position in english . however , this is not necessarily always appropriate for languages like japanese and english . to overcome these limitations , we suggest to perform a linear transformation of the context vectors , which is defined by a matrix . we define the optimal transformation matrix by using a bayesian probabilistic model , and show that it is feasible to find an approximate solution using markov chain monte carlo methods . our experiments demonstrate that our proposed method constantly improves translation accuracy .
RANK = 15; score = 0.19912536943502654; correct = False; id = 2d797e3aaeb1a28a594d6d350492a124c0b745e1
decoding algorithm in statistical machine translation decoding algorithm is a crucial part in statistical machine translation . we describe a stack decoding algorithm in this paper . we present the hypothesis scoring method and the heuristics used in our algorithm . we report several techniques deployed to improve the performance of the decoder . we also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process . we evaluate and compare these techniques / models in our statistical machine translation system . 1 i n t r o d u c t i o n 1.1 s t a t i s t i c a l m a c h i n e t r a n s l a t i o n statistical machine translation is based on a channel model . given a sentence t in one language ( german ) to be translated into another language ( english ) , it considers t as the target of a communication channel , and its translation s as the source of the channel . hence the machine translation task becomes to recover the source from the target . basically every english sentence is a possible source for a german target sentence . if we assign a probability p ( s i t ) to each pair of sentences ( s , t ) , then the problem of translation is to find the source s for a given target t , such that p(s [ t ) is the maximum . according to bayes rule , p ( s it ) = p ( s ) p ( t i s ) p ( t ) ( 1 ) since the denominator is independent of s , we have -arg m a x p ( s ) p ( t i s ) ( 2 ) s therefore a statistical machine translation system must deal with the following three problems : • modeling problem : how to depict the process of generating a sentence in a source language , and the process used by a channel to generate a target sentence upon receiving a source sentence ? the former is the problem of language modeling , and the later is the problem of translation modeling . they provide a framework for calculating p(s ) and p(w i s ) in ( 2 ) . • learning problem : given a statistical language model p(s ) and a statistical translation model p ( t i s ) , how to estimate the parameters in these models from a bilingual corpus of sentences ? • decoding problem : with a fully specified ( framework and parameters ) language and translation model , given a target sentence t , how to efficiently search for the source sentence that satisfies ( 2 ) . the modeling and learning issues have been discussed in ( brown et ah , 1993 ) , where ngram model was used for language modeling , and five different translation models were introduced for the translation process . we briefly introduce the model 2 here , for which we built our decoder . in model 2 , upon receiving a source english sentence e = el , . • - , el , the channel generates a german sentence g = gl , • • " , g , n at the target end in the following way : 1 . with a distribution p ( m i e ) , randomly choose the length m of the german translation g. in model 2 , the distribution is independent of m and e : p(m [ e ) = e where e is a small , fixed number . 2 . for each position i ( 0 < i < m ) in g , find the corresponding position ai in e according to an al ignment distribution p ( a i i i , a~ -1 , m , e ) . in model 2 , the distribution only depends on i , ai and the length of the english and german sentences : p(a i l i , a ~ l , m , e ) = a(ai l i , m , l ) 3 . generate the word gl at the position i of the german sentence from the english word ea~ at
RANK = 16; score = 0.19726460544258206; correct = False; id = 89ee0b3210c3cc5f6f79a11b625f6e7ad19a05c9
online learning approaches in computer assisted translation we present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario . with the introduction of a simple online feature , we are able to adapt the translation model on the fly to the corrections made by the translators . additionally , we do online adaption of the feature weights with a large margin algorithm . our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 bleu points absolute , and a standard incremental adaptation approach by 2 bleu points absolute .
RANK = 17; score = 0.19622219168882363; correct = True; id = 07a9478e87a8304fc3267fa16e83e9f3bbd98b27
a decomposable attention model for natural language inference we propose a simple neural architecture for natural language inference . our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable . on the stanford natural language inference ( snli ) dataset , we obtain state - of - the - art results with almost an order of magnitude fewer parameters than previous work and without relying on any word - order information . adding intra - sentence attention that takes a minimum amount of order into account yields further improvements .
RANK = 18; score = 0.19547241296243234; correct = False; id = 8e0dccbba2aa4e58b79b419a596775a6fba86a26
insertion position selection model for flexible non - terminals in dependency tree - to - tree machine translation dependency tree - to - tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs . the translation process is easy if it can be accomplished only by replacing non - terminals in translation rules with other rules . however it is sometimes necessary to adjoin translation rules . flexible non - terminals have been proposed as a promising solution for this problem . a flexible non - terminal provides several insertion position candidates for the rules to be adjoined , but it increases the computational cost of decoding . in this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions . the experimental results show the proposed model can select the appropriate insertion position with a high accuracy . it reduces the decoding time and improves the translation quality owing to reduced search space .
RANK = 19; score = 0.19513948173901172; correct = False; id = 21c2f726e606506bbfcca85840ef18494a42ec74
combining distributed vector representations for words recent interest in distributed vector representations for words has resulted in an increased diversity of approaches , each with strengths and weaknesses . we demonstrate how diverse vector representations may be inexpensively composed into hybrid representations , effectively leveraging strengths of individual components , as evidenced by substantial improvements on a standard word analogy task . we further compare these results over different sizes of training sets and find these advantages are more pronounced when training data is limited . finally , we explore the relative impacts of the differences in the learning methods themselves and the size of the contexts they access .
RANK = 20; score = 0.19410496568702823; correct = False; id = d6ae7ae2779f91b9fd5ea227cb03abc5a12c88a1
learning distributed representations for multilingual text sequences we propose a novel approach to learning distributed representations of variable - length text sequences in multiple languages simultaneously . unlike previous work which often derive representations of multi - word sequences as weighted sums of individual word vectors , our model learns distributed representations for phrases and sentences as a whole . our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning - equivalent text sequences of multiple languages in the same semantic space . our learned embeddings achieve state - of - theart performance in the often used crosslingual document classification task ( cldc ) with an accuracy of 92.7 for english to german and 91.5 for german to english . by learning text sequence representations as a whole , our model performs equally well in both classification directions in the cldc task in which past work did not achieve .

RANKING 447
QUERY
code - mixed question answering challenge : crowd - sourcing data and techniques code - mixing ( cm ) is the phenomenon of alternating between two or more languages which is prevalent in biand multi - lingual communities . most nlp applications today are still designed with the assumption of a single interaction language and are most likely to break given a cm utterance with multiple languages mixed at a morphological , phrase or sentence level . for example , popular commercial search engines do not yet fully understand the intents expressed in cm queries . as a first step towards fostering research which supports cm in nlp applications , we systematically crowd - sourced and curated an evaluation dataset for factoid question answering in three cm languages hinglish ( hindi+english ) , tenglish ( telugu+english ) and tamlish ( tamil+english ) which belong to two language families . we share the details of our data collection process , techniques which were used to avoid inducing lexical bias amongst the crowd workers and other cm specific linguistic properties of the dataset . our final dataset , which is available freely for research purposes , has 1,694 hinglish , 2,848 tamlish and 1,391 tenglish factoid questions and their answers . we discuss the techniques used by the participants for the first edition of this ongoing challenge .
First cited at 9
TOP CITED PAPERS
RANK 9
" i am borrowing ya mixing ? " an analysis of english - hindi code mixing in facebook code - mixing is a frequently observed phenomenon in social media content generated by multi - lingual users . the processing of such data for linguistic analysis as well as computational modelling is challenging due to the linguistic complexity resulting from the nature of the mixing as well as the presence of non - standard variations in spellings and grammar , and transliteration . our analysis shows the extent of code - mixing in english - hindi data . the classification of code - mixed words based on frequency and linguistic typology underline the fact that while there are easily identifiable cases of borrowing and mixing at the two ends , a large majority of the words form a continuum in the middle , emphasizing the need to handle these at different levels for automatic processing of the data .
RANK 25
code mixing : a challenge for language identification in the language of social media in social media communication , multilingual speakers often switch between languages , and , in such an environment , automatic language identification becomes both a necessary and challenging task . in this paper , we describe our work in progress on the problem of automatic language identification for the language of social media . we describe a new dataset that we are in the process of creating , which contains facebook posts and comments that exhibit code mixing between bengali , english and hindi . we also present some preliminary word - level language identification experiments using this dataset . different techniques are employed , including a simple unsupervised dictionary - based approach , supervised word - level classification with and without contextual clues , and sequence labelling using conditional random fields . we find that the dictionary - based approach is surpassed by supervised classification and sequence labelling , and that it is important to take contextual clues into consideration .
RANK 41
squad : 100 , 000 + questions for machine comprehension of text we present the stanford question answering dataset ( squad ) , a new reading comprehension dataset consisting of 100,000 + questions posed by crowdworkers on a set of wikipedia articles , where the answer to each question is a segment of text from the corresponding reading passage . we analyze the dataset to understand the types of reasoning required to answer the questions , leaning heavily on dependency and constituency trees . we build a strong logistic regression model , which achieves an f1 score of 51.0 % , a significant improvement over a simple baseline ( 20 % ) . however , human performance ( 86.8 % ) is much higher , indicating that the dataset presents a good challenge problem for future research . the dataset is freely available at https://stanford-qa.com .
TOP UNCITED PAPERS
RANK 1
bulk processing of text on a massively parallel computer dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the connection machine t m computer ( cm ) . several algorithms for parallel dictionary lookup are discussed , including one that allows the cm to lookup words at a rate 450 times that of lookup on a symbolics 3600 lisp machine . 1 an overv iew of the d ic t ionary p r o b l e m this paper will discuss one of the text processing problems that was encountered during the implementation of the cm - indexer , a natural language processing program that runs on the connection machine ( cm ) . the problem is that of parallel dictionary lookup : given both a dictionary and a text consisting of many thousands of words , how can the appropriate definitions be distr ibuted to the words in the text as rapidly as possible ? a parallel dictionary lookup algorithm that makes efficient use of the cm hardware was discovered and is described in this paper . it is clear that there are many natural language processing applications in which such a dictionary algori thm is necessary . indexing and searching of databases consisting of unformatted natural language text is one such application . the proliferation of personal computers , the widespread use of electronic memos and electronic mail in large corporations , and the cd - rom are all contributing to an explosion in the amount of useful unformatted text in computer readable form . parallel computers and algorithms provide one way of dealing with this explosion . 2 the cm : m a c h i n e descr ip t ion the cm consists of a large number number of processor /memory cells . these cells are used to store data structures . in accordance with a stream of instructions that are broadcast from a single conventional host computer , the many processors can manipulate the data in the nodes of the data structure in parallel . each processor in the cm can have its own local variables . these variables are called parallel variables , or parallel fields . when a host computer program performs a serial operation on a parallel variable , that operation is performed separately in each processor in the cm . for example , a program might compare two parallel string variables . each cm processor would execute the comparison on its own local data and produce its own local result . thus , a single command can result in tens of thousands of simultaneous cm comparisons . in addition to their computation ability , cm processors can communicate with each other via a special hardware communication network . in effect , communication is the parallel analog of the pointer - following executed by a serial computer as it traverses the links of a data structure or graph . 3 d ic t ionary access a dictionary may be defined as a mapping that takes a particular word and returns a group of status bits . status bits indicate which sets or groups of words a particular word belongs to . some of the sets that are useful in natural language processing include syntactic categories such as nouns , verbs , and prepositions . programs also can use semantic characterization information . for example , knowing whether a word is name of a famous person ( i.e. lincoln , churchill ) , a place , an interjection , or a time or calendar term will often be useful to a text processing program . the task of looking up the definition of a word consists of returning a binary number that contains l ' s only . in bit positions that correspond with the groups to which that word belongs . thus , the definition of " lincoln " contains a zero in the bit that indicates a word can serve as a verb , but it contains a 1 in the famous person 's name bit . while all of the examples in this paper involve only a few words , it should be understood that the cm is efficient and cost effective only when large amounts of
RANK 2
developing feature types for classifying clinical notes this paper proposes a machine learning approach to the task of assigning the international standard on classification of diseases icd-9-cm codes to clinical records . by treating the task as a text categorisation problem , a classification system was built which explores a variety of features including negation , different strategies of measuring gloss overlaps between the content of clinical records and icd-9-cm code descriptions together with expansion of the glosses from the icd-9-cm hierarchy . the best classifier achieved an overall f1 value of 88.2 on a data set of 978 free text clinical records , and was better than the performance of two out of three human annotators .
RANK 3
automatic question answering : beyond the factoid in this paper we describe and evaluate a question answering system that goes beyond answering factoid questions . we focus on faqlike questions and answers , and build our system around a noisy - channel architecture which exploits both a language model for answers and a transformation model for answer / question terms , trained on a corpus of 1 million question / answer pairs collected from the web .
TOP 20
RANK = 1; score = 0.351183916732511; correct = False; id = 7ef35e2e9b4638248dac37c0f3cce3fc1ce9d073
bulk processing of text on a massively parallel computer dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the connection machine t m computer ( cm ) . several algorithms for parallel dictionary lookup are discussed , including one that allows the cm to lookup words at a rate 450 times that of lookup on a symbolics 3600 lisp machine . 1 an overv iew of the d ic t ionary p r o b l e m this paper will discuss one of the text processing problems that was encountered during the implementation of the cm - indexer , a natural language processing program that runs on the connection machine ( cm ) . the problem is that of parallel dictionary lookup : given both a dictionary and a text consisting of many thousands of words , how can the appropriate definitions be distr ibuted to the words in the text as rapidly as possible ? a parallel dictionary lookup algorithm that makes efficient use of the cm hardware was discovered and is described in this paper . it is clear that there are many natural language processing applications in which such a dictionary algori thm is necessary . indexing and searching of databases consisting of unformatted natural language text is one such application . the proliferation of personal computers , the widespread use of electronic memos and electronic mail in large corporations , and the cd - rom are all contributing to an explosion in the amount of useful unformatted text in computer readable form . parallel computers and algorithms provide one way of dealing with this explosion . 2 the cm : m a c h i n e descr ip t ion the cm consists of a large number number of processor /memory cells . these cells are used to store data structures . in accordance with a stream of instructions that are broadcast from a single conventional host computer , the many processors can manipulate the data in the nodes of the data structure in parallel . each processor in the cm can have its own local variables . these variables are called parallel variables , or parallel fields . when a host computer program performs a serial operation on a parallel variable , that operation is performed separately in each processor in the cm . for example , a program might compare two parallel string variables . each cm processor would execute the comparison on its own local data and produce its own local result . thus , a single command can result in tens of thousands of simultaneous cm comparisons . in addition to their computation ability , cm processors can communicate with each other via a special hardware communication network . in effect , communication is the parallel analog of the pointer - following executed by a serial computer as it traverses the links of a data structure or graph . 3 d ic t ionary access a dictionary may be defined as a mapping that takes a particular word and returns a group of status bits . status bits indicate which sets or groups of words a particular word belongs to . some of the sets that are useful in natural language processing include syntactic categories such as nouns , verbs , and prepositions . programs also can use semantic characterization information . for example , knowing whether a word is name of a famous person ( i.e. lincoln , churchill ) , a place , an interjection , or a time or calendar term will often be useful to a text processing program . the task of looking up the definition of a word consists of returning a binary number that contains l ' s only . in bit positions that correspond with the groups to which that word belongs . thus , the definition of " lincoln " contains a zero in the bit that indicates a word can serve as a verb , but it contains a 1 in the famous person 's name bit . while all of the examples in this paper involve only a few words , it should be understood that the cm is efficient and cost effective only when large amounts of
RANK = 2; score = 0.25164778010332084; correct = False; id = 865fa0669ab9e17e119f0242909845b956682c4e
developing feature types for classifying clinical notes this paper proposes a machine learning approach to the task of assigning the international standard on classification of diseases icd-9-cm codes to clinical records . by treating the task as a text categorisation problem , a classification system was built which explores a variety of features including negation , different strategies of measuring gloss overlaps between the content of clinical records and icd-9-cm code descriptions together with expansion of the glosses from the icd-9-cm hierarchy . the best classifier achieved an overall f1 value of 88.2 on a data set of 978 free text clinical records , and was better than the performance of two out of three human annotators .
RANK = 3; score = 0.19831913639408694; correct = False; id = b4bd48d2e4a84f9897cb765e3aeaeca5733da96d
automatic question answering : beyond the factoid in this paper we describe and evaluate a question answering system that goes beyond answering factoid questions . we focus on faqlike questions and answers , and build our system around a noisy - channel architecture which exploits both a language model for answers and a transformation model for answer / question terms , trained on a corpus of 1 million question / answer pairs collected from the web .
RANK = 4; score = 0.19502454593250879; correct = False; id = 3de9227aec2c04e3fc50f7deef59bf0b7dce6df3
word - level language identification in bi - lingual code - switched texts code - switching is the practice of moving back and forth between two languages in spoken or written form of communication . in this paper , we address the problem of word - level language identification of code - switched sentences . here , we primarily consider hindi - english ( hinglish ) code - switching , which is a popular phenomenon among urban indian youth , though the approach is generic enough to be extended to other language pairs . identifying word - level languages in code - switched texts is associated with two major challenges . firstly , people often use non - standard english transliterated forms of hindi words . secondly , the transliterated hindi words are often confused with english words having the same spelling . most existing works tackle the problem of language identification using n - grams of characters . we propose some techniques to learn sequence of character(s ) frequently substituted for character(s ) in standard transliterated forms . we illustrate the superior performance of these techniques in identifying hindi words corresponding to the given transliterated forms . we adopt a novel experimental model which considers the language and part - of - speech of adjoining words for word - level language identification . our test results show that the proposed model significantly increases the accuracy over existing approaches . we achieved f1-score of 98.0 % for recognizing hindi words and 94.8 % for recognizing english words .
RANK = 5; score = 0.19303348956816047; correct = False; id = 4b30a3ea864c4bd43a91c9fbba1527be5385f734
unraveling the english - bengali code - mixing phenomenon code - mixing is a prevalent phenomenon in modern day communication . though several systems enjoy success in identifying a single language , identifying languages of words in code - mixed texts is a herculean task , more so in a social media context . this paper explores the english - bengali code - mixing phenomenon and presents algorithms capable of identifying the language of every word to a reasonable accuracy in specific cases and the general case . we create and test a predictorcorrector model , develop a new code - mixed corpus from facebook chat ( made available for future research ) and test and compare the efficiency of various machine learning algorithms ( j48 , ibk , random forest ) . the paper also seeks to remove the ambiguities in the token identification process .
RANK = 6; score = 0.19273240079370746; correct = False; id = 2f13877be3029b3f4f9e0846438e364ca8bdeb35
focus annotation of task - based data : establishing the quality of crowd annotation we explore the annotation of information structure in german and compare the quality of expert annotation with crowdsourced annotation taking into account the cost of reaching crowd consensus . concretely , we discuss a crowd - sourcing effort annotating focus in a task - based corpus of german containing reading comprehension questions and answers . against the backdrop of a gold standard reference resulting from adjudicated expert annotation , we evaluate a crowd sourcing experiment using majority voting to determine a baseline performance . to refine the crowd - sourcing setup , we introduce the consensus cost as a measure of agreement within the crowd . we investigate the usefulness of consensus cost as a measure of crowd annotation quality both intrinsically , in relation to the expert gold standard , and extrinsically , by integrating focus annotation information into a system performing short answer assessment taking into account the consensus cost . we find that low consensus cost in crowd sourcing indicates high quality , though high cost does not necessarily indicate low accuracy but increased variability . overall , taking consensus cost into account improves both intrinsic and extrinsic evaluation measures .
RANK = 7; score = 0.1912181561878802; correct = False; id = f10e1d80456dbe50498b6aaf3de9ea678a8c6368
part - of - speech tagging of code - mixed social media text a common step in the processing of any text is the part - of - speech tagging of the input text . in this paper , we present an approach to tackle code - mixed text from three different languages bengali , hindi , and tamil apart from english . our system uses conditional random field , a sequence learning method , which is useful to capture patterns of sequences containing code switching to tag each word with accurate part - of - speech information . we have used various pre - processing and post - processing modules to improve the performance of our system . the results were satisfactory , with a highest of 75.22 % accuracy in bengali - english mixed data . the methodology that we employed in the task can be used for any resource poor language . we adapted standard learning approaches that work well with scarce data . we have also ensured that the system is portable to different platforms and languages and can be deployed for real - time analysis .
RANK = 8; score = 0.18821909465826858; correct = False; id = 10d9246498e6d3ec5d93a7bc5f785a46eadc620d
opinion and generic question answering systems : a performance analysis the importance of the new textual genres such as blogs or forum entries is growing in parallel with the evolution of the social web . this paper presents two corpora of blog posts in english and in spanish , annotated according to the emotiblog annotation scheme . furthermore , we created 20 factual and opinionated questions for each language and also the gold standard for their answers in the corpus . the purpose of our work is to study the challenges involved in a mixed fact and opinion question answering setting by comparing the performance of two question answering ( qa ) systems as far as mixed opinion and factual setting is concerned . the first one is open domain , while the second one is opinionoriented . we evaluate separately the two systems in both languages and propose possible solutions to improve qa systems that have to process mixed questions . introduction and motivation in the last few years , the number of blogs has grown exponentially . thus , the web contains more and more subjective texts . a research from the pew institute shows that 75.000 blogs are created daily ( pang and lee , 2008 ) . they approach a great variety of topics ( computer science , sociology , political science or economics ) and are written by different types of people , thus are a relevant resource for large community behavior analysis . due to the high volume of data contained in blogs , new natural language processing ( nlp ) resources , tools and methods are needed in order to manage their language understanding . our fist contribution consists in carrying out a multilingual research , for english and spanish . secondly , many sources are present in blogs , as people introduce quotes from newspaper articles or other information to support their arguments and make references to previous posts in the discussion thread . thus , when performing a task such as question answering ( qa ) , many new aspects have to be taken into consideration . previous studies in the field ( stoyanov , cardie and wiebe , 2005 ) showed that certain types of queries , which are factual in nature , require the use of opinion mining ( om ) resources and techniques to retrieve the correct answers . a further contribution this paper brings is the analysis and definition of the criteria for the discrimination among types of factual versus opinionated questions . previous researchers mainly concentrated on newspaper collections . we formulated and annotated of a set of questions and answers over a multilingual blog collection . a further contribution is the evaluation and comparison of two different approaches to qa a fact - oriented one and another designed for opinion qa scenarios .
RANK = 9; score = 0.18612335889762013; correct = True; id = 803b769b073d36523bcc2a35528090e85b69c3c8
" i am borrowing ya mixing ? " an analysis of english - hindi code mixing in facebook code - mixing is a frequently observed phenomenon in social media content generated by multi - lingual users . the processing of such data for linguistic analysis as well as computational modelling is challenging due to the linguistic complexity resulting from the nature of the mixing as well as the presence of non - standard variations in spellings and grammar , and transliteration . our analysis shows the extent of code - mixing in english - hindi data . the classification of code - mixed words based on frequency and linguistic typology underline the fact that while there are easily identifiable cases of borrowing and mixing at the two ends , a large majority of the words form a continuum in the middle , emphasizing the need to handle these at different levels for automatic processing of the data .
RANK = 10; score = 0.18507348259173464; correct = False; id = ff2a26d922c7fd9267c2063d702022f2c7019635
semantic based query expansion for arabic question answering systems question answering systems have emerged as a good alternative to search engines where they produce the desired information in a very precise way in the real time . however , one serious concern with the question answering system is that despite having answers of the questions in the knowledge base , they are not able to retrieve the answer due to mismatch between the words used by users and content creators . there has been a lot of research in the field of english and some european language question answering systems to handle this issue . however , arabic question answering systems could not match the pace due to some inherent difficulties with the language itself as well as due to lack of tools available to assist the researchers . in this paper , we are presenting a method to add semantically equivalent keywords in the questions by using semantic resources . the experiments suggest that the proposed research can deliver highly accurate answers for arabic questions .
RANK = 11; score = 0.179964506846885; correct = False; id = 5facd87fcc8c8d7e0090f4476b8037f493d7def8
collecting a why - question corpus for development and evaluation of an automatic qa - system question answering research has only recently started to spread from short factoid questions to more complex ones . one significant challenge is the evaluation : manual evaluation is a difficult , time - consuming process and not applicable within efficient development of systems . automatic evaluation requires a corpus of questions and answers , a definition of what is a correct answer , and a way to compare the correct answers to automatic answers produced by a system . for this purpose we present a wikipedia - based corpus of whyquestions and corresponding answers and articles . the corpus was built by a novel method : paid participants were contacted through a web - interface , a procedure which allowed dynamic , fast and inexpensive development of data collection methods . each question in the corpus has several corresponding , partly overlapping answers , which is an asset when estimating the correctness of answers . in addition , the corpus contains information related to the corpus collection process . we believe this additional information can be used to post - process the data , and to develop an automatic approval system for further data collection projects conducted in a similar manner .
RANK = 12; score = 0.17953513126210124; correct = False; id = 382b8080bd67ecdf35c364c6af38167cfab584a5
global thread - level inference for comment classification in community question answering community question answering , a recent evolution of question answering in the web context , allows a user to quickly consult the opinion of a number of people on a particular topic , thus taking advantage of the wisdom of the crowd . here we try to help the user by deciding automatically which answers are good and which are bad for a given question . in particular , we focus on exploiting the output structure at the thread level in order to make more consistent global decisions . more specifically , we exploit the relations between pairs of comments at any distance in the thread , which we incorporate in a graph - cut and in an ilp frameworks . we evaluated our approach on the benchmark dataset of semeval-2015 task 3 . results improved over the state of the art , confirming the importance of using thread level in-
RANK = 13; score = 0.17907985354153513; correct = False; id = 158a3ed2b8981fef5b6d0bc69941367bdb4be2eb
feedback enhances the security of state - dependent degraded broadcast channels with confidential messages in this paper , we investigate the state - dependent degraded broadcast channels with confidential messages ( sd - dbc - cm ) , and with or without noiseless feedback . in this new model , the channel state is available at the transmitter in a noncausal or causal manner , and there may exist a noiseless feedback link from the nondegraded receiver to the transmitter . for the discrete memoryless sd - dbc - cm , inner and outer bounds on the capacity - equivocation regions are provided for both the noncausal and causal cases . for the discrete memoryless sd - dbc - cm with feedback , inner and outer bounds on the capacity - equivocation region are provided for the noncausal case , and the capacity - equivocation region is determined for the causal case . the results of this paper are further explained via gaussian and binary examples .
RANK = 14; score = 0.17837877785549275; correct = False; id = 0d01505985083d40f9cd8c6ae8a7d61b91aa624d
a shared task involving multi - label classification of clinical free text this paper reports on a shared task involving the assignment of icd-9-cm codes to radiology reports . two features distinguished this task from previous shared tasks in the biomedical domain . one is that it resulted in the first freely distributable corpus of fully anonymized clinical text . this resource is permanently available and will ( we hope ) facilitate future research . the other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels . the number of participants was larger than in any previous biomedical challenge task . we describe the data production process and the evaluation measures , and give a preliminary analysis of the results . many systems performed at levels approaching the inter - coder agreement , suggesting that human - like performance on this task is within the reach of currently available technologies .
RANK = 15; score = 0.1781922403608537; correct = False; id = 2a1f11f130349c53a179fca76391f717ed52857c
factoid question answering with web , mobile and speech interfaces in this paper we describe the web and mobile - phone interfaces to our multilanguage factoid question answering ( qa ) system together with a prototype speech interface to our english - language qa system . using a statistical , data - driven approach to factoid question answering has allowed us to develop qa systems in five languages in a matter of months . in the web - based system , which is accessible at http://asked.jp , we have combined the qa system output with standard search - engine - like results by integrating it with an open - source web search engine . the prototype speech interface is based around a voicexml application running on the voxeo developer platform . recognition of the user ’s question is performed on a separate speech recognition server dedicated to recognizing questions . an adapted version of the sphinx-4 recognizer is used for this purpose . once the question has been recognized correctly it is passed to the qa system and the resulting answers read back to the user by speech synthesis . our approach is modular and makes extensive use of opensource software . consequently , each component can be easily and independently improved and easily extended to other languages .
RANK = 16; score = 0.17588105667191845; correct = False; id = d3c9da94aa17bae4117d99fc8eb05f5c62b6ec4e
using question series to evaluate question answering system effectiveness the original motivation for using question series in the trec 2004 question answering track was the desire to model aspects of dialogue processing in an evaluation task that included different question types . the structure introduced by the series also proved to have an important additional benefit : the series is at an appropriate level of granularity for aggregating scores for an effective evaluation . the series is small enough to be meaningful at the task level since it represents a single user interaction , yet it is large enough to avoid the highly skewed score distributions exhibited by single questions . an analysis of the reliability of the per - series evaluation shows the evaluation is stable for differences in scores seen in the track . the development of question answering technology in recent years has been driven by tasks defined in community - wide evaluations such as trec , ntcir , and clef . the trec question answering ( qa ) track started in 1999 , with the first several editions of the track focused on factoid questions . a factoid question is a fact - based , short answer question such as how many calories are there in a big mac ? . the track has evolved by increasing the type and difficulty of questions that are included in the test set . the task in the trec 2003 qa track was a combined task that contained list and definition questions in addition to factoid questions ( voorhees , 2004 ) . a list question asks for different instances of a particular kind of information to be returned , such as list the names of chewing gums . answering such questions requires a system to assemble an answer from information located in multiple documents . a definition question asks for interesting information about a particular person or thing such as who is vlad the impaler ? or what is a golden parachute ? . definition questions also require systems to locate information in multiple documents , but in this case the information of interest is much less crisply delineated . like the ntcir4 qaciad challenge ( kato et al . , 2004 ) , the trec 2004 qa track grouped questions into series , using the series as abstractions of information - seeking dialogues . in addition to modeling a real user task , the series are a step toward incorporating context - processing into qa evaluation since earlier questions in a series provide some context for the current question . in the case of the trec series , each series contained factoid and list questions and had the target of a definition associated with it . each question in a series asked for some information about the target . in addition , the final question in each series was an explicit “ other ” question , which was to be interpreted as “ tell me other interesting things about this target i do n’t know enough to ask directly ” . this last question was roughly equivalent to the definition questions in the
RANK = 17; score = 0.1755339081075462; correct = False; id = 27ab2cbb3b26a431d5ede5be34e9ee9a464d144c
a machine learning approach to answering questions for reading comprehension tests in this paper , we report results on answering questions for the reading comprehension task , using a machine learning approach . we evaluated our approach on the remedia data set , a common data set used in several recent papers on the reading comprehension task . our learning approach achieves accuracy competitive to previous approaches that rely on handcrafted , deterministic rules and algorithms . to the best of our knowledge , this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests . 1 i n t r o d u c t i o n the advent of the internet has resulted in a massive information explosion . we need to have an effective and efficient means of locating just the desired information . the field of information retrieval ( ir ) is the traditional discipline that addresses this problem . however , most of the prior work in ir deal more with document retrieval rather than " information " retrieval . this also applies to search engines on the internet . current search engines take a list of input words and return a ranked list of web pages that contain ( or not contain ) the given words . it is then left to the user to search through the returned list of web pages for the information that he needs . while finding the web pages that contain the desired information is an important first step , what an information seeker needs is often an answer to a question . that is , given a question , we want a system to return the exact answers to the question , and not just the documents to allow us to further search for the * leong hwee teo 's current a ~ a t i o n : defence medical research institute , defence science and technology agency , 1 depot road , defence technology tower a , ~19 - 05 , singapore 109679 l : leonghw ~ dsl : a.gov , sg a n s w e r s . the need for question answering ( qa ) systems has prompted the initiation of the question answering track in trec-8 ( voorhees and tice , 2000 ) to address this problem . in the qa track , each participant is given a list of 200 questions , and the goal is to locate answers to these questions from a document database consisting of hundreds of thousands of documents ( about two gigabytes of text ) . each participant is to return a ranked list of the five best answer strings for each question , where each answer string is a string of 50 bytes ( or 250 bytes ) that contains an answer to the question . what , when , where , and who questions that have explicit answers given in some document in the database are emphasized , but not why questions . in a related but independent effort , a group at mitre has investigated question answering in the context of the reading comprehension task ( hirschman et al . , 1999 ) . the documents in this task axe 115 children stories at grade two to five from remedia publications , and the task involves answering five questions ( who , what , when , where , and why question ) per story , as a measure of how well a system has understood the story . each story has an average of 20 sentences , and the question answering task as formulated for a computer program is to select a sentence in the story that answers to a question . for about 10 % of the questions , there is not a single sentence in the story that is judged to answer the question . conversely , a question can have multiple correct answers , where each of several individual sentences is a correct answer . an example story from the remedia corpus and its five accompanying questions axe given in figure 1 . each story has a title ( such as " storybook person found alive ! " ) and dateline ( such as " england , june , 1989 " ) in the remedia corpus .
RANK = 18; score = 0.17176126142720374; correct = False; id = eb10c5f7fe06a695c66ad90b4812458c34ff62cc
evaluating the evaluation : a case study using the trec 2002 question answering track evaluating competing technologies on a common problem set is a powerful way to improve the state of the art and hasten technology transfer . yet poorly designed evaluations can waste research effort or even mislead researchers with faulty conclusions . thus it is important to examine the quality of a new evaluation task to establish its reliability . this paper provides an example of one such assessment by analyzing the task within the trec 2002 question answering track . the analysis demonstrates that comparative results from the new task are stable , and empirically estimates the size of the difference required between scores to confidently conclude that two runs are different . metric - based evaluations of human language technology such as muc and trec and duc continue to proliferate ( sparck jones , 2001 ) . this proliferation is not difficult to understand : evaluations can forge communities , accelerate technology transfer , and advance the state of the art . yet evaluations are not without their costs . in addition to the financial resources required to support the evaluation , there are also the costs of researcher time and focus . since a poorly defined evaluation task wastes research effort , it is important to examine the validity of an evaluation task . in this paper , we assess the quality of the new question answering task that was the focus of the trec 2002 question answering track . trec is a workshop series designed to encourage research on text retrieval for realistic applications by providing large test collections , uniform scoring procedures , and a forum for organizations interested in comparing results . the conference has focused primarily on the traditional information retrieval problem of retrieving a ranked list of documents in response to a statement of information need , but also includes other tasks , called tracks , that focus on new areas or particularly difficult aspects of information retrieval . a question answering ( qa ) track was started in trec in 1999 ( trec-8 ) to address the problem of returning answers , rather than document lists , in response to a question . the task for each of the first three years of the qa track was essentially the same . participants received a large corpus of newswire documents and a set of factoid questions such as how many calories are in a big mac ? and who invented the paper clip ? . systems were required to return a ranked list of up to five [ document - id , answerstring ] pairs per question such that each answer string was believed to contain an answer to the question . human assessors read each string and decided whether the string actually did contain an answer to the question . an individual question received a score equal to the reciprocal of the rank at which the first correct response was returned , or zero if none of the five responses contained a correct answer . the score for a submission was then the mean of the individual questions’ reciprocal ranks . analysis of the trec-8 track confirmed the reliability of this evaluation task ( voorhees and tice , 2000 ) : the assessors understood and could do their assessing job ; relative scores between systems were stable despite differences of opinion by assessors ; and intuitively better systems received better scores . the task for the trec 2002 qa track changed significantly from the previous years’ task , and thus a new assessment of the track is needed . this paper provides that assessment by examining both the ability of the human assessors to make the required judgments and the effect that differences in assessor opinions have on comparative results , plus empirically establishing confidence intervals for the reliability of a comparison as a function of the difference in effectiveness scores . the first section defines the 2002 qa task and provides a brief summary of the system results . the following three sections look at each of the evaluation issues in turn . the final section summarizes the findings , and outlines shortcomings of the evaluation that remain to be addressed . 1 the trec 2002 qa track the goal of the question answering track is to foster research on systems that retrieve answers rather than documents , with particular emphasis on systems that function in unrestricted domains . to date the track has considered only a very restricted version of the general question answering problem , finding answers to closed - class questions in a large corpus of newspaper articles . kupiec defined a closed - class question as “ a question stated in natural language , which assumes some definite answer typified by a noun phrase rather than a procedural answer ” ( kupiec , 1993 ) . the trec 2002 track continued to use closed - class questions , but made two major departures from the task as defined in earlier years . the first difference was that systems were to return exact answers rather than the text snippets containing an answer that were accepted previously . the second difference was that systems were required to return exactly one response per question and the questions were to be ranked by the system ’s confidence in the answer it had found . the change to exact answers was motivated by the belief that a system ’s ability to recognize the precise extent of the answer is crucial to improving question answering technology . the problems with using text snippets as responses were illustrated in the trec 2001 track . each of the answer strings shown in figure 1 was judged correct for the question what river in the us is known as the big muddy ? , yet earlier responses are clearly better than later ones . accepting only exact answers as correct forces systems to demonstrate that they know precisely where the answer lies in the snippets . the second change , ranking questions by confidence in the answer , tested a system ’s ability to recognize when it has found a correct answer . systems must be able to recognize when they do not know the answer to avoid returning incorrect responses . in many applications returning a wrong answer is much worse than returning a “ do n’t know ” response .
RANK = 19; score = 0.17059347813279574; correct = False; id = db631af229192dd51525dd37f87797fb1cd234ed
automatic code assignment to medical text code assignment is important for handling large amounts of electronic medical data in the modern hospital . however , only expert annotators with extensive training can assign codes . we present a system for the assignment of icd-9-cm clinical codes to free text radiology reports . our system assigns a code configuration , predicting one or more codes for each document . we combine three coding systems into a single learning system for higher accuracy . we compare our system on a real world medical dataset with both human annotators and other automated systems , achieving nearly the maximum score on the computational medicine center ’s challenge .
RANK = 20; score = 0.17059329628718756; correct = False; id = 85d87c3e26888dad0d07b1f4fcc00ba400facfad
realtext - asg : a model to present answers utilizing the linguistic structure of source question recent trends in question answering ( qa ) have led to numerous studies focusing on presenting answers in a form which closely resembles a human generated answer . these studies have used a range of techniques which use the structure of knowledge , generic linguistic structures and template based approaches to construct answers as close as possible to a human generate answer , referred to as human competitive answers . this paper reports the results of an empirical study which uses the linguistic structure of the source question as the basis for a human competitive answer . we propose a typed dependency based approach to generate an answer sentence where linguistic structure of the question is transformed and realized into a sentence containing the answer . we employ the factoid questions from qald-2 training question set to extract typed dependency patterns based on the root of the parse tree . using identified patterns we generate a rule set which is used to generate a natural language sentence containing the answer extracted from a knowledge source , realized into a linguistically correct sentence . the evaluation of the approach is performed using qald-2 testing factoid questions sets with a 78.84 % accuracy . the top-10 patterns extracted from training dataset were able to cover 69.19 % of test questions .

RANKING 2128
QUERY
distant supervision from disparate sources for low - resource part - of - speech tagging we introduce dsds : a cross - lingual neural part - of - speech tagger that learns from disparate sources of distant supervision , and realistically scales to hundreds of low - resource languages . the model exploits annotation projection , instance selection , tag dictionaries , morphological lexicons , and distributed representations , all in a uniform framework . the approach is simple , yet surprisingly effective , resulting in a new state of the art without access to any gold annotated data .
First cited at 3
TOP CITED PAPERS
RANK 3
learning a part - of - speech tagger from two hours of annotation most work on weakly - supervised learning for part - of - speech taggers has been based on unrealistic assumptions about the amount and quality of training data . for this paper , we attempt to create true low - resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages kinyarwanda and malagasy . given these severely limited amounts of either type supervision ( tag dictionaries ) or token supervision ( labeled sentences ) , we are able to dramatically improve the learning of a hidden markov model through our method of automatically generalizing the annotations , reducing noise , and inducing word - tag frequency information .
RANK 10
real - world semi - supervised learning of pos - taggers for low - resource languages developing natural language processing tools for low - resource languages often requires creating resources from scratch . while a variety of semi - supervised methods exist for training from incomplete data , there are open questions regarding what types of training data should be used and how much is necessary . we discuss a series of experiments designed to shed light on such questions in the context of part - of - speech tagging . we obtain timed annotations from linguists for the low - resource languages kinyarwanda and malagasy ( as well as english ) and evaluate how the amounts of various kinds of data affect performance of a trained pos - tagger . our results show that annotation of word types is the most important , provided a sufficiently capable semi - supervised learning infrastructure is in place to project type information onto a raw corpus . we also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available .
RANK 25
unsupervised part - of - speech tagging with bilingual graph - based projections we describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language . our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages . we use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( bergkirkpatrick et al . , 2010 ) . across eight european languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden markov models induced with the expectation maximization algorithm .
TOP UNCITED PAPERS
RANK 1
infusion of labeled data into distant supervision for relation extraction distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models . however , in some cases a small amount of human labeled data is available . in this paper , we demonstrate how a state - of - theart multi - instance multi - label model can be modified to make use of these reliable sentence - level labels in addition to the relation - level distant supervision from a database . experiments show that our approach achieves a statistically significant increase of 13.5 % in f - score and 37 % in area under the precision recall curve .
RANK 2
combining generative and discriminative model scores for distant supervision distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text . in this work we combine the output of a discriminative at - least - one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data . the combination significantly increases the ranking quality of extracted facts and achieves state - of - the - art extraction performance in an end - to - end setting . a simple linear interpolation of the model scores performs better than a parameter - free scheme based on nondominated sorting .
RANK 4
improving distant supervision using inference learning distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort . however , this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data . this work proposes a novel method for detecting potential false negative training examples using a knowledge inference method . results show that our approach improves the performance of relation extraction systems trained using distantly supervised data .
TOP 20
RANK = 1; score = 0.2840176617838275; correct = False; id = 7d7eee7135ceef77683c406a996e3f387e6c941c
infusion of labeled data into distant supervision for relation extraction distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models . however , in some cases a small amount of human labeled data is available . in this paper , we demonstrate how a state - of - theart multi - instance multi - label model can be modified to make use of these reliable sentence - level labels in addition to the relation - level distant supervision from a database . experiments show that our approach achieves a statistically significant increase of 13.5 % in f - score and 37 % in area under the precision recall curve .
RANK = 2; score = 0.28271890073954486; correct = False; id = 229528c5b746c2f05e04f239632dd9d1de6dd102
combining generative and discriminative model scores for distant supervision distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text . in this work we combine the output of a discriminative at - least - one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data . the combination significantly increases the ranking quality of extracted facts and achieves state - of - the - art extraction performance in an end - to - end setting . a simple linear interpolation of the model scores performs better than a parameter - free scheme based on nondominated sorting .
RANK = 3; score = 0.2738180303427955; correct = True; id = 727209a57c643472c8fc166ba3cc373936acc8d0
learning a part - of - speech tagger from two hours of annotation most work on weakly - supervised learning for part - of - speech taggers has been based on unrealistic assumptions about the amount and quality of training data . for this paper , we attempt to create true low - resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages kinyarwanda and malagasy . given these severely limited amounts of either type supervision ( tag dictionaries ) or token supervision ( labeled sentences ) , we are able to dramatically improve the learning of a hidden markov model through our method of automatically generalizing the annotations , reducing noise , and inducing word - tag frequency information .
RANK = 4; score = 0.2553378912499678; correct = False; id = 4a3c9cba5a12d1c8704fb3d58a7215cf7e565265
improving distant supervision using inference learning distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort . however , this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data . this work proposes a novel method for detecting potential false negative training examples using a knowledge inference method . results show that our approach improves the performance of relation extraction systems trained using distantly supervised data .
RANK = 5; score = 0.25521238250590245; correct = False; id = 8497a5821d5d0d0439b57cf5b1e24c788481d72a
memory - based morphological analysis generation and part - of - speech tagging of arabic we explore the application of memorybased learning to morphological analysis and part - of - speech tagging of written arabic , based on data from the arabic treebank . morphological analysis – the construction of all possible analyses of isolated unvoweled wordforms – is performed as a letter - by - letter operation prediction task , where the operation encodes segmentation , part - of - speech , character changes , and vocalization . part - of - speech tagging is carried out by a bi - modular tagger that has a subtagger for known words and one for unknown words . we report on the performance of the morphological analyzer and part - of - speech tagger . we observe that the tagger , which has an accuracy of 91.9 % on new data , can be used to select the appropriate morphological analysis of words in context at a precision of 64.0 and a recall of 89.7 .
RANK = 6; score = 0.2546537666926271; correct = False; id = 5068d09bb8ce6166fc6f4ca79363b37359082ac6
unsupervised and lightly supervised part - of - speech tagging using recurrent neural networks in this paper , we propose a novel approach to induce automatically a part - of - speech ( pos ) tagger for resource - poor languages ( languages that have no labeled training data ) . this approach is based on cross - language projection of linguistic annotations from parallel corpora without the use of word alignment information . our approach does not assume any knowledge about foreign languages , making it applicable to a wide range of resource - poor languages . we use recurrent neural networks ( rnns ) as multilingual analysis tool . our approach combined with a basic crosslingual projection method ( using word alignment information ) achieves comparable results to the state - of - the - art . we also use our approach in a weakly supervised context , and it shows an excellent potential for very lowresource settings ( less than 1k training utterances ) .
RANK = 7; score = 0.25357677249789484; correct = False; id = 09a245d80a9aadf987ccaa4108dccc6a01b06a55
cross - lingual part - of - speech tagging through ambiguous learning when part - of - speech annotated data is scarce , e.g. for under - resourced languages , one can turn to cross - lingual transfer and crawled dictionaries to collect partially supervised data . we cast this problem in the framework of ambiguous learning and show how to learn an accurate history - based model . experiments on ten languages show significant improvements over prior state of the art performance .
RANK = 8; score = 0.2460301752953718; correct = False; id = 1757c977e99a8505ba989f2ac83c4791eb40a3cf
what can we get from 1000 tokens ? a case study of multilingual pos tagging for resource - poor languages in this paper we address the problem of multilingual part - of - speech tagging for resource - poor languages . we use parallel data to transfer part - of - speech information from resource - rich to resourcepoor languages . additionally , we use a small amount of annotated data to learn to “ correct ” errors from projected approach such as tagset mismatch between languages , achieving state - of - the - art performance ( 91.3 % ) across 8 languages . our approach is based on modest data requirements , and uses minimum divergence classification . for situations where no universal tagset mapping is available , we propose an alternate method , resulting in state - of - the - art 85.6 % accuracy on the resource - poor language malagasy .
RANK = 9; score = 0.24535961530628655; correct = False; id = 4291fd9987414f546fafa477edd718dffdcb2ebd
cross - lingual morphological tagging for low - resource languages morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools . we propose models suitable for training morphological taggers with rich tagsets for low - resource languages without using direct supervision . our approach extends existing approaches of projecting part - of - speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token . we propose a tagging model using wsabie , a discriminative embeddingbased model with rank - based learning . in our evaluation on 11 languages , on average this model performs on par with a baseline weakly - supervised hmm , while being more scalable . multilingual experiments show that the method performs best when projecting between related language pairs . despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by + 0.6 las on average .
RANK = 10; score = 0.24252626958041154; correct = True; id = 44261037e21b53775289d93ff64f93da0011108e
real - world semi - supervised learning of pos - taggers for low - resource languages developing natural language processing tools for low - resource languages often requires creating resources from scratch . while a variety of semi - supervised methods exist for training from incomplete data , there are open questions regarding what types of training data should be used and how much is necessary . we discuss a series of experiments designed to shed light on such questions in the context of part - of - speech tagging . we obtain timed annotations from linguists for the low - resource languages kinyarwanda and malagasy ( as well as english ) and evaluate how the amounts of various kinds of data affect performance of a trained pos - tagger . our results show that annotation of word types is the most important , provided a sufficiently capable semi - supervised learning infrastructure is in place to project type information onto a raw corpus . we also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available .
RANK = 11; score = 0.2420260377835907; correct = False; id = bccf9149c1c6e4087d15004c04a6715fef94b636
induction of fine - grained part - of - speech taggers via classifier combination and crosslingual projection this paper presents an original approach to part - of - speech tagging of fine - grained features ( such as case , aspect , and adjective person / number ) in languages such as english where these properties are generally not morphologically marked . the goals of such rich lexical tagging in english are to provide additional features for word alignment models in bilingual corpora ( for statistical machine translation ) , and to provide an information source for part - of - speech tagger induction in new languages via tag projection across bilingual corpora . first , we present a classifier - combination approach to tagging english bitext with very fine - grained part - of - speech tags necessary for annotating morphologically richer languages such as czech and french , combining the extracted features of three major english parsers , and achieve fine - grained - tag - level syntactic analysis accuracy higher than any individual parser . second , we present experimental results for the cross - language projection of partof - speech taggers in czech and french via word - aligned bitext , achieving successful fine - grained part - of - speech tagging of these languages without any czech or french training data of any kind .
RANK = 12; score = 0.23457891337625886; correct = False; id = 9da121d2b6a6de555d08c4d048ad3cddac771a3c
part - of - speech taggers for low - resource languages using cca features in this paper , we address the challenge of creating accurate and robust partof - speech taggers for low - resource languages . we propose a method that leverages existing parallel data between the target language and a large set of resourcerich languages without ancillary resources such as tag dictionaries . crucially , we use cca to induce latent word representations that incorporate cross - genre distributional cues , as well as projected tags from a full array of resource - rich languages . we develop a probability - based confidence model to identify words with highly likely tag projections and use these words to train a multi - class svm using the cca features . our method yields average performance of 85 % accuracy for languages with almost no resources , outperforming a state - of - the - art partiallyobserved crf model .
RANK = 13; score = 0.22388518247912315; correct = False; id = 820f51be1ca2979d5f48675aac82736e34d7ec32
cross - lingual language modeling with syntactic reordering for low - resource speech recognition this paper proposes cross - lingual language modeling for transcribing source resourcepoor languages and translating them into target resource - rich languages if necessary . our focus is to improve the speech recognition performance of low - resource languages by leveraging the language model statistics from resource - rich languages . the most challenging work of cross - lingual language modeling is to solve the syntactic discrepancies between the source and target languages . we therefore propose syntactic reordering for cross - lingual language modeling , and present a first result that compares inversion transduction grammar ( itg ) reordering constraints to ibm and local constraints in an integrated speech transcription and translation system . evaluations on resource - poor cantonese speech transcription and cantonese to resource - rich mandarin translation tasks show that our proposed approach improves the system performance significantly , up to 3.4 % relative wer reduction in cantonese transcription and 13.3 % relative bilingual evaluation understudy ( bleu ) score improvement in mandarin transcription compared with the system without reordering .
RANK = 14; score = 0.22353862784758471; correct = False; id = e01f462a876ba13db59db681520f004c6dfca5c4
transfer learning for low - resource neural machine translation the encoder - decoder framework for neural machine translation ( nmt ) has been shown effective in large data scenarios , but is much less effective for low - resource languages . we present a transfer learning method that significantly improves bleu scores across a range of low - resource languages . our key idea is to first train a high - resource language pair ( the parent model ) , then transfer some of the learned parameters to the low - resource pair ( the child model ) to initialize and constrain training . using our transfer learning method we improve baseline nmt models by an average of 5.6 bleu on four low - resource language pairs . ensembling and unknown word replacement add another 2 bleu which brings the nmt performance on low - resource machine translation close to a strong syntax based machine translation ( sbmt ) system , exceeding its performance on one language pair . additionally , using the transfer learning model for re - scoring , we can improve the sbmt system by an average of 1.3 bleu , improving the state - of - the - art on low - resource machine translation .
RANK = 15; score = 0.21984468459185325; correct = False; id = 04a43d750e7f8ad91a318cb4498e5a37ae3b00d3
a comparison of weak supervision methods for knowledge base construction we present a comparison of weak and distant supervision methods for producing proxy examples for supervised relation extraction . we find that knowledge - based weak supervision tends to outperform popular distance supervision techniques , providing a higher yield of positive examples and more accurate models .
RANK = 16; score = 0.21659164191868788; correct = False; id = 5020a9c06e82d4122708f9ca68276ebc2735e1ea
coupling an annotated corpus and a morphosyntactic lexicon for state - of - the - art pos tagging with less human effort this paper investigates how to best couple hand - annotated data with information extracted from an external lexical resource to improve pos tagging performance . focusing on french tagging , we introduce a maximum entropy conditional sequence tagging system that is enriched with information extracted from a morphological resource . this system gives a 97.7 % accuracy on the french treebank , an error reduction of 23 % ( 28 % on unknown words ) over the same tagger without lexical information . we also conduct experiments on datasets and lexicons of varying sizes in order to assess the best trade - off between annotating data vs. developing a lexicon . we find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource , and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data by at least one half .
RANK = 17; score = 0.21607730870743797; correct = False; id = ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f
end - to - end relation extraction using distant supervision from external semantic repositories in this paper , we extend distant supervision ( ds ) based on wikipedia for relation extraction ( re ) by considering ( i ) relations defined in external repositories , e.g. yago , and ( ii ) any subset of wikipedia documents . we show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision . our experiments with state - of - the - art relation extraction models , trained on the above data , show a meaningful f1 of 74.29 % on a manually annotated test set : this highly improves the state - of - art in re using ds . additionally , our end - to - end experiments demonstrated that our extractors can be applied to any general text document .
RANK = 18; score = 0.2151483274121774; correct = False; id = 3e01260b55a37c7b391122a120c067350f6976cf
a convex relaxation for weakly supervised relation extraction a promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents . using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships . however , distant supervision leads to a challenging multiple instance , multiple label learning problem . most of the proposed solutions to this problem are based on non - convex formulations , and are thus prone to local minima . in this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation . we demonstrate that our approach outperforms state - of - the - art methods on the challenging dataset introduced by riedel et al . ( 2010 ) .
RANK = 19; score = 0.21427174488552778; correct = False; id = 05369d319bc1811fbfc57abfcef00273b325128e
big data versus the crowd : looking for relationships in all the right places classically , training relation extractors relies on high - quality , manually annotated training data , which can be expensive to obtain . to mitigate this cost , nlu researchers have considered two newly available sources of less expensive ( but potentially lower quality ) labeled data from distant supervision and crowd sourcing . there is , however , no study comparing the relative impact of these two sources on the precision and recall of post - learning answers . to fill this gap , we empirically study how state - of - the - art techniques are affected by scaling these two sources . we use corpus sizes of up to 100 million documents and tens of thousands of crowd - source labeled examples . our experiments show that increasing the corpus size for distant supervision has a statistically significant , positive impact on quality ( f1 score ) . in contrast , human feedback has a positive and statistically significant , but lower , impact on precision and recall .
RANK = 20; score = 0.2121319415354613; correct = False; id = 063330ce9119b089523946b1c06bcd4d8cdcf46d
a syntax - based part - of - speech analyser there are two main methodologies for constructing the knowledge base of a natural language analyser : the linguistic and the data - driven . recent state - of - the - art part - of - speech taggers are based on the data - driven approach . because of the known feasibility of the linguistic rule - based approach at related levels of description , the success of the data - driven approach in part - of - speech analysis may appear surprising . in this paper 1 , a case is made for the syntactic nature of part - of - speech tagging . a new tagger of english that uses only linguistic distributional rules is outlined and empirically evaluated . tested against a benchmark corpus of 38,000 words of previously unseen text , this syntax - based system reaches an accuracy of above 99 % . compared to the 95 - 97 % accuracy of its best competitors , this result suggests the feasibility of the linguistic approach also in part - of - speech analysis .

RANKING 46
QUERY
cl scholar : the acl anthology knowledge graph miner . we present cl scholar , the acl anthology knowledge graph miner to facilitate highquality search and exploration of current research progress in the computational linguistics community . in contrast to previous works , periodically crawling , indexing and processing of new incoming articles is completely automated in the current system . cl scholar utilizes both textual and network information for knowledge graph construction . as an additional novel initiative , cl scholar supports more than 1200 scholarly natural language queries along with standard keywordbased search on constructed knowledge graph . it answers binary , statistical and list based natural language queries . the current system is deployed at http://cnerg.iitkgp . ac.in/aclakg . we also provide rest api support along with bulk download facility . our code and data are available at https : //github.com / clscholar .
First cited at 4733
TOP CITED PAPERS
RANK 4733
context - enhanced citation sentiment detection sentiment analysis of citations in scientific papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics . current work on citation sentiment detection focuses on only the citation sentence . in this paper , we address the problem of context - enhanced citation sentiment detection . we present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account . we believe that this gold standard is closer to the truth than annotation that looks only at the citation sentence itself . we then explore the effect of context windows of different lengths on the performance of a stateof - the - art citation sentiment detection system when using this context - enhanced gold standard definition .
TOP UNCITED PAPERS
RANK 1
the acl anthology searchbench we describe a novel application for structured search in scientific digital libraries . the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology . the application provides search in both its bibliographic metadata and semantically analyzed full textual content . by combining these two features , very efficient and focused queries are possible . at the same time , the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology . the system currently indexes the textual content of 7,500 anthology papers from 2002–2009 with predicateargument - like semantic structures . it also provides useful search filters based on bibliographic metadata . it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques .
RANK 2
towards an acl anthology corpus with logical document structure . an overview of the acl 2012 contributed task the acl 2012 contributed task is a community effort aiming to provide the full acl anthology as a high - quality corpus with rich markup , following the tei p5 guidelines— a new resource dubbed the acl anthology corpus ( aac ) . the goal of the task is threefold : ( a ) to provide a shared resource for experimentation on scientific text ; ( b ) to serve as a basis for advanced search over the acl anthology , based on textual content and citations ; and , by combining the aforementioned goals , ( c ) to present a showcase of the benefits of natural language processing to a broader audience . the contributed task extends the current anthology reference corpus ( arc ) both in size , quality , and by aiming to provide tools that allow the corpus to be automatically extended with new content — be they scanned or born - digital .
RANK 3
a knowledge engineering approach to natural language understanding this paper describes the results of a preliminary study of a knowledge engineering approach to natural language understanding . a computer system is being developed to handle the acquisition , representation , and use of linguistic knowledge . the computer system is rule - based and utilizes a semantic network for knowledge storage and representation . in order to facilitate the interaction between user and system , input of linguistic knowledge and computer responses are in natural language . knowledge of various types can be entered and utilized : syntactic and semantic ; assertions and rules . the inference tracing facility is also being developed as a part of the rule - based system with output in natural language . a detailed example is presented to illustrate the current capabilities and features of the system .
TOP 20
RANK = 1; score = 0.24474530206802367; correct = False; id = 2e0264c4a6b31f1032a4079b7f259d07d13cf4d9
the acl anthology searchbench we describe a novel application for structured search in scientific digital libraries . the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology . the application provides search in both its bibliographic metadata and semantically analyzed full textual content . by combining these two features , very efficient and focused queries are possible . at the same time , the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology . the system currently indexes the textual content of 7,500 anthology papers from 2002–2009 with predicateargument - like semantic structures . it also provides useful search filters based on bibliographic metadata . it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques .
RANK = 2; score = 0.23011697045265375; correct = False; id = d91c724f45a328ce4f7a44142cf53bbc1b9a809d
towards an acl anthology corpus with logical document structure . an overview of the acl 2012 contributed task the acl 2012 contributed task is a community effort aiming to provide the full acl anthology as a high - quality corpus with rich markup , following the tei p5 guidelines— a new resource dubbed the acl anthology corpus ( aac ) . the goal of the task is threefold : ( a ) to provide a shared resource for experimentation on scientific text ; ( b ) to serve as a basis for advanced search over the acl anthology , based on textual content and citations ; and , by combining the aforementioned goals , ( c ) to present a showcase of the benefits of natural language processing to a broader audience . the contributed task extends the current anthology reference corpus ( arc ) both in size , quality , and by aiming to provide tools that allow the corpus to be automatically extended with new content — be they scanned or born - digital .
RANK = 3; score = 0.20863712442851107; correct = False; id = 486875ce588d9d28b1f097817ebd5bdad7178acf
a knowledge engineering approach to natural language understanding this paper describes the results of a preliminary study of a knowledge engineering approach to natural language understanding . a computer system is being developed to handle the acquisition , representation , and use of linguistic knowledge . the computer system is rule - based and utilizes a semantic network for knowledge storage and representation . in order to facilitate the interaction between user and system , input of linguistic knowledge and computer responses are in natural language . knowledge of various types can be entered and utilized : syntactic and semantic ; assertions and rules . the inference tracing facility is also being developed as a part of the rule - based system with output in natural language . a detailed example is presented to illustrate the current capabilities and features of the system .
RANK = 4; score = 0.2016737650189674; correct = False; id = 6deb726c84ec0509c9b2bbcdc3ba175c6a75b17e
interactive visualization for computational linguistics interactive information visualization is an emerging and powerful research technique that can be used to understand models of language and their abstract representations . much of what computational linguists fall back upon to improve nlp applications and to model language “ understanding ” is structure that has , at best , only an indirect attestation in observable data . an important part of our research progress thus depends on our ability to fully investigate , explain , and explore these structures , both empirically and relative to accepted linguistic theory . the sheer complexity of these abstract structures , and the observable patterns on which they are based , usually limits their accessibility — often even to the researchers creating or attempting to learn them . to aid in this understanding , visual ‘ externalizations’ are used for presentation and explanation — traditional statistical graphs and custom - designed illustrations fill the pages of acl papers . these visualizations provide post hoc insight into the representations and algorithms designed by researchers , but visualization can also assist in the process of research itself . there are special statistical methods , falling under the rubric of “ exploratory data analysis , ” and visualization techniques just for this purpose , in fact , but these are not widely used or even known in cl . these techniques offer the potential for revealing structure and detail in data , before anyone else has noticed them . when observing natural language engineers at work , we also notice that , even without a formal visualization background , they often create sketches to aid in their understanding and communication of complex structures . these are ad hoc visualizations , but they , too , can be extended by taking advantage of current information visualization research . this tutorial will enable members of the acl community to leverage information visualization theory into exploratory data analysis , algorithm design , and data presentation techniques for their own research . we draw on fundamental studies in cognitive psychology to introduce ‘ visual variables’ — visual dimensions on which data can be encoded . we also discuss the use of interaction and animation to enhance the usability and usefulness of visualizations . topics covered in this tutorial include a review of information visualization techniques that are applicable to cl , pointers to existing visualization tools and programming toolkits , and new directions in visualizing cl data and results . we also discuss the challenges of evaluating visualizations , noting differences from the evaluation methods traditionally used in cl , and discuss some heuristic approaches and techniques used for measuring insight . information visualizations in cl research can also be measured by the impact they have on algorithm and data structure design . information visualization is also filled with opportunities to make more creative visualizations that benefit from the cl community ’s deeper collective understanding of natural language . given that most visualizations of language are created by researchers with little or no linguistic expertise , we ’ll cover some open and very ripe possibilities for improving the state of the art in text - based visualizations .
RANK = 5; score = 0.1990147847343099; correct = False; id = 5cdabb942b458486b43214de4bf9359cd74cff64
data - driven graph construction for semi - supervised graph - based learning in nlp graph - based semi - supervised learning has recently emerged as a promising approach to data - sparse learning problems in natural language processing . all graph - based algorithms rely on a graph that jointly represents labeled and unlabeled data points . the problem of how to best construct this graph remains largely unsolved . in this paper we introduce a data - driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classifier . we apply this technique in the framework of label propagation and evaluate it on two different classification tasks , a multi - class lexicon acquisition task and a word sense disambiguation task . significant improvements are demonstrated over both label propagation using conventional graph construction and state - of - the - art supervised classifiers .
RANK = 6; score = 0.19432400024728566; correct = False; id = 213e4ce3e67e1f22644ad56eb33b9c6c51a2f81d
explorations in disambiguation using xml text representation in senseval-3 , cl research participated in four tasks : english all - words , english lexical sample , disambiguation of wordnet glosses , and automatic labeling of semantic roles . this participation was performed within the development of cl research ’s knowledge management system , which massively tags texts with syntactic , semantic , and discourse characterizations and attributes . this system is fully integrated with cl research ’s dimap dictionary maintenance software , which provides access to one or more dictionaries for disambiguation and representation . our core disambiguation functionality , unchanged since senseval-2 , performed at a level comparable to our previous performance . our participation in the senseval-3 tasks was concerned primarily with text processing and representation issues and did not advance our disambiguation capabilities .
RANK = 7; score = 0.19431005553987468; correct = False; id = f5fd5e0e5da36b1a1e7f1a66d4292ba22e53429b
graph parsing with s - graph grammars a key problem in semantic parsing with graph - based semantic representations is graph parsing , i.e. computing all possible analyses of a given graph according to a grammar . this problem arises in training synchronous string - to - graph grammars , and when generating strings from them . we present two algorithms for graph parsing ( bottom - up and top - down ) with s - graph grammars . on the related problem of graph parsing with hyperedge replacement grammars , our implementations outperform the best previous system by several orders of magnitude .
RANK = 8; score = 0.19389168223410508; correct = False; id = 81bf8e6408fda9d789e34225d220468d2fb291e3
word embeddings pointing the way for late antiquity continuous space representations of words are currently at the core of many state - of - the - art approaches to problems in natural language processing . in spite of several advantages of using such methods , they have seen little usage within digital humanities . in this paper , we show a case study of how such models can be used to find interesting relationships within the field of late antiquity . we use a word2vec model trained on over one billion words of latin to investigate the relationships between persons and concepts of interest from works of the 6th - century scholar cassiodorus . the results show that the method has high potential to aid the humanities scholar , but that caution must be taken as the analysis requires the assessment by the traditional historian .
RANK = 9; score = 0.18516773866679323; correct = False; id = 3aa0dceaaa8483d382ed1e1394f91f4be8858478
strong and weak quantifiers in focused nl$$_{\text { cl}}$$ 
RANK = 10; score = 0.18352134245079435; correct = False; id = 3c17c1a3bec7907dd047367bad551845c725ecc2
integrating user - generated content in the acl anthology the acl anthology was revamped in 2012 to its second major version , encompassing faceted navigation , social media use , as well as authorand reader - generated content and comments on published work as part of the revised frontend user interface . at the backend , the anthology was updated to incorporate its publication records into a database . we describe the acl anthology ’s previous legacy , redesign and revamp process and technologies , and its resulting functionality .
RANK = 11; score = 0.17101169894850876; correct = False; id = 6ad51ac518e97ba9eaf8e264aebcbe5b88ccc3fe
cl research 's knowledge management system cl research began experimenting with massive xml tagging of texts to answer questions in trec 2002 . in duc 2003 , the experiments were extended into text summarization . based on these experiments , the knowledge management system ( kms ) was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration . kms has been extended to include web question answering , both general and topic - based summarization , information extraction , and document exploration . the document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation . as development of kms has continued , user modeling has become a key research issue : how will different users want to use the information they identify .
RANK = 12; score = 0.166001422798278; correct = False; id = 239e5924499377a2af1d03ab8fe20de0acea25de
unsupervised natural language processing using graph models in the past , nlp has always been based on the explicit or implicit use of linguistic knowledge . in classical computer linguistic applications explicit rule based approaches prevail , while machine learning algorithms use implicit knowledge for generating linguistic knowledge . the question behind this work is : how far can we go in nlp without assuming explicit or implicit linguistic knowledge ? how much efforts in annotation and resource building are needed for what level of sophistication in text processing ? this work tries to answer the question by experimenting with algorithms that do not presume any linguistic knowledge in the system . the claim is that the knowledge needed can largely be acquired by knowledge - free and unsupervised methods . here , graph models are employed for representing language data . a new graph clustering method finds related lexical units , which form word sets on various levels of homogeneity . this is exemplified and evaluated on language separation and unsupervised part - of - speech tagging , further applications are discussed .
RANK = 13; score = 0.16418598261683065; correct = False; id = 8b1a9a39811acf872037ef2f93db21910ba431ee
graph - based semi - supervised learning algorithms for nlp while labeled data is expensive to prepare , ever increasing amounts of unlabeled linguistic data are becoming widely available . in order to adapt to this phenomenon , several semi - supervised learning ( ssl ) algorithms , which learn from labeled as well as unlabeled data , have been developed . in a separate line of work , researchers have started to realize that graphs provide a natural way to represent data in a variety of domains . graph - based ssl algorithms , which bring together these two lines of work , have been shown to outperform the state - ofthe - art in many applications in speech processing , computer vision and nlp . in particular , recent nlp research has successfully used graph - based ssl algorithms for pos tagging ( subramanya et al . , 2010 ) , semantic parsing ( das and smith , 2011 ) , knowledge acquisition ( talukdar et al . , 2008 ) , sentiment analysis ( goldberg and zhu , 2006 ) and text categorization ( subramanya and bilmes , 2008 ) . recognizing this promising and emerging area of research , this tutorial focuses on graph - based ssl algorithms ( e.g. , label propagation methods ) . the tutorial is intended to be a sequel to the acl 2008 ssl tutorial , focusing exclusively on graph - based ssl methods and recent advances in this area , which were beyond the scope of the previous tutorial . the tutorial is divided in two parts . in the first part , we will motivate the need for graph - based ssl methods , introduce some standard graph - based ssl algorithms , and discuss connections between these approaches . we will also discuss how linguistic data can be encoded as graphs and show how graph - based algorithms can be scaled to large amounts of data ( e.g. , web - scale data ) . part 2 of the tutorial will focus on how graph - based methods can be used to solve several critical nlp tasks , including basic problems such as pos tagging , semantic parsing , and more downstream tasks such as text categorization , information acquisition , and sentiment analysis . we will conclude the tutorial with some exciting avenues for future work . familiarity with semi - supervised learning and graph - based methods will not be assumed , and the necessary background will be provided . examples from nlp tasks will be used throughout the tutorial to convey the necessary concepts . at the end of this tutorial , the attendee will walk away with the following : • an in - depth knowledge of the current state - ofthe - art in graph - based ssl algorithms , and the ability to implement them . • the ability to decide on the suitability of graph - based ssl methods for a problem . • familiarity with different nlp tasks where graph - based ssl methods have been successfully applied . in addition to the above goals , we hope that this tutorial will better prepare the attendee to conduct exciting research at the intersection of nlp and other emerging areas with natural graph - structured data ( e.g. , computation social science ) . please visit http://graph-ssl.wikidot.com/ for details .
RANK = 14; score = 0.16360467796715333; correct = False; id = 6f53a3cf093eb22fb2fa4f684c947dbaf1016991
ubc - as : a graph based unsupervised system for induction and classification this paper describes a graph - based unsupervised system for induction and classification . the system performs a two stage graph based clustering where a cooccurrence graph is first clustered to compute similarities against contexts . the context similarity matrix is pruned and the resulting associated graph is clustered again by means of a random - walk type algorithm . the system relies on a set of parameters that have been tuned to fit the corpus data . the system has participated in tasks 2 and 13 of the semeval-2007 competition , on word sense induction and web people search , respectively , with mixed results .
RANK = 15; score = 0.16321150614459284; correct = False; id = 248a90801a218e92f65cd28db4f9da405030a217
expert stance graphs for computational argumentation we describe the construction of an expert stance graph , a novel , large - scale knowledge resource that encodes the stance of more than 100,000 experts towards a variety of controversial topics . we suggest that this graph may be valuable for various fundamental tasks in computational argumentation . experts and topics in our graph are wikipedia entries . both automatic and semi - automatic methods for building the graph are explored , and manual assessment validates the high accuracy of the resulting graph .
RANK = 16; score = 0.15898843817306163; correct = False; id = 9501796c96303b605075510ddd2d54b6a30e79ee
graph - based algorithms for natural language processing and information retrieval graph theory is a well studied discipline , and so are the fields of natural language processing and information retrieval . however , most of the times , they are perceived as different disciplines , with different algorithms , different applications , and different potential end - users . the goal of this tutorial is to provide an overview of methods and applications in natural language processing and information retrieval that rely on graph - based algorithms . this will include techniques for graph traversal , minimum path length , min - cut algorithms , minimum spanning trees , random walks , etc . and their application to information retrieval and web search , text understanding ( word sense disambiguation and semantic classes ) , parsing , text summarization , keyword extraction , text clustering , and others .
RANK = 17; score = 0.1563886486149008; correct = False; id = b6fb98da4c2f0d8d534668be9fe61395a095306e
graph - based clustering for computational linguistics : a survey in this survey we overview graph - based clustering and its applications in computational linguistics . we summarize graph - based clustering as a five - part story : hypothesis , modeling , measure , algorithm and evaluation . we then survey three typical nlp problems in which graph - based clustering approaches have been successfully applied . finally , we comment on the strengths and weaknesses of graph - based clustering and envision that graph - based clustering is a promising solution for some emerging nlp problems .
RANK = 18; score = 0.15622934915955222; correct = False; id = d3924eddda7f7d80216f8d8d63d19b41bfcd5132
what you need to know about chinese for chinese language processing the synergy between language sciences and language technology has been an elusive one for the computational linguistics community , especially when dealing with a language other than english . the reasons are two - fold : the lack of an accessible comprehensive and robust account of a specific language so as to allow strategic linking between a processing task to linguistic devices , and the lack of successful computational studies taking advantage of such links . with a fast growing number of available online resources , as well as a rapidly increasing number of members of the cl community who are interested in and/or working on chinese language processing , the time is ripe to take a serious look at how knowledge of chinese can help chinese language processing .
RANK = 19; score = 0.15492412377811238; correct = False; id = 07c5dddd22260d538c58d2bd71ee3bfbde3b231f
joint annotation of search queries marking up search queries with linguistic annotations such as part - of - speech tags , capitalization , and segmentation , is an important part of query processing and understanding in information retrieval systems . due to their brevity and idiosyncratic structure , search queries pose a challenge to existing nlp tools . to address this challenge , we propose a probabilistic approach for performing joint query annotation . first , we derive a robust set of unsupervised independent annotations , using queries and pseudo - relevance feedback . then , we stack additional classifiers on the independent annotations , and exploit the dependencies between them to further improve the accuracy , even with a very limited amount of available training data . we evaluate our method using a range of queries extracted from a web search log . experimental results verify the effectiveness of our approach for both short keyword queries , and verbose natural language queries .
RANK = 20; score = 0.15286962793327877; correct = False; id = a80499b650652fbfaf5dc542db327f7f0f950c5e
gowvis : a web application for graph - of - words - based text visualization and summarization we introduce gowvis1 , an interactive web application that represents any piece of text inputted by the user as a graph - ofwords and leverages graph degeneracy and community detection to generate an extractive summary ( keyphrases and paragraph ) of the inputted text in an unsupervised fashion . the entire analysis can be fully customized via the tuning of many text preprocessing , graph building , and graph mining parameters . our system is thus well suited to educational purposes , exploration and early research experiments . the new summarization strategy we propose also shows promise .

